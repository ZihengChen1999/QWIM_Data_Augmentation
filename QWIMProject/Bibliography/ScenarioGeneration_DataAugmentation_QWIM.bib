% Encoding: UTF-8

@Article{AlJanabi-2015,
  author               = {Al Janabi, Mazin, A. M.},
  date                 = {2015-09},
  journaltitle         = {International Journal of Financial Engineering},
  title                = {Scenario optimization technique for the assessment of downside-risk and investable portfolios in post-financial crisis},
  doi                  = {10.1142/s2424786315500280},
  number               = {03},
  pages                = {1550028+},
  volume               = {02},
  abstract             = {The aim of this paper is to develop an optimization technique for the assessment of downside-risk limits and investable financial portfolios under crisis-driven outlooks subject to applying meaningful financial and operational constraints. The simulation and testing methods are based on the renowned concept of liquidity-adjusted value-at-risk (LVaR) along with the development of an optimization risk-algorithm utilizing matrix?algebra technique.

With the purpose of demonstrating the effectiveness of LVaR and stress-testing techniques, real-world quantitative analysis of structured equity portfolios are depicted for the Gulf Cooperation Council (GCC) financial markets. To this end, several structural simulations studies are accomplished with the goal of establishing realistic financial modeling algorithm for the calculation of downside-risk parameters and to empirically assess portfolio managers optimal and investable portfolios.

The developed methodology and risk valuation algorithms can aid in advancing risk assessment and portfolio management practices in emerging markets, particularly in the wake of the most recent credit crunch and the subsequent financial turmoil.},
  citeulike-article-id = {13935035},
  citeulike-linkout-0  = {http://dx.doi.org/10.1142/s2424786315500280},
  citeulike-linkout-1  = {http://www.worldscientific.com/doi/abs/10.1142/S2424786315500280},
  day                  = {1},
  groups               = {Scenario generation, Scenario_Portfolio, Scenario_Market, Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:30:01},
  publisher            = {World Scientific Publishing Co.},
  timestamp            = {2020-02-29 18:34},
}

@Article{Bass-et-al-2018,
  author         = {Bass, Robert and Gallagher, Katelyn and Ratcliffe, Ronald and Shah, Sunil},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Factor Performance Across Market-Driven Scenarios},
  doi            = {10.2139/ssrn.3184905},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3184905},
  abstract       = {We develop a methodology to model factor returns in scenarios which are yet to occur or do not correspond to past market conditions. We infer factor performance conditional on hypothetical market-driven scenarios, which are determined by a parsimonious number of underlying policy shocks. We derive regime-dependent corresponding security and asset class returns, which are functions of factors and policy drivers. The analysis is helpful in constructing robust portfolios designed for more stable performance across various scenarios, and can aid in framing and communicating portfolio risks.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_Market, Scenario_Portfolio},
  timestamp      = {2020-02-29 18:34},
}

@Article{Beraldi-et-al-2012,
  author               = {Beraldi, Patrizia and De Simone, Francesco and Violi, Antonio and Consigli, Giorgio and Iaquinta, Gaetano},
  date                 = {2012-10},
  journaltitle         = {IMA Journal of Management Mathematics},
  title                = {Scenario-based dynamic corporate bond portfolio management},
  doi                  = {10.1093/imaman/dps017},
  issn                 = {1471-6798},
  number               = {4},
  pages                = {341--364},
  volume               = {23},
  abstract             = {The 2008 credit crisis has deeply affected the price of corporate liabilities in both equity and fixed income secondary markets leading to unprecedented portfolio losses by financial investors. A coordinated intervention by monetary institutions limited the systemic consequences of the crisis, without, however, avoiding a significant fall of corporate bond prices across international markets. In this article, we analyse alternative portfolio optimization approaches in the fixed income market over the 2008 2009 period, a time in which credit derivative markets became very illiquid. All policies are analysed relying on a unique set of market and credit scenarios generated by common and idiosyncratic risk factors on an extended investment universe. The crisis provides an interesting test period to analyse in particular the potential of dynamic versus static portfolio selection approaches. We also consider dynamic portfolio strategies based on multistage stochastic programming versus policy rule-based methods and analyse their relative performance against a corporate bond index widely adopted in practice as a market benchmark.},
  citeulike-article-id = {13988679},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/imaman/dps017},
  citeulike-linkout-1  = {http://imaman.oxfordjournals.org/content/23/4/341.abstract},
  citeulike-linkout-2  = {http://imaman.oxfordjournals.org/content/23/4/341.full.pdf},
  day                  = {01},
  groups               = {PortfOptim_Scenario, Scenario_Portfolio, Scenario_Market, PortfOptim_Dynamic},
  owner                = {cristi},
  posted-at            = {2016-03-26 23:44:09},
  publisher            = {Oxford University Press},
  timestamp            = {2020-02-29 18:34},
}

@InCollection{ClementGrandcourt-Fraysse-2015,
  author               = {Clement-Grandcourt, Arnaud and Fraysse, Herve},
  booktitle            = {Hazardous Forecasts and Crisis Scenario Generator},
  date                 = {2015},
  title                = {How to Use These Scenarios for Asset Management?},
  doi                  = {10.1016/b978-1-78548-028-7.50003-5},
  isbn                 = {9781785480287},
  pages                = {75--126},
  publisher            = {Elsevier},
  abstract             = {The University of Zurich did some research on features to overcome the weaknesses of the traditional mean variance optimization. Indeed, the mean variance utility function of the Sharpe method is the easiest to implement, but using the second-order moment causes several optimization issues. Variance, as an addition of squared values, does not distinguish positive and negative moves: this is an important problem when there are dissymmetric fat tails. Moreover, in a bear market, volume is decreasing and operators are more nervous; consequently, noise becomes important and adds up in variance. Studies of noise in the markets are numerous: for instance, the papers by Filippi, Lepage, Meyrignac and Pochon on high-frequency statements of stock prices [FRA 12], Kai Yao [YIN 90] on low frequency and Professor Haugen's studies on weekly statements and monthly statements showed important random noises that increase in bear markets. Trading algorithms use filters, such as the Kalman filter for low-frequency information or the particle filter for high-frequency information; smoothing formulas are used by Professor Haugen and many others. Trading systems are issuing more than one order out of two in New York and one out of three in Europe.},
  citeulike-article-id = {14433186},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/b978-1-78548-028-7.50003-5},
  groups               = {Scenario_Portfolio, Scenario_Market},
  posted-at            = {2017-09-17 14:35:12},
  timestamp            = {2020-02-29 18:34},
}

@Book{Allen-2012,
  author    = {Steve L. Allen},
  date      = {2012},
  title     = {Financial Risk Management: A Practitioner s Guide to Managing Market and Credit Risk},
  publisher = {Wiley},
  url       = {https://www.wiley.com/en-us/Financial+Risk+Management%3A+A+Practitioner%27s+Guide+to+Managing+Market+and+Credit+Risk%2C+2nd+Edition-p-9781118175453},
  abstract  = {market risk expert Steve Allen offers an insider view of this discipline and covers the strategies, principles, and measurement techniques necessary to manage and measure financial risk. Fully revised to reflect today dynamic environment and the lessons to be learned from the 2008 global financial crisis, this reliable resource provides a comprehensive overview of the entire field of risk management.

Allen explores real-world issues such as proper mark-to-market valuation of trading positions and determination of needed reserves against valuation uncertainty, the structuring of limits to control risk taking, and a review of mathematical models and how they can contribute to risk control. Along the way, he shares valuable lessons that will help to develop an intuitive feel for market risk measurement and reporting.

Presents key insights on how risks can be isolated, quantified, and managed from a top risk management practitioner Offers up-to-date examples of managing market and credit risk Provides an overview and comparison of the various derivative instruments and their use in risk hedging Companion Website contains supplementary materials that allow you to continue to learn in a hands-on fashion long after closing the book},
  groups    = {Scenario_ExpertView},
  owner     = {zkgst0c},
  timestamp = {2020-02-29 18:35},
}

@Article{AndersonCook-et-al-2015,
  author               = {Anderson-Cook, Christine and Morzinski, Jerome and Blecker, Kenneth},
  date                 = {2015-08},
  journaltitle         = {Systems},
  title                = {Statistical Model Selection for Better Prediction and Discovering Science Mechanisms That Affect Reliability},
  doi                  = {10.3390/systems3030109},
  number               = {3},
  pages                = {109--132},
  volume               = {3},
  abstract             = {Understanding the impact of production, environmental exposure and age characteristics on the reliability of a population is frequently based on underlying science and empirical assessment. When there is incomplete science to prescribe which inputs should be included in a model of reliability to predict future trends, statistical model/variable selection techniques can be leveraged on a stockpile or population of units to improve reliability predictions as well as suggest new mechanisms affecting reliability to explore. We describe a five-step process for exploring relationships between available summaries of age, usage and environmental exposure and reliability. The process involves first identifying potential candidate inputs, then second organizing data for the analysis. Third, a variety of models with different combinations of the inputs are estimated, and fourth, flexible metrics are used to compare them. Finally, plots of the predicted relationships are examined to distill leading model contenders into a prioritized list for subject matter experts to understand and compare. The complexity of the model, quality of prediction and cost of future data collection are all factors to be considered by the subject matter experts when selecting a final model.},
  citeulike-article-id = {14071197},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/systems3030109},
  citeulike-linkout-1  = {http://www.mdpi.com/2079-8954/3/3/109},
  citeulike-linkout-2  = {http://www.mdpi.com/2079-8954/3/3/109/pdf},
  day                  = {19},
  groups               = {Scenario_ExpertView},
  owner                = {zkgst0c},
  posted-at            = {2016-06-17 22:13:02},
  timestamp            = {2020-02-29 18:35},
}

@Article{Armstrong-et-al-2015,
  author               = {Armstrong, J. Scott and Green, Kesten C. and Graefe, Andreas},
  date                 = {2015-08},
  journaltitle         = {Journal of Business Research},
  title                = {Golden rule of forecasting: Be conservative},
  doi                  = {10.1016/j.jbusres.2015.03.031},
  issn                 = {0148-2963},
  number               = {8},
  pages                = {1717--1731},
  volume               = {68},
  abstract             = {This article proposes a unifying theory, or the Golden Rule, of forecasting. The Golden Rule of Forecasting is to be conservative. A conservative forecast is consistent with cumulative knowledge about the present and the past. To be conservative, forecasters must seek out and use all knowledge relevant to the problem, including knowledge of methods validated for the situation. Twenty-eight guidelines are logically deduced from the Golden Rule. A review of evidence identified 105 papers with experimental comparisons; 102 support the guidelines. Ignoring a single guideline increased forecast error by more than two-fifths on average. Ignoring the Golden Rule is likely to harm accuracy most when the situation is uncertain and complex, and when bias is likely. Non-experts who use the Golden Rule can identify dubious forecasts quickly and inexpensively. To date, ignorance of research findings, bias, sophisticated statistical procedures, and the proliferation of big data, have led forecasters to violate the Golden Rule. As a result, despite major advances in evidence-based forecasting methods, forecasting practice in many fields has failed to improve over the past half-century.},
  citeulike-article-id = {14072221},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbusres.2015.03.031},
  groups               = {Scenario_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-06-20 01:26:35},
  timestamp            = {2020-02-29 18:35},
}

@Article{BenTaieb-et-al-2012,
  author               = {Ben Taieb, Souhaib and Bontempi, Gianluca and Atiya, Amir F. and Sorjamaa, Antti},
  date                 = {2012-06},
  journaltitle         = {Expert Systems with Applications},
  title                = {A review and comparison of strategies for multi-step ahead time series forecasting based on the NN5 forecasting competition},
  doi                  = {10.1016/j.eswa.2012.01.039},
  issn                 = {0957-4174},
  number               = {8},
  pages                = {7067--7083},
  volume               = {39},
  abstract             = {Multi-step ahead forecasting is still an open challenge in time series forecasting. Several approaches that deal with this complex problem have been proposed in the literature but an extensive comparison on a large number of tasks is still missing. This paper aims to fill this gap by reviewing existing strategies for multi-step ahead forecasting and comparing them in theoretical and practical terms. To attain such an objective, we performed a large scale comparison of these different strategies using a large experimental benchmark (namely the 111 series from the NN5 forecasting competition). In addition, we considered the effects of deseasonalization, input variable selection, and forecast combination on these strategies and on multi-step ahead forecasting at large. The following three findings appear to be consistently supported by the experimental results: Multiple-Output strategies are the best performing approaches, deseasonalization leads to uniformly improved forecast accuracy, and input selection is more effective when performed in conjunction with deseasonalization.},
  citeulike-article-id = {10451749},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2012.01.039},
  groups               = {Scenario_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-06-20 01:18:05},
  timestamp            = {2020-02-29 18:35},
}

@Article{Bolger-Houlding-2017,
  author               = {Bolger, Donnacha and Houlding, Brett},
  date                 = {2017-02},
  journaltitle         = {Reliability Engineering and System Safety},
  title                = {Deriving the probability of a linear opinion pooling method being superior to a set of alternatives},
  doi                  = {10.1016/j.ress.2016.10.008},
  issn                 = {0951-8320},
  pages                = {41--49},
  volume               = {158},
  abstract             = {A novel context for combination of expert opinion is provided. A dynamic reliability assessment method is stated, justified by properties and a data study. The theoretical grounding underlying the data-driven justification is explored. We conclude with areas for expansion and further relevant research. Linear opinion pools are a common method for combining a set of distinct opinions into a single succinct opinion, often to be used in a decision making task. In this paper we consider a method, termed the Plug-in approach, for determining the weights to be assigned in this linear pool, in a manner that can be deemed as rational in some sense, while incorporating multiple forms of learning over time into its process. The environment that we consider is one in which every source in the pool is herself a decision maker (DM), in contrast to the more common setting in which expert judgments are amalgamated for use by a single DM. We discuss a simulation study that was conducted to show the merits of our technique, and demonstrate how theoretical probabilistic arguments can be used to exactly quantify the probability of this technique being superior (in terms of a probability density metric) to a set of alternatives. Illustrations are given of simulated proportions converging to these true probabilities in a range of commonly used distributional cases.},
  citeulike-article-id = {14332500},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ress.2016.10.008},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-04-05 20:25:18},
  timestamp            = {2020-02-29 18:35},
}

@Article{Bolger-Wright-2017,
  author               = {Bolger, Fergus and Wright, George},
  date                 = {2017-01},
  journaltitle         = {International Journal of Forecasting},
  title                = {Use of expert knowledge to anticipate the future: Issues, analysis and directions},
  doi                  = {10.1016/j.ijforecast.2016.11.001},
  issn                 = {0169-2070},
  number               = {1},
  pages                = {230--243},
  volume               = {33},
  abstract             = {Unless an anticipation problem is routine and short-term, and objective data are plentiful, expert judgment will be needed. Risk assessment is analogous to anticipating the future, in that models need to be developed and applied to data. Since objective data are often scanty, expert knowledge elicitation (EKE) techniques have been developed for risk assessment that allow models to be developed and parametrized using expert judgment with minimal cognitive and social biases. Here, we conceptualize how EKE can be developed and applied to support anticipation of the future. Accordingly, we begin by defining EKE as a complete process, which involves considering experts as a source of data, and comprises various methods for ensuring the quality of this data, including selecting the best experts, training experts in the normative aspects of anticipation, and combining judgments from several experts, as well as eliciting unbiased estimates and constructs from experts. We detail various aspects of the papers that constitute this special issue and analyse them in terms of the stages of the EKE future-anticipation process that they address. We also identify the remaining gaps in our knowledge. Our conceptualization of EKE with the aim of supporting anticipation of the future is compared and contrasted with the extant research on judgmental forecasting.},
  citeulike-article-id = {14332267},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2016.11.001},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-04-05 11:58:38},
  timestamp            = {2020-02-29 18:35},
}

@InCollection{Cerqueira-et-al-2017,
  author         = {Cerqueira, Vitor and Torgo, Luis and Pinto, Fabio and Soares, Carlos},
  booktitle      = {Machine learning and knowledge discovery in databases},
  date           = {2017},
  title          = {Arbitrated ensemble for time series forecasting},
  doi            = {10.1007/978-3-319-71246-8\_29},
  editor         = {Ceci, Michelangelo and Hollmen, Jaakko and Todorovski, Ljupco and Vens, Celine and Dzeroski, Saso},
  isbn           = {978-3-319-71245-1},
  pages          = {478--494},
  publisher      = {Springer International Publishing},
  series         = {Lecture notes in computer science},
  volume         = {10535},
  abstract       = {This paper proposes an ensemble method for time series forecasting tasks. Combining different forecasting models is a common approach to tackle these problems. State-of-the-art methods track the loss of the available models and adapt their weights accordingly. Metalearning strategies such as stacking are also used in these tasks. We propose a metalearning approach for adaptively combining forecasting models that specializes them across the time series. Our assumption is that different forecasting models have different areas of expertise and a varying relative performance. Moreover, many time series show recurring structures due to factors such as seasonality. Therefore, the ability of a method to deal with changes in relative performance of models as well as recurrent changes in the data distribution can be very useful in dynamic environments. Our approach is based on an ensemble of heterogeneous forecasters, arbitrated by a metalearning model. This strategy is designed to cope with the different dynamics of time series and quickly adapt the ensemble to regime changes. We validate our proposal using time series from several real world domains. Empirical results show the competitiveness of the method in comparison to state-of-the-art approaches for combining forecasters.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_ExpertView, ML_ForcstTimeSrs},
  issn           = {0302-9743},
  timestamp      = {2020-02-29 18:35},
}

@Article{Chen-Skoglund-2013,
  author       = {Wei Chen and Jimmy Skoglund},
  date         = {2013},
  journaltitle = {Journal of Risk Model Validation},
  title        = {An integrated stress testing framework via Markov switching simulation},
  doi          = {10.21314/JRMV.2013.104},
  number       = {2},
  pages        = {3--29},
  url          = {https://www.risk.net/journal-of-risk-model-validation/2275643/an-integrated-stress-testing-framework-via-markov-switching-simulation},
  volume       = {7},
  abstract     = {Capturing tail events, especially those that include the rare possibility of severe loss, is one of the important objectives of modern risk analysis. However, the past behavior of financial data is not necessarily an accurate predictor of possible scenarios in the future. The economic turmoil of recent years has resulted in calls for a more forward-looking approach to financial risk management that integrates expert knowledge on plausible future scenarios with classical risk management models calibrated on past behavior. As a complementary risk analysis tool, stress testing is receiving more and more attention, both from regulators and from practitioners. Nevertheless, classical risk analysis models, such as value-at-risk models, that are based on historical data, are often disconnected from stress testing. This disconnection can prevent a comprehensive view of the risk profile of a financial institution.

This paper proposes a multiperiod switching simulation based method for integrated stress testing risk analysis that incorporates plausible events that are not necessarily captured in historical data or in historical stressed calibration of risk models. Our application focuses on the integration of rare stress events and model stress into a market risk portfolio. However, the proposed method is also applicable to other financial risk models, eg, portfolio credit risk models. An integrated risk model and stress testing framework not only leads to forward-looking tail risk measurement, which mitigates the black swan effect, but also takes stress testing into advanced risk management decision making analysis, such as scenario-based portfolio optimization.},
  groups       = {Scenario_ExpertView, StressTest_ExpertView},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:35},
}

@Article{Clemen-Winkler-1999,
  author         = {Clemen, Robert T. and Winkler, Robert L.},
  date           = {1999-04},
  journaltitle   = {Risk analysis},
  title          = {Combining probability distributions from experts in risk analysis},
  doi            = {10.1111/j.1539-6924.1999.tb00399.x},
  issn           = {0272-4332},
  number         = {2},
  pages          = {187--203},
  volume         = {19},
  abstract       = {This paper concerns the combination of experts' probability distributions in risk analysis, discussing a variety of combination methods and attempting to highlight the important conceptual and practical issues to be considered in designing a combination process in practice. The role of experts is important because their judgments can provide valuable information, particularly in view of the limited availability of data regarding many important uncertainties in risk analysis. Because uncertainties are represented in terms of probability distributions in probabilistic risk analysis (PRA), we consider expert information in terms of probability distributions. The motivation for the use of multiple experts is simply the desire to obtain as much information as possible. Combining experts' probability distributions summarizes the accumulated information for risk analysts and decision makers. Procedures for combining probability distributions are often compartmentalized as mathematical aggregation methods or behavioral approaches, and we discuss both categories. However, an overall aggregation process could involve both mathematical and behavioral aspects, and no single process is best in all circumstances. An understanding of the pros and cons of different methods and the key issues to consider is valuable in the design of a combination process for a specific PRA. The output, a probability distribution, can ideally be viewed as representing a summary of the current state of expert opinion regarding the uncertainty of interest.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:35},
}

@Article{Colson-Cooke-2017,
  author               = {Colson, Abigail R. and Cooke, Roger M.},
  date                 = {2017-07},
  journaltitle         = {Reliability Engineering and System Safety},
  title                = {Cross validation for the classical model of structured expert judgment},
  doi                  = {10.1016/j.ress.2017.02.003},
  issn                 = {0951-8320},
  pages                = {109--120},
  volume               = {163},
  abstract             = {Mathematical aggregation schemes are evaluated against data. Performance weighting is cross validated. Performance weighting is significantly better than equal weighting. We update the 2008 TU Delft structured expert judgment database with data from 33 professionally contracted Classical Model studies conducted between 2006 and March 2015 to evaluate its performance relative to other expert aggregation models. We briefly review alternative mathematical aggregation schemes, including harmonic weighting, before focusing on linear pooling of expert judgments with equal weights and performance-based weights. Performance weighting outperforms equal weighting in all but 1 of the 33 studies in-sample. True out-of-sample validation is rarely possible for Classical Model studies, and cross validation techniques that split calibration questions into a training and test set are used instead. Performance weighting incurs an "out-of-sample penalty"and its statistical accuracy out-of-sample is lower than that of equal weighting. However, as a function of training set size, the statistical accuracy of performance-based combinations reaches 75 percent of the equal weight value when the training set includes 80 percent of calibration variables. At this point the training set is sufficiently powerful to resolve differences in individual expert performance. The information of performance-based combinations is double that of equal weighting when the training set is at least 50 percent of the set of calibration variables. Previous out-of-sample validation work used a Total Out-of-Sample Validity Index based on all splits of the calibration questions into training and test subsets, which is expensive to compute and includes small training sets of dubious value. As an alternative, we propose an Out-of-Sample Validity Index based on averaging the product of statistical accuracy and information over all training sets sized at 80 percent of the calibration set. Performance weighting outperforms equal weighting on this Out-of-Sample Validity Index in 26 of the 33 post-2006 studies; the probability of 26 or more successes on 33 trials if there were no difference between performance weighting and equal weighting is 0.001.},
  citeulike-article-id = {14332371},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ress.2017.02.003},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-04-05 14:47:00},
  timestamp            = {2020-02-29 18:35},
}

@Article{Ando-Bai-2017,
  author               = {Ando, Tomohiro and Bai, Jushan},
  date                 = {2017-06},
  journaltitle         = {Journal of the American Statistical Association},
  title                = {Clustering Huge Number of Financial Time Series: A Panel Data Approach With High-Dimensional Predictors and Factor Structures},
  doi                  = {10.1080/01621459.2016.1195743},
  number               = {519},
  pages                = {1182--1198},
  volume               = {112},
  abstract             = {AbstractThis article introduces a new procedure for clustering a large number of financial time series based on high-dimensional panel data with grouped factor structures. The proposed method attempts to capture the level of similarity of each of the time series based on sensitivity to observable factors as well as to the unobservable factor structure. The proposed method allows for correlations between observable and unobservable factors and also allows for cross-sectional and serial dependence and heteroscedasticities in the error structure, which are common in financial markets. In addition, theoretical properties are established for the procedure. We apply the method to analyze the returns for over 6000 international stocks from over 100 financial markets. The empirical analysis quantifies the extent to which the U.S. subprime crisis spilled over to the global financial markets. Furthermore, we find that nominal classifications based on either listed market, industry, country or region are insufficient to characterize the heterogeneity of the global financial markets. Supplementary materials for this article are available online.},
  citeulike-article-id = {14431349},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/01621459.2016.1195743},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/01621459.2016.1195743},
  day                  = {10},
  groups               = {Clustering and network analysis, Scenario_TimeSeries},
  posted-at            = {2017-09-16 16:57:06},
  publisher            = {Taylor \& Francis},
  timestamp            = {2020-02-29 18:35},
}

@Article{Barbarino-Bura-2017,
  author               = {Barbarino, Alessandro and Bura, Efstathia},
  date                 = {2017-01},
  journaltitle         = {Finance and Economics Discussion Series},
  title                = {A Unified Framework for Dimension Reduction in Forecasting},
  doi                  = {10.17016/feds.2017.004},
  issn                 = {1936-2854},
  number               = {004},
  volume               = {2017},
  abstract             = {Factor models are widely used in summarizing large datasets with few underlying latent factors and in building time series forecasting models for economic variables. In these models, the reduction of the predictors and the modeling and forecasting of the response y are carried out in two separate and independent phases. We introduce a potentially more attractive alternative, Sufficient Dimension Reduction (SDR), that summarizes x as it relates to y, so that all the information in the conditional distribution of y|x is preserved. We study the relationship between SDR and popular estimation methods, such as ordinary least squares (OLS), dynamic factor models (DFM), partial least squares (PLS) and RIDGE regression, and establish the connection and fundamental differences between the DFM and SDR frameworks. We show that SDR significantly reduces the dimension of widely used macroeconomic series data with one or two sufficient reductions delivering similar forecasting performance to that of competing methods in macro-forecasting.},
  citeulike-article-id = {14315467},
  citeulike-linkout-0  = {http://dx.doi.org/10.17016/feds.2017.004},
  groups               = {Scenario_TimeSeries, Dimens_Reduc},
  posted-at            = {2017-03-21 18:16:31},
  timestamp            = {2020-02-29 18:35},
}

@Article{Balibek-Koksalan-2012,
  author               = {Balibek, E. and Koksalan, M.},
  date                 = {2012-12},
  journaltitle         = {Journal of the Operational Research Society},
  title                = {A visual interactive approach for scenario-based stochastic multi-objective problems and an application},
  doi                  = {10.1057/jors.2012.25},
  issn                 = {0160-5682},
  number               = {12},
  pages                = {11773-1787773--1787},
  volume               = {63},
  abstract             = {In many practical applications of stochastic programming, discretization of continuous random variables in the form of a scenario tree is required. In this paper, we deal with the randomness in scenario generation and present a visual interactive method for scenario-based stochastic multi-objective problems. The method relies on multi-variate statistical analysis of solutions obtained from a multi-objective stochastic problem to construct joint confidence regions for the objective function values. The decision maker (DM) explores desirable parts of the efficient frontier using a visual representation that depicts the trajectories of the objective function values within confidence bands. In this way, we communicate the effects of randomness inherent in the problem to the DM to help her understand the trade-offs and the levels of risk associated with each objective.},
  citeulike-article-id = {14171150},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jors.2012.25},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/01605682/2012/00000063/00000012/art00011},
  groups               = {Scenario_Best},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:38:49},
  publisher            = {Palgrave Macmillan},
  timestamp            = {2020-02-29 18:35},
}

@Article{AlJanabi-2014,
  author               = {Al Janabi, Mazin, A. M.},
  date                 = {2014-06},
  journaltitle         = {Economic Modelling},
  title                = {Optimal and investable portfolios: An empirical analysis with scenario optimization algorithms under crisis market prospects},
  doi                  = {10.1016/j.econmod.2013.11.021},
  issn                 = {0264-9993},
  pages                = {369--381},
  volume               = {40},
  abstract             = {We develop optimization algorithms for investable portfolios under crisis outlooks. Developed algorithms can aid in advancing portfolio management in financial markets. We test optimum and investable portfolios subject to meaningful financial constraints. Empirical results show that investable portfolios lie off the efficient frontier. Investable portfolios cannot be achieved via Markowitz's classical portfolio approach. This paper develops scenario optimization algorithms for the assessment of investable financial portfolios under crisis market outlooks.

To this end, this research study examines from portfolio managers' standpoint the performance of optimum and investable portfolios subject to applying meaningful financial and operational constraints as a result of a financial turmoil. Specifically, the paper tests a number of alternative scenarios considering both long-only and long and short-sales positions subject to minimizing the Liquidity-Adjusted Value-at-Risk (LVaR) and various financial and operational constraints such as target expected return, portfolio trading volume, close-out periods and portfolio weights. Robust optimization algorithms to set coherent asset allocations for investment management industries in emerging markets and particularly in Gulf Cooperation Council (GCC) financial markets are developed.

The results show that the obtained investable portfolios lie off the efficient frontier, but that long-only portfolios appear to lie much closer to the frontier than portfolios including both long and short-sales positions. The proposed optimization algorithms can be useful in developing enterprise-wide portfolio management models in light of the aftermaths of the most-recent financial crisis. The developed methodology and risk optimization algorithms can aid in advancing portfolio management practices in emerging markets and predominantly in the wake of the latest credit crunch.},
  citeulike-article-id = {13978406},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.econmod.2013.11.021},
  groups               = {Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-12 10:44:28},
  timestamp            = {2020-02-29 18:36},
}

@Article{AlJanabi-et-al-2017,
  author               = {Al Janabi, Mazin A. M. and Arreola Hernandez, Jose and Berger, Theo and Nguyen, Duc K.},
  date                 = {2017-06},
  journaltitle         = {European Journal of Operational Research},
  title                = {Multivariate dependence and portfolio optimization algorithms under illiquid market scenarios},
  doi                  = {10.1016/j.ejor.2016.11.019},
  issn                 = {0377-2217},
  number               = {3},
  pages                = {1121--1131},
  volume               = {259},
  abstract             = {An analytical framework for Liquidity-adjusted Value-at-Risk is introduced. We develop a model for optimizing structured portfolios with liquidity constraints. Dynamic copula is used to gauge the multivariate dependence between assets. Our approach gives better efficient frontiers than competing optimization models. We propose a model for optimizing structured portfolios with liquidity-adjusted Value-at-Risk (LVaR) constraints, whereby linear correlations between assets are replaced by the multivariate nonlinear dependence structure based on Dynamic conditional correlation t-copula modeling. Our portfolio optimization algorithm minimizes the LVaR function under adverse market circumstances and multiple operational and financial constraints. When considering a diversified portfolio of international stock and commodity market indices under multiple realistic portfolio optimization scenarios, the obtained results consistently show the superiority of our approach, relative to other competing portfolio strategies including the minimum-variance, risk-parity and equally weighted portfolio allocations.},
  citeulike-article-id = {14336726},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2016.11.019},
  groups               = {Scenario_Portfolio},
  posted-at            = {2017-04-13 11:02:12},
  timestamp            = {2020-02-29 18:36},
}

@Article{Bernard-et-al-2012,
  author       = {Bernard, Carole and Chen, Jit Seng and Vanduffel, Steven},
  date         = {2012},
  journaltitle = {SSRN e-Print},
  title        = {Optimal Portfolios Under Worst-Case Scenarios},
  abstract     = {In standard portfolio theories such as Mean-Variance optimization, expected utility theory, rank dependent utility heory, Yaari's dual theory and cumulative prospect theory, the worst outcomes for optimal strategies occur when the market declines (e.g. during crises), which is at odds with the needs of many investors. Hence, we depart from the traditional settings and study optimal strategies for investors who impose additional constraints on their final wealth in the states corresponding to a stressed financial market.

We provide a framework that maintains the stylized features of the SP/A theory while dealing with the goal of security in a more flexible way. Preferences become state-dependent, and we assess the impact of these preferences on trading decisions. We construct optimal strategies explicitly and show how they outperform traditional diversified strategies under worst-case scenarios.},
  groups       = {Scenario_Portfolio},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2349120},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:36},
}

@Article{Beyer-et-al-2014,
  author               = {Beyer, Hans-Georg and Finck, Steffen and Breuer, Thomas},
  date                 = {2014-08},
  journaltitle         = {Swarm and Evolutionary Computation},
  title                = {Evolution on trees: On the design of an evolution strategy for scenario-based multi-period portfolio optimization under transaction costs},
  doi                  = {10.1016/j.swevo.2014.03.002},
  issn                 = {2210-6502},
  pages                = {74--87},
  volume               = {17},
  abstract             = {Scenario-based optimization is a problem class often occurring in finance, planning and control. While the standard approach is usually based on linear stochastic programming, this paper develops an Evolution Strategy (ES) that can be used to treat nonlinear planning problems arising from Value at Risk (VaR)-constraints and not necessarily proportional transaction costs. Due to the VaR-constraints the optimization problem is generally of non-convex type and its decision version is already NP-complete. The developed ES is the first algorithm in the field of evolutionary and swarm intelligence that tackles this kind of optimization problem. The algorithm design is based on the covariance matrix self-adaptation ES (CMSA-ES). The optimization is performed on scenario trees where in each node specific constraints (balance equations) must be fulfilled. In order to evaluate the performance of the ES proposed, instances of increasing problem hardness are considered. The application to the general case with nonlinear node constraints shows not only the potential of the ES designed, but also its limitations. The latter are basically determined by the high dimensionalities of the search spaces defined by the scenario trees.},
  citeulike-article-id = {14160230},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.swevo.2014.03.002},
  groups               = {Scenario_Portfolio},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 18:12:01},
  timestamp            = {2020-02-29 18:36},
}

@InCollection{Calafiore-2016,
  author               = {Calafiore, GiuseppeCarlo},
  booktitle            = {Optimal Financial Decision Making under Uncertainty},
  date                 = {2016},
  title                = {Scenario Optimization Methods in Portfolio Analysis and Design},
  doi                  = {10.1007/978-3-319-41613-7\_3},
  editor               = {Consigli, Giorgio and Kuhn, Daniel and Brandimarte, Paolo},
  pages                = {55--87},
  publisher            = {Springer International Publishing},
  volume               = {245},
  abstract             = {This chapter discusses techniques for analysis and optimization of portfolio statistics, based on direct use of samples of random data. For a given and fixed portfolio of financial assets, a classical approach for evaluating, say, the value-at-risk (V@R) of the portfolio is a model-based one, whereby one first assumes some stochastic model for the component returns (e.g., Normal), then estimates the parameters of this model from data, and finally computes the portfolio V@R. Such a process hinges upon critical assumptions (e.g., the elicited return distribution), and leaves unclear the effects of model estimation errors on the computed quantity of interest. Here, we propose an alternative direct route that bypasses the assumption and estimation of a model for the returns, and provides the estimated quantity of interest (together with its out-of-sample reliability tag) directly from data generated by a scenario generation oracle. This idea is then extended to the situation where one simultaneously optimizes over the portfolio composition, in order to achieve an optimal portfolio with a guaranteed level of expected shortfall probability. Such a scenario-based portfolio design approach is here developed for both single-period and multi-period allocation problems. The methodology underpinning the proposed computational method is that of random convex programming (RCP). Besides the described data-driven problems, we show in this chapter that the RCP paradigm can also be employed alongside more standard mean-variance portfolio optimization settings, in the presence of ambiguity in the statistical model of the returns, providing a viable technique to address robust portfolio optimization problems.},
  citeulike-article-id = {14180487},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-41613-73},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-41613-73},
  groups               = {PortfOptim_Scenario, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-11-10 14:32:33},
  timestamp            = {2020-02-29 18:36},
  year                 = {2016},
}

@Article{Consiglio-et-al-2007,
  author               = {Consiglio, Andrea and Cocco, Flavio and Zenios, StavrosA},
  date                 = {2006-07},
  journaltitle         = {Annals of Operations Research},
  title                = {Scenario optimization asset and liability modelling for individual investors},
  doi                  = {10.1007/s10479-006-0133-5},
  issn                 = {0254-5330},
  number               = {1},
  pages                = {167--191},
  volume               = {152},
  abstract             = {We develop a scenario optimization model for asset and liability management of individual investors. The individual has a given level of initial wealth and a target goal to be reached within some time horizon. The individual must determine an asset allocation strategy so that the portfolio growth rate will be sufficient to reach the target. A scenario optimization model is formulated which maximizes the upside potential of the portfolio, with limits on the downside risk. Both upside and downside are measured vis-a-vis the goal. The stochastic behavior of asset returns is captured through bootstrap simulation, and the simulation is embedded in the model to determine the optimal portfolio. Post-optimality analysis using out-of-sample scenarios measures the probability of success of a given portfolio. It also allows us to estimate the required increase in the initial endowment so that the probability of success is improved.},
  citeulike-article-id = {1194522},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10479-006-0133-5},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/klu/anor/2007/00000152/00000001/00000133},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1007/s10479-006-0133-5},
  groups               = {Scenario generation, ALM_Individual, Scenario_Portfolio, Scenario_Risk},
  journal              = {Annals of Operations Research},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:17:07},
  publisher            = {Kluwer Academic Publishers-Plenum Publishers},
  timestamp            = {2020-02-29 18:36},
  year                 = {2007},
}

@InCollection{Better-Glover-2007,
  author               = {Better, Marco and Glover, Fred},
  booktitle            = {Encyclopedia of Quantitative Risk Analysis and Assessment},
  date                 = {2007},
  title                = {Scenario-Based Risk Management and Simulation Optimization},
  doi                  = {10.1002/9780470061596.risk0091},
  publisher            = {John Wiley and Sons, Ltd},
  abstract             = {Traditional optimization approaches for handling uncertainty and risk typically require severe assumptions that are often not satisfied in complex practical settings. In an effort to overcome such limitations, several methods have been developed to handle uncertainty when the data and associated real-world parameters do not behave according to classical assumptions. Two of the leading and most widely used examples are scenario optimization and robust optimization, both of which seek high-quality decisions that are feasible for all scenarios. However, both of these approaches likewise succumb to deficiencies encountered by classical methods, by exhibiting serious limitations in terms of the complexity and size of the models they can handle. Simulation optimization overcomes these limitations, and its flexibility and ease of use has contributed to its popularity as a preferred optimization approach to risk-management applications.},
  citeulike-article-id = {14177393},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/9780470061596.risk0091},
  groups               = {Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-11-04 21:11:32},
  timestamp            = {2020-02-29 18:36},
}

@Article{Consiglio-et-al-2016,
  author               = {Consiglio, Andrea and Carollo, Angelo and Zenios, Stavros A.},
  date                 = {2016-02},
  journaltitle         = {Quantitative Finance},
  title                = {A parsimonious model for generating arbitrage-free scenario trees},
  doi                  = {10.1080/14697688.2015.1114359},
  number               = {2},
  pages                = {201--212},
  volume               = {16},
  abstract             = {Simulation models of economic, financial and business risk factors are widely used to assess risks and support decision-making. Extensive literature on scenario generation methods aims at describing some underlying stochastic processes with the least number of scenarios to overcome the ?curse of dimensionality?. There is, however, an important requirement that is usually overlooked when one departs from the application domain of security pricing: the no-arbitrage condition. We formulate a moment matching model to generate multi-factor scenario trees for stochastic optimization satisfying no-arbitrage restrictions with a minimal number of scenarios and without any distributional assumptions. The resulting global optimization problem is quite general. However, it is non-convex and can grow significantly with the number of risk factors, and we develop convex lower bounding techniques for its solution exploiting the special structure of the problem. Applications to some standard problems from the literature show that this is a robust approach for tree generation. We use it to price a European basket option in complete and incomplete markets.},
  citeulike-article-id = {13933166},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1114359},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1114359},
  day                  = {1},
  groups               = {Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:08:42},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 18:36},
}

@Article{Abdymomunov-Gerlach-2014,
  author               = {Abdymomunov, Azamat and Gerlach, Jeffrey},
  date                 = {2014-12},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Stress testing interest rate risk exposure},
  doi                  = {10.1016/j.jbankfin.2014.08.013},
  issn                 = {0378-4266},
  pages                = {287--301},
  volume               = {49},
  abstract             = {In the current low interest rate environment, the possibility of a sudden increase in rates is a potentially serious threat to financial stability. As a result, analyzing interest rate risk (IRR) is critical for financial institutions and supervisory agencies. We propose a new method for generating yield-curve scenarios for stress testing banks' exposure to IRR based on the Nelson-Siegel (1987) yield-curve model. We show that our method produces yield-curve scenarios with a wider variety of slopes and shapes than scenarios generated by the historical and hypothetical methods typically used in the banking industry and proposed in the literature. We stress test the economic value of equity of a bank balance sheet based on Call Report data from a large U.S. bank. We show that our method provides more information about the bank's exposure to IRR using fewer yield-curve scenarios than the alternative historical and hypothetical methods.},
  citeulike-article-id = {14087947},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2014.08.013},
  groups               = {Test_Scenario, Risk_Stress, Scenario_YieldCurve},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:29:31},
  timestamp            = {2020-02-29 18:36},
}

@Article{Chen-et-al-2017a,
  author               = {Chen, Jingnan and Flood, Mark D. and Sowers, Richard B.},
  date                 = {2017-04},
  journaltitle         = {Quantitative Finance},
  title                = {Measuring the unmeasurable: an application of uncertainty quantification to Treasury bond portfolios},
  doi                  = {10.1080/14697688.2017.1296176},
  pages                = {1--17},
  abstract             = {We extract from the yield curve a new measure of fundamental economic uncertainty, based on McDiarmid?s diameter and related methods for optimal uncertainty quantification (OUQ). OUQ seeks analytical bounds on a system?s behaviour, even where aspects of the underlying data-generating process and system response function are not completely known. We use OUQ to stress test a simple fixed-income portfolio, certifying its safety?i.e. that potential losses will be ?small? in an appropriate sense. The results give explicit tradeoffs between: scenario count, maximum loss, test horizon, and confidence level. Unfortunately, uncertainty peaks in late 2008, weakening certification assurances just when they are needed most.},
  citeulike-article-id = {14338564},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2017.1296176},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2017.1296176},
  day                  = {3},
  groups               = {Scenario_YieldCurve},
  posted-at            = {2017-04-17 11:37:58},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 18:36},
}

@Article{Flood-Korenko-2015,
  author       = {Mark D. Flood and George G. Korenko},
  date         = {2015},
  journaltitle = {Quantitative Finance},
  title        = {Systematic scenario selection: stress testing and the nature of uncertainty},
  doi          = {10.1080/14697688.2014.926018},
  number       = {1},
  pages        = {43--59},
  url          = {https://www.tandfonline.com/doi/abs/10.1080/14697688.2014.926018},
  volume       = {15},
  abstract     = {We present a technique for selecting multidimensional shock scenarios for use in financial stress testing. The methodology systematically enforces internal consistency among the shock dimensions by sampling points of arbitrary severity from a plausible joint probability distribution. The approach involves a grid search of sparse, well distributed, stress-test scenarios, which we regard as a middle ground between traditional stress testing and reverse stress testing.

Choosing scenarios in this way reduces the danger of blind spots' in stress testing. We suggest extensions to address the issues of non-monotonic loss functions and univariate shocks. We provide tested and commented source code in Matlab},
  groups       = {Test_Scenario, StressTest_Reverse},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:39},
}

@Article{Giacometti-Mignacca-2010,
  author               = {Giacometti, Rosella and Mignacca, Domenico},
  date                 = {2010-10},
  journaltitle         = {Journal of Asset Management},
  title                = {Using the Black and Litterman framework for stress test analysis in asset management},
  doi                  = {10.1057/jam.2009.33},
  issn                 = {1470-8272},
  number               = {4},
  pages                = {286--297},
  volume               = {11},
  abstract             = {In the classic Black and Litterman approach, it is possible, using reverse engineering, to obtain the expected asset equilibrium returns implied by the weights of the market portfolio, that is, the benchmark. However, given that analysts may have different views on some of the expected returns implied by the benchmark weights, it is possible to obtain the posterior distribution by combining analysts' views and prior market information. In this article, we propose a methodology for the stress test analysis of a current managed portfolio, in which two different shock types are combined.

More precisely:

we shock a set of factors that affect asset returns, imposing the analysts' views on any variation from the expected levels;

we assume that a mixture of normal distributions describe the presence of hectic periods and quiet periods.

The asset correlation breakdown effect is well known that is ' joint distributions estimated over periods without panics will misestimate the degree of correlation between asset returns during panics' (Alan Greenspan, 1999). To this end, we introduce a number of macroeconomic factors that affect asset returns, such as volatilities, interest rates, oil price and so on. At this stage, a multi-factor analysis is not carried out, although we include the information in the covariance matrix.

We assume that a mixture of normal distributions is well suited to describing and representing the presence of high- and low-volatility periods, taking into account extreme movements in the market. We derive the conditional moments of the posterior distribution by combining views on factors and market information.},
  citeulike-article-id = {8027802},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2009.33},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2010/00000011/00000004/art00005},
  groups               = {Black_Litterman, StressTest_Reverse},
  owner                = {cristi},
  posted-at            = {2016-03-06 02:58:03},
  publisher            = {Palgrave Macmillan},
  timestamp            = {2020-02-29 18:39},
}

@Article{Glasserman-et-al-2015,
  author       = {Paul Glasserman and Chulmin Kang and Wanmo Kang},
  date         = {2015},
  journaltitle = {Quantitative Finance},
  title        = {Stress scenario selection by empirical likelihood},
  doi          = {10.1080/14697688.2014.926019},
  number       = {1},
  pages        = {25--41},
  url          = {https://www.tandfonline.com/doi/abs/10.1080/14697688.2014.926019},
  volume       = {15},
  abstract     = {This paper develops a method for selecting and analysing stress scenarios for financial risk assessment, with particular emphasis on identifying sensible combinations of stresses to multiple factors. We focus primarily on reverse stress testing - finding the most likely scenarios leading to losses exceeding a given threshold. We approach this problem using a nonparametric empirical likelihood estimator of the conditional mean of the underlying market factors given large losses. We then scale confidence regions for the conditional mean by a coefficient that depends on the tails of the market factors to estimate the most likely loss scenarios.

We provide rigorous justification for the confidence regions and the scaling procedure when the joint distribution of the market factors and portfolio loss is elliptically contoured. We explicitly characterize the impact of the heaviness of the tails of the distribution, contrasting a broad spectrum of cases including exponential tails and regularly varying tails. The key to this analysis lies in the asymptotics of the conditional variances and covariances in extremes.

These results also lead to asymptotics for marginal expected shortfall and the corresponding variance, conditional on a market stress; we combine these results with empirical likelihood significance tests of systemic risk rankings based on marginal expected shortfall in stress scenarios.},
  groups       = {Scenario generation, Estim_MaxLikelihood, Scenario_Market, Scenario_SubsetFactor, StressTest_Reverse},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:39},
}

@Article{Grundke-2011,
  author       = {Peter Grundke},
  date         = {2011},
  journaltitle = {Journal of Risk Model Validation},
  title        = {Reverse stress tests with bottom-up approaches},
  doi          = {10.21314/JRMV.2011.068},
  number       = {1},
  url          = {https://www.risk.net/journal-risk-model-validation/2161312/reverse-stress-tests-bottom-approaches},
  volume       = {5},
  abstract     = {This paper contributes to the sparse literature on reverse stress testing, the necessity of which is emphasized by banks' supervisory authorities. While, for regular stress tests, scenarios are chosen based on historical experience or expert knowledge and their influence on the bank's survivability is tested, reverse stress tests aim to find exactly those scenarios that cause the bank to cross the frontier between survival and default. Afterward, the most likely of these scenarios has to be found.

We argue that bottom-up approaches, as specific integrated risk management techniques, are ideal candidates for carrying out quantitative reverse stress tests because they model interactions between different risk types already on the level of the individual financial instruments and risk factors. This is exemplified with an extended CreditMetrics model that exhibits correlated interest rates and rating-specific credit-spread risk},
  groups       = {Test_Scenario, StressTest_Reverse},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:39},
}

@Article{Grundke-2012,
  author       = {Peter Grundke},
  date         = {2012},
  journaltitle = {Journal of Risk Model Validation},
  title        = {Further recipes for quantitative reverse stress testing},
  doi          = {10.21314/JRMV.2012.089},
  number       = {2},
  url          = {https://www.risk.net/journal-of-risk-model-validation/2186770/further-recipes-for-quantitative-reverse-stress-testing},
  volume       = {6},
  abstract     = {As a consequence of the 2007-9 financial crisis, the regulatory authorities now oblige banks to carry out reverse stress tests. This new instrument in the stress test toolbox aims to find those scenarios that cause a bank to cross the frontier between survival and default. Afterward, the scenario that is most likely to occur has to be identified.

In a 2011 paper by Grundke it is argued that bottom-up approaches, which are a specific integrated risk measurement technique, are basically well suited to solving the inversion problem inherent in a reverse stress test and, in particular, for computing the probabilities of reverse stress test scenarios while taking existing risk dependencies into account.

Building upon the earlier work of Grundke, this paper shows how the modeling framework previously presented can be extended to incorporate more real-world features such as a time-varying credit quality of the bank or contagion effects. The consequences on the results of the reverse stress test are analyzed for each modification.},
  groups       = {StressTest_Reverse},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:39},
}

@Book{Danielsson-2011,
  author       = {Danielsson, J},
  date         = {2011},
  title        = {Financial Risk Forecasting: The Theory and Practice of Forecasting Market Risk with Implementation in R and Matlab},
  publisher    = {Wiley},
  url          = {https://onlinelibrary.wiley.com/doi/book/10.1002/9781119205869},
  abstract     = {Financial Risk Forecasting is a complete introduction to practical quantitative risk management, with a focus on market risk. Derived from the authors teaching notes and years spent training practitioners in risk management techniques, it brings together the three key disciplines of finance, statistics and modeling (programming), to provide a thorough grounding in risk management techniques.

Written by renowned risk expert Jon Danielsson, the book begins with an introduction to financial markets and market prices, volatility clusters, fat tails and nonlinear dependence. It then goes on to present volatility forecasting with both univariate and multivariate methods, discussing the various methods used by industry, with a special focus on the GARCH family of models. The evaluation of the quality of forecasts is discussed in detail. Next, the main concepts in risk and models to forecast risk are discussed, especially volatility, value-at-risk and expected shortfall.

The focus is both on risk in basic assets such as stocks and foreign exchange, but also calculations of risk in bonds and options, with analytical methods such as delta-normal VaR and duration-normal VaR and Monte Carlo simulation. The book then moves on to the evaluation of risk models with methods like backtesting, followed by a discussion on stress testing. The book concludes by focussing on the forecasting of risk in very large and uncommon events with extreme value theory and considering the underlying assumptions behind almost every risk model in practical use - that risk is exogenous - and what happens when those assumptions are violated.

Every method presented brings together theoretical discussion and derivation of key equations and a discussion of issues in practical implementation. Each method is implemented in both MATLAB and R, two of the most commonly used mathematical programming languages for risk forecasting with which the reader can implement the models illustrated in the book.

The book includes four appendices. The first introduces basic concepts in statistics and financial time series referred to throughout the book. The second and third introduce R and MATLAB, providing a discussion of the basic implementation of the software packages. And the final looks at the concept of maximum likelihood, especially issues in implementation and testing.

The book is accompanied by a website - www.financialriskforecasting.com - which features downloadable code as used in the book.},
  groups       = {StressTest_ExpertView, FrcstQWIM_ShortTerm, FrcstQWIM_Other},
  howpublished = {Available at http://www.financialriskforecasting.com/},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:39},
}

@Article{Davis-Lleo-2015,
  author               = {Davis, Mark and Lleo, Sebastien},
  date                 = {2015-09},
  journaltitle         = {SSRN e-Print},
  title                = {Behaviouralizing Black-Litterman: Expert Opinions and Behavioural Biases in a Diffusion Setting},
  url                  = {https://ssrn.com/abstract=2663650},
  abstract             = {This paper proposes a continuous time version of the Black-Litterman model that accounts for, and corrects, some of the main behavioural biases that analysts may exhibit. We formulate the model as a stochastic control problem under partial observations, and derive the optimal investment strategy and value function in closed form. We implement this model with three partially observable state variables corresponding to the three factors of the Fama-French model, and fourteen sources of observations: market data from eleven ETFs, views from two analysts, and one stress test scenario. With this example, we show concretely how to calibrate analyst views and mitigate the impact of behavioural biases. To explore the effect that views and biases have on the asset allocation, we compare the results of six dynamic investment models. We find that the views have a modest impact on the Kelly portfolio, but the confidence intervals around the views have a large impact on the intertemporal hedging portfolio.},
  citeulike-article-id = {13997078},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2663650},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2691993code1677436.pdf?abstractid=2663650 and mirid=1},
  day                  = {22},
  groups               = {Black_Litterman, Scenario_ExpertView, StressTest_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-04-04 17:47:35},
  timestamp            = {2020-02-29 18:39},
}

@Article{Durante-et-al-2013,
  author               = {Durante, Fabrizio and Pappada, Roberta and Torelli, Nicola},
  date                 = {2013-12},
  journaltitle         = {Advances in Data Analysis and Classification},
  title                = {Clustering of financial time series in risky scenarios},
  doi                  = {10.1007/s11634-013-0160-4},
  issn                 = {1862-5347},
  number               = {4},
  pages                = {359--376},
  volume               = {8},
  abstract             = {A methodology is presented for clustering financial time series according to the association in the tail of their distribution. The procedure is based on the calculation of suitable pairwise conditional Spearman's correlation coefficients extracted from the series. The performance of the method has been tested via a simulation study. As an illustration, an analysis of the components of the Italian FTSE-MIB is presented. The results could be applied to construct financial portfolios that can manage to reduce the risk in case of simultaneous large losses in several markets.},
  citeulike-article-id = {14150074},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11634-013-0160-4},
  day                  = {22},
  groups               = {Networks and investment management, Scenario generation, Scenario_Market, Scenario_TimeSeries},
  owner                = {cristi},
  posted-at            = {2016-10-01 20:45:04},
  timestamp            = {2020-02-29 18:40},
}

@Article{Guastroba-et-al-2009,
  author       = {Guastroba, G. and Mansini, R. and Speranza, M. G.},
  date         = {2009},
  journaltitle = {European Journal of Operational Research},
  title        = {On the effectiveness of scenario generation techniques in single-period portfolio optimization},
  pages        = {500--511},
  abstract     = {In single-period portfolio selection problems the expected value of both the risk measure and the portfolio return have to be estimated. Historical data realizations, used as equally probable scenarios, are frequently used to this aim. Several other parametric and non-parametric methods can be applied. When dealing with scenario generation techniques practitioners are mainly concerned on how reliable and effective such methods are when embedded into portfolio selection models.

In this paper we survey different techniques to generate scenarios for the rates of return. We also compare the techniques by providing in-sample and out-of-sample analysis of the portfolios obtained by using these techniques to generate the rates of return. Evidence on the computational burden required by the different techniques is also provided. As reference model we use the Worst Conditional Expectation model with transaction costs.

Extensive computational results based on different historical data sets from London Stock Exchange Market (FTSE) are presented and some interesting financial conclusions are drawn.},
  groups       = {Scenario generation, Scenario_Market},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:40},
}

@Article{Davis-Lleo-2016,
  author               = {Davis, Mark H. A. and Lleo, Sebastien},
  date                 = {2016-07},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {A Simple Procedure for Combining Expert Opinion with Statistical Estimates to Achieve Superior Portfolio Performance},
  doi                  = {10.3905/jpm.2016.42.4.049},
  issn                 = {0095-4918},
  number               = {4},
  pages                = {49--58},
  volume               = {42},
  abstract             = {In this article, the authors describe a simple procedure for combining statistical estimates with expert opinions to produce a view of future asset performance. The authors discuss the impact of behavioral bias on these views and propose general modeling principles to reduce this bias. They use standard linear filtering techniques to combine statistical estimates with expert opinions seamlessly and discuss applications to dynamic portfolio optimization.},
  citeulike-article-id = {14119431},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2016.42.4.049},
  groups               = {Scenario_ExpertView, Indic_FuturePerf},
  owner                = {zkgst0c},
  posted-at            = {2016-08-21 11:58:32},
  timestamp            = {2020-02-29 18:40},
}

@Article{Feuerriegel-Gordon-2018a,
  author         = {Feuerriegel, Stefan and Gordon, Julius},
  date           = {2018-01-22},
  journaltitle   = {European Journal of Operational Research},
  title          = {News-based forecasts of macroeconomic indicators: A semantic path model for interpretable predictions},
  doi            = {10.1016/j.ejor.2018.05.068},
  url            = {https://www.sciencedirect.com/science/article/abs/pii/S0377221718304879},
  abstract       = {The macroeconomic climate influences operations with regard to, e.g., raw material prices, financing, supply chain utilization and demand quotas. In order to adapt to the economic environment, decision-makers across the public and private sectors require accurate forecasts of the economic outlook. Existing predictive frameworks base their forecasts primarily on time series analysis, as well as the judgments of experts. As a consequence, current approaches are often biased and prone to error. In order to reduce forecast errors, this paper presents an innovative methodology that extends lag variables with unstructured data in the form of financial news: (1) we apply a variety of models from machine learning to word counts as a high-dimensional input. However, this approach suffers from low interpretability and overfitting, motivating the following remedies. (2) We follow the intuition that the economic climate is driven by general sentiments and suggest a projection of words onto latent semantic structures as a means of feature engineering. (3) We propose a semantic path model, together with estimation technique based on regularization, in order to yield full interpretability of the forecasts. We demonstrate the predictive performance of our approach by utilizing 80,813 ad hoc announcements in order to make long-term forecasts of up to 24 months ahead regarding key macroeconomic indicators. Back-testing reveals a considerable reduction in forecast errors.},
  day            = {22},
  f1000-projects = {QuantInvest},
  groups         = {ML_Interpretability, Scenario_ExpertView, ML_Overfitting, ML_ForcstTimeSrs, ML_Text_QWIM},
  timestamp      = {2020-02-29 18:40},
}

@Article{Franses-deBruijn-2017,
  author               = {Franses, Philip H. and de Bruijn, Bert},
  date                 = {2017-01},
  journaltitle         = {International Journal of Finance and Economics},
  title                = {Benchmarking Judgmentally Adjusted Forecasts},
  doi                  = {10.1002/ijfe.1569},
  issn                 = {1076-9307},
  number               = {1},
  pages                = {3--11},
  volume               = {22},
  abstract             = {Many publicly available macroeconomic forecasts are judgmentally adjusted model-based forecasts. In practice, usually only a single final forecast is available, and not the underlying econometric model, nor are the size and reason for adjustment known. Hence, the relative weights given to the model forecasts and to the judgement are usually unknown to the analyst.

This paper proposes a methodology to evaluate the quality of such final forecasts, also to allow learning from past errors. To do so, the analyst needs benchmark forecasts. We propose two such benchmarks. The first is the simple no-change forecast, which is the bottom line forecast that an expert should be able to improve. The second benchmark is an estimated model-based forecast, which is found as the best forecast given the realizations and the final forecasts. We illustrate this methodology for two sets of GDP growth forecasts, one for the USA and one for the Netherlands. These applications tell us that adjustment appears most effective in periods of first recovery from a recession.},
  citeulike-article-id = {14364661},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/ijfe.1569},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-05-29 18:49:12},
  timestamp            = {2020-02-29 18:40},
}

@Article{Gronau-et-al-2017,
  author         = {Gronau, Quentin F. and Ly, Alexander and Wagenmakers, Eric-Jan},
  date           = {2017-04-08},
  journaltitle   = {arXiv e-Print},
  title          = {Informed Bayesian T-Tests},
  url            = {https://arxiv.org/abs/1704.02479},
  abstract       = {Across the empirical sciences, few statistical procedures rival the popularity of the frequentist t-test. In contrast, the Bayesian versions of the t-test have languished in obscurity. In recent years, however, the theoretical and practical advantages of the Bayesian t-test have become increasingly apparent. Various objective Bayesian t-tests have been proposed based on certain desiderata as well as subjective ones that enable incorporation of expert knowledge. Here we propose to use a flexible t-prior for the standardized effect size which allows computation of the Bayes factor by solving a single numerical integral. This specification contains previous subjective and objective t-test Bayes factors as special cases and thereby bridges objective and subjective approaches. Furthermore, we propose two measures that quantify the departure from the objective Bayes factor desiderata of predictive matching and information consistency for informed prior distributions. We illustrate the practical value of informed prior distributions based on an expert prior elicitation effort and a reanalysis of 593 published t-tests.},
  day            = {8},
  f1000-projects = {QuantInvest},
  groups         = {Proba_Freq, Stat_Test, Scenario_ExpertView},
  timestamp      = {2020-02-29 18:40},
}

@Article{GrushkaCockayne-et-al-2017,
  author               = {Grushka-Cockayne, Yael and Jose, Victor R. and Lichtendahl, Kenneth C.},
  date                 = {2017-04},
  journaltitle         = {Management Science},
  title                = {Ensembles of Overfit and Overconfident Forecasts},
  doi                  = {10.1287/mnsc.2015.2389},
  issn                 = {0025-1909},
  number               = {4},
  pages                = {1110--1130},
  volume               = {63},
  abstract             = {Firms today average forecasts collected from multiple experts and models. Because of cognitive biases, strategic incentives, or the structure of machine-learning algorithms, these forecasts are often overfit to sample data and are overconfident. Little is known about the challenges associated with aggregating such forecasts. We introduce a theoretical model to examine the combined effect of overfitting and overconfidence on the average forecast. Their combined effect is that the mean and median probability forecasts are poorly calibrated with hit rates of their prediction intervals too high and too low, respectively. Consequently, we prescribe the use of a trimmed average, or trimmed opinion pool, to achieve better calibration. We identify the random forest, a leading machine-learning algorithm that pools hundreds of overfit and overconfident regression trees, as an ideal environment for trimming probabilities. Using several known data sets, we demonstrate that trimmed ensembles can significantly improve the random forest's predictive accuracy.},
  citeulike-article-id = {14332498},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/mnsc.2015.2389},
  groups               = {Scenario_ExpertView, ML_Test_OOS, ML_Overfitting},
  posted-at            = {2017-04-05 20:20:24},
  timestamp            = {2020-02-29 18:40},
}

@Article{Gzyl-et-al-2017,
  author         = {Gzyl, Henryk and ter Horst, Enrique and Molina, German},
  date           = {2017-03},
  journaltitle   = {Applied Mathematical Modelling},
  title          = {Inferring probability densities from expert opinion},
  doi            = {10.1016/j.apm.2016.11.006},
  issn           = {0307-904X},
  pages          = {306--320},
  volume         = {43},
  abstract       = {When experts are asked to assess a situation, they often express their opinions providing estimates of the probability of observing the occurrence of a random variable in given intervals, sometimes up to a range of values, rather than simply providing point estimates. The problem we face is how to translate that expert opinion into probability distributions. We examine a novel way of solving that problem by making use of the maximum entropy method in the data to deal with expert opinions expressed with or without uncertainty bands. Our method allows us to unveil underlying probability distributions driving expert opinions expressed with and without uncertainty.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:40},
}

@Article{Hackethal-et-al-2018,
  author         = {Hackethal, Andreas and Laudenbach, Christine and Meyer, Steffen and Weber, Annika},
  date           = {2018-05-17},
  journaltitle   = {SSRN e-Print},
  title          = {Client Involvement in Expert Advice: Antibiotics in Finance?},
  url            = {https://ssrn.com/abstract=3178664},
  abstract       = {We use minutes from 17,000 financial advisory sessions and corresponding client portfolio data to study how active client involvement affects advisor recommendations and portfolio outcomes. We find that advisors confronted with acquiescent clients stick to their standards and recommend expensive but well diversified mutual fund portfolios. However, if clients take an active role in the meetings, advisors deviate markedly from their standards, resulting in poorer portfolio diversification and lower Sharpe ratios. Our findings that advisors cater to client requests parallel the phenomenon of doctors prescribing antibiotics to insistent patients even if inappropriate, and imply that pandering diminishes the quality of advice.},
  day            = {17},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:40},
}

@Article{Hsu-et-al-2016b,
  author               = {Hsu, Ming-Wei and Lessmann, Stefan and Sung, Ming-Chien and Ma, Tiejun and Johnson, Johnnie E. V.},
  date                 = {2016-11},
  journaltitle         = {Expert Systems with Applications},
  title                = {Bridging the divide in financial market forecasting: machine learners vs. financial economists},
  doi                  = {10.1016/j.eswa.2016.05.033},
  issn                 = {0957-4174},
  pages                = {215--234},
  volume               = {61},
  abstract             = {An extensive benchmark in financial time series forecasting is performed. Best machine learning(ML) methods out-perform best econometric methods. The ML methodology employed significantly affects forecasting accuracy. Market maturity, forecast horizon and model-assessment method affect forecast accuracy. Evidence against the informational value of technical indicators. Financial time series forecasting is a popular application of machine learning methods. Previous studies report that advanced forecasting methods predict price changes in financial markets with high accuracy and that profit can be made trading on these predictions. However, financial economists point to the informational efficiency of financial markets, which questions price predictability and opportunities for profitable trading. The objective of the paper is to resolve this contradiction. To this end, we undertake an extensive forecasting simulation, based on data from thirty-four financial indices over six years. These simulations confirm that the best machine learning methods produce more accurate forecasts than the best econometric methods. We also examine the methodological factors that impact the predictive accuracy of machine learning forecasting experiments. The results suggest that the predictability of a financial market and the feasibility of profitable model-based trading are significantly influenced by the maturity of the market, the forecasting method employed, the horizon for which it generates predictions and the methodology used to assess the model and simulate model-based trading. We also find evidence against the informational value of indicators from the field of technical analysis. Overall, we confirm that advanced forecasting methods can be used to predict price changes in some financial markets and we discuss whether these results question the prevailing view in the financial economics literature that financial markets are efficient.},
  citeulike-article-id = {14070589},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.eswa.2016.05.033},
  groups               = {Scenario_ExpertView, ML_ForcstTimeSrs},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:16:26},
  timestamp            = {2020-02-29 18:40},
}

@Article{Husic-Pande-2018,
  author         = {Husic, Brooke E. and Pande, Vijay S.},
  date           = {2018-02-21},
  journaltitle   = {Journal of the American Chemical Society},
  title          = {Markov state models: from an art to a science.},
  doi            = {10.1021/jacs.7b12191},
  number         = {7},
  pages          = {2386--2396},
  volume         = {140},
  abstract       = {Markov state models (MSMs) are a powerful framework for analyzing dynamical systems, such as molecular dynamics (MD) simulations, that have gained widespread use over the past several decades. This perspective offers an overview of the MSM field to date, presented for a general audience as a timeline of key developments in the field. We sequentially address early studies that motivated the method, canonical papers that established the use of MSMs for MD analysis, and subsequent advances in software and analysis protocols. The derivation of a variational principle for MSMs in 2013 signified a turning point from expertise-driving MSM building to a systematic, objective protocol. The variational approach, combined with best practices for model selection and open-source software, enabled a wide range of MSM analysis for applications such as protein folding and allostery, ligand binding, and protein-protein association. To conclude, the current frontiers of methods development are highlighted, as well as exciting applications in experimental design and drug discovery.},
  day            = {21},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  pmid           = {29323881},
  timestamp      = {2020-02-29 18:40},
}

@Article{Dantas-Oliveira-2018,
  author         = {Dantas, Tiago Mendes and Oliveira, Fernando Luiz Cyrino},
  date           = {2018-10},
  journaltitle   = {International Journal of Forecasting},
  title          = {Improving time series forecasting: An approach combining bootstrap aggregation, clusters and exponential smoothing},
  doi            = {10.1016/j.ijforecast.2018.05.006},
  issn           = {0169-2070},
  number         = {4},
  pages          = {748--761},
  volume         = {34},
  abstract       = {Some recent papers have demonstrated that combining bagging (bootstrap aggregating) with exponential smoothing methods can produce highly accurate forecasts and improve the forecast accuracy relative to traditional methods. We therefore propose a new approach that combines the bagging, exponential smoothing and clustering methods. The existing methods use bagging to generate and aggregate groups of forecasts in order to reduce the variance. However, none of them consider the effect of covariance among the group of forecasts, even though it could have a dramatic impact on the variance of the group, and therefore on the forecast accuracy. The proposed approach, referred to here as Bagged Cluster ETS, aims to reduce the covariance effect by using partitioning around medoids (PAM) to produce clusters of similar forecasts, then selecting several forecasts from each cluster to create a group with a reduced variance. This approach was tested on various different time series sets from the M3 and CIF 2016 competitions. The empirical results have shown a substantial reduction in the forecast error, considering sMAPE and MASE.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_TimeSrs, Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:41},
}

@Article{Dette-Wied-2015,
  author               = {Dette, Holger and Wied, Dominik},
  date                 = {2016-03},
  journaltitle         = {Journal of the Royal Statistical Society: Series B (Statistical Methodology)},
  title                = {Detecting relevant changes in time series models},
  doi                  = {10.1111/rssb.12121},
  issn                 = {1369-7412},
  number               = {2},
  pages                = {371--394},
  volume               = {78},
  abstract             = {Most of the literature on change point analysis by means of hypothesis testing considers hypotheses of the form H0:theta1 = theta2 versus H1:theta1 theta2, where theta1 and theta2 denote parameters of the process before and after a change point. The paper takes a different perspective and investigates the null hypotheses of no relevant changes, i.e. math formula, where | | is an appropriate norm. This formulation of the testing problem is motivated by the fact that in many applications a modification of the statistical analysis might not be necessary, if the difference between the parameters before and after the change point is small. A general approach to problems of this type is developed which is based on the cumulative sum principle. For the asymptotic analysis weak convergence of the sequential empirical process must be established under the alternative of non-stationarity, and it is shown that the resulting test statistic is asymptotically normally distributed. The results can also be used to establish similarity of the parameters, i.e. math formula, at a controlled type 1 error and to estimate the magnitude math formula of the change with a corresponding confidence interval. Several applications of the methodology are given including tests for relevant changes in the mean, variance, parameter in a linear regression model and distribution function among others. The finite sample properties of the new tests are investigated by means of a simulation study and illustrated by analysing a data example from portfolio management.},
  citeulike-article-id = {14445614},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/rssb.12121},
  groups               = {Regression_Linear, Scenario_TimeSeries, ChngPoints_TimeSrs},
  posted-at            = {2017-10-05 09:17:48},
  timestamp            = {2020-02-29 18:41},
}

@Article{Dias-et-al-2015,
  author       = {Dias, Jose G. and Vermunt, Jeroen K. and Ramos, Sofia},
  date         = {2015},
  journaltitle = {European Journal of Operational Research},
  title        = {Clustering financial time series: New insights from an extended hidden Markov model},
  doi          = {10.1016/j.ejor.2014.12.041},
  number       = {3},
  pages        = {852--864},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S0377221714010595},
  volume       = {243},
  abstract     = {In recent years, large amounts of financial data have become available for analysis. We propose exploring returns from 21 European stock markets by model-based clustering of regime switching models. These econometric models identify clusters of time series with similar dynamic patterns and moreover allow relaxing assumptions of existing approaches, such as the assumption of conditional Gaussian returns.

The proposed model handles simultaneously the heterogeneity across stock markets and over time, i.e., time-constant and time-varying discrete latent variables capture unobserved heterogeneity between and within stock markets, respectively. The results show a clear distinction between two groups of stock markets, each one characterized by different regime switching dynamics that correspond to different expected return-risk patterns.

We identify three regimes: the so-called bull and bear regimes, as well as a stable regime with returns close to 0, which turns out to be the most frequently occurring regime. This is consistent with stylized facts in financial econometrics.},
  groups       = {Networks and investment management, Clustering and network analysis, Scenario_TimeSeries},
  keywords     = {Data mining; Hidden Markov model; Stock indexes; Latent class model; Regime-switching model;},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:41},
}

@InCollection{Fakhrazari-Vakilzadian-2017,
  author               = {Fakhrazari, Amin and Vakilzadian, Hamid},
  booktitle            = {IEEE International Conference on Electro Information Technology (EIT)},
  date                 = {2017-05},
  title                = {A survey on time series data mining},
  doi                  = {10.1109/eit.2017.8053409},
  isbn                 = {978-1-5090-4767-3},
  location             = {Lincoln, NE, USA},
  pages                = {476--481},
  publisher            = {IEEE},
  abstract             = {In this paper, an overview on existing data mining techniques for time series modeling and analysis will be provided. Classification of available literature on time series data mining shows that the main research orientations can be divided into three subfields: Dimensionality Reduction (Time Series Representation), Similarity Measures and Data Mining Tasks.},
  citeulike-article-id = {14499104},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/eit.2017.8053409},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-12-08 01:17:22},
  timestamp            = {2020-02-29 18:41},
}

@Article{Hua-et-al-2016,
  author               = {Hua, Jia-Chen and Roy, Sukesh and McCauley, Joseph L. and Gunaratne, Gemunu H.},
  date                 = {2016-04},
  journaltitle         = {Physica A: Statistical Mechanics and its Applications},
  title                = {Using dynamic mode decomposition to extract cyclic behavior in the stock market},
  doi                  = {10.1016/j.physa.2015.12.059},
  issn                 = {0378-4371},
  pages                = {172--180},
  volume               = {448},
  abstract             = {The presence of cyclic expansions and contractions in the economy has been known for over a century. The work reported here searches for similar cyclic behavior in stock valuations. The variations are subtle and can only be extracted through analysis of price variations of a large number of stocks. Koopman mode analysis is a natural approach to establish such collective oscillatory behavior. The difficulty is that even non-cyclic and stochastic constituents of a finite data set may be interpreted as a sum of periodic motions. However, deconvolution of these irregular dynamical facets may be expected to be non-robust, i.e., to depend on specific data set. We propose an approach to differentiate robust and non-robust features in a time series; it is based on identifying robust features with reproducible Koopman modes, i.e., those that persist between distinct sub-groupings of the data. Our analysis of stock data discovered four reproducible modes, one of which has period close to the number of trading days/year. To the best of our knowledge these cycles were not reported previously. It is particularly interesting that the cyclic behaviors persisted through the great recession even though phase relationships between stocks within the modes evolved in the intervening period.},
  citeulike-article-id = {14520894},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.physa.2015.12.059},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2018-01-22 01:28:57},
  timestamp            = {2020-02-29 18:41},
}

@Article{Izakian-et-al-2014,
  author               = {Izakian, Hesam and Pedrycz, Witold and Jamal, Iqbal},
  date                 = {2015-03},
  journaltitle         = {Engineering Applications of Artificial Intelligence},
  title                = {Fuzzy clustering of time series data using dynamic time warping distance},
  doi                  = {10.1016/j.engappai.2014.12.015},
  issn                 = {0952-1976},
  pages                = {235--244},
  volume               = {39},
  abstract             = {Clustering is a powerful vehicle to reveal and visualize structure of data. When dealing with time series, selecting a suitable measure to evaluate the similarities/dissimilarities within the data becomes necessary and subsequently it exhibits a significant impact on the results of clustering. This selection should be based upon the nature of time series and the application itself. When grouping time series based on their shape information is of interest (shape-based clustering), using a Dynamic Time Warping (DTW) distance is a desirable choice. Using stretching or compressing segments of temporal data, DTW determines an optimal match between any two time series. In this way, time series exhibiting similar patterns occurring at different time periods, are considered as being similar. Although DTW is a suitable choice for comparing data with respect to their shape information, calculating the average of a collection of time series (which is required in clustering methods) based on this distance becomes a challenging problem. As the result, employing clustering techniques like K-Means and Fuzzy C-Means (where the cluster centers - prototypes are calculated through averaging the data) along with the DTW distance is a challenging task and may produce unsatisfactory results. In this study, three alternatives for fuzzy clustering of time series using DTW distance are proposed. In the first method, a DTW-based averaging technique proposed in the literature, has been applied to the Fuzzy C-Means clustering. The second method considers a Fuzzy C-Medoids clustering, while the third alternative comes as a hybrid technique, which exploits the advantages of both the Fuzzy C-Means and Fuzzy C-Medoids when clustering time series. Experimental studies are reported over a set of time series coming from the UCR time series database.},
  citeulike-article-id = {14485204},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.engappai.2014.12.015},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-11-28 14:19:15},
  timestamp            = {2020-02-29 18:41},
}

@Article{Deniz-Luxhoj-2011,
  author               = {Deniz, Erhan and Luxhoj, James T.},
  date                 = {2011-07},
  journaltitle         = {The Engineering Economist},
  title                = {A Scenario Generation Method with Heteroskedasticity and Moment Matching},
  doi                  = {10.1080/0013791x.2011.599918},
  number               = {3},
  pages                = {231--253},
  volume               = {56},
  abstract             = {We present a portfolio management framework composed of a new scenario generation algorithm and a stochastic programming (SP) model. The algorithm is built on heteroskedastic models and a moment matching approach to construct a scenario tree that is a calibrated representation of the randomness in risky asset returns. We also present a multistage SP model that maximizes the expected final wealth and controls the risk exposure through limiting conditional value-at-risk (CVaR) at each decision epoch over the scenario tree generated by the algorithm.},
  citeulike-article-id = {13988713},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/0013791x.2011.599918},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/0013791X.2011.599918},
  day                  = {1},
  groups               = {Scenario_Best, Data_Heteroskdstcty},
  owner                = {cristi},
  posted-at            = {2016-03-27 02:14:24},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-29 18:41},
}

@Article{Erlwein-et-al-2012,
  author               = {Erlwein, Christina and Mitra, Gautam and Roman, Diana},
  date                 = {2012-03},
  journaltitle         = {Annals of Operations Research},
  title                = {HMM based scenario generation for an investment optimisation problem},
  doi                  = {10.1007/s10479-011-0865-8},
  issn                 = {0254-5330},
  number               = {1},
  pages                = {173--192},
  volume               = {193},
  abstract             = {The Geometric Brownian motion (GBM) is a standard method for modelling financial time series. An important criticism of this method is that the parameters of the GBM are assumed to be constants; due to this fact, important features of the time series, like extreme behaviour or volatility clustering cannot be captured. We propose an approach by which the parameters of the GBM are able to switch between regimes, more precisely they are governed by a hidden Markov chain. Thus, we model the financial time series via a hidden Markov model (HMM) with a GBM in each state. Using this approach, we generate scenarios for a financial portfolio optimisation problem in which the portfolio CVaR is minimised. Numerical results are presented.},
  citeulike-article-id = {9114057},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10479-011-0865-8},
  citeulike-linkout-1  = {http://www.springerlink.com/content/v6398l86634753nu},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1007/s10479-011-0865-8},
  day                  = {29},
  groups               = {Scenario generation, Regime based investing, Scenario_Portfolio, Vol_Cluster},
  owner                = {cristi},
  posted-at            = {2016-03-27 03:03:53},
  publisher            = {Springer US},
  timestamp            = {2020-02-29 18:42},
}

@Article{Fairbrother-et-al-2015,
  author               = {Fairbrother, Jamie and Turner, Amanda and Wallace, Stein},
  date                 = {2015-11},
  journaltitle         = {arXiv e-Print},
  title                = {Scenario generation for portfolio selection problems with tail risk measure},
  eprint               = {1511.04935},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1511.04935},
  abstract             = {Tail risk measures such as the conditional value-at-risk are useful in the context of portfolio selection for quantifying potential losses in worst cases. However, for scenario-based problems these are problematic: because the value of a tail risk measure only depends on a small subset of the support of the distribution of asset returns, traditional scenario based methods, which spread scenarios evenly across the whole support of the distribution, yield very unstable solutions unless we use a very large number scenarios.

In this paper we propose a problem-driven scenario generation methodology for portfolio selection problems using a tail risk measure where the the asset returns have elliptical or near-elliptical distribution. Our approach in effect prioritizes the construction of scenarios in the areas of the distribution which correspond to the tail losses of feasible portfolios.

The methodology is shown to work particularly well when the distribution of assets returns are positively correlated and heavy-tailed, and the performance is shown to improve as we tighten the constraints on feasible assets.},
  citeulike-article-id = {13926391},
  citeulike-linkout-0  = {http://arxiv.org/abs/1511.04935},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1511.04935},
  day                  = {16},
  groups               = {Scenario generation, PortfOptim_Scenario, Invest_TailRisk, Scenario_Portfolio},
  howpublished         = {Available at Arxiv: http://arxiv.org/abs/1511.04935},
  owner                = {zkgst0c},
  posted-at            = {2016-02-05 21:39:57},
  timestamp            = {2020-02-29 18:42},
}

@MastersThesis{Georgelis-Nyberg-2013,
  author               = {Georgelis, Nikos and Nyberg, Mikael},
  date                 = {2013},
  institution          = {Royal Institute of Technology},
  title                = {A Scenario Based Allocation Model Using Entropy Pooling for Computing the Scenario Probabilities},
  url                  = {http://kth.diva-portal.org/smash/record.jsf?pid=diva2%3A633872&dswid=-7785},
  abstract             = {We introduce a scenario based allocation model (SBAM) that uses entropy pooling for computing scenario probabilities. Compared to most other models that allow the investor to blend historical data with subjective views about the future, the SBAM does not require the investor to quantify a level of confidence in the subjective views.

A quantitative test is performed on a simulated systematic fund offered by the fund company Informed Portfolio Management in Stockholm, Sweden. The simulated fund under study consists of four individual systematic trading strategies and the test is simulated on a monthly basis during the years 1986-2010.

We study how the selection of views might affect the SBAM portfolios, creating three systematic views and combining them in different variations creating seven SBAM portfolios. We also compare how the size of sample data affects the results.

Furthermore, the SBAM is compared to more common allocation methods, namely an equally weighted portfolio and a portfolio optimization based only on historical data.

We find that the SBAM portfolios produced higher annual returns and information ratio than the equally weighted portfolio or the portfolio optimized only on historical data.},
  citeulike-article-id = {13931090},
  groups               = {Scenario generation, Scenario_Portfolio},
  howpublished         = {Available at http://www.diva-portal.org/smash/record.jsf?pid=diva2A633872 and dswid=1598},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:30:16},
  timestamp            = {2020-02-29 18:42},
}

@Article{Gosling-2010,
  author               = {Gosling, Susan},
  date                 = {2010-10},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {A Scenarios Approach to Asset Allocation},
  doi                  = {10.3905/jpm.2010.37.1.053},
  issn                 = {0095-4918},
  number               = {1},
  pages                = {53--66},
  volume               = {37},
  abstract             = {A number of different approaches to asset allocation are used by practitioners, including purely qualitative assessment, simple mean-variance analysis, and more complex multifactor modeling. Since Markowitz published his seminal paper in 1952, however, approaches that rely on the selection of particular parametric return distributions, on summary measures of risk, and on historical data as an indicator of the future still remain widespread. Little doubt exists that such reliance has resulted in serious mismeasurement of risk and misallocation of assets.

In this article, Gosling proposes an alternative approach that is important in its implications for investment philosophy and practice. The approach makes more complete use of the information available about the future and virtually forces serious consideration of different time frames, alternate outcomes, and tail risk. The depth of information provided about risk and diversification is also a principal benefit of the approach. The information is not provided by forecasting the future, but by describing what could happen. These changes have the potential to make a significant difference to long-term investment outcomes.},
  citeulike-article-id = {13911157},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jpm.2010.37.1.053},
  groups               = {Scenario generation, PortfOptim_Scenario, Scenario_Risk, Scenario_Portfolio, FrcstQWIM_MedLngTerm},
  owner                = {cristi},
  posted-at            = {2016-01-17 20:26:46},
  timestamp            = {2020-02-29 18:42},
}

@Article{Gulpinar-et-al-2004,
  author               = {Gulpinar, Nalan and Rustem, Berc and Settergren, Reuben},
  date                 = {2004-04},
  journaltitle         = {Journal of Economic Dynamics and Control},
  title                = {Simulation and optimization approaches to scenario tree generation},
  doi                  = {10.1016/s0165-1889(03)00113-1},
  issn                 = {0165-1889},
  number               = {7},
  pages                = {1291--1315},
  volume               = {28},
  abstract             = {In this paper, three approaches are presented for generating scenario trees for financial portfolio problems. These are based on simulation, optimization and hybrid simulation/optimization. In the simulation approach, the price scenarios at each time period are generated as the centroids of random scenario simulations generated sequentially or in parallel. The optimization method generates a number of discrete outcomes which satisfy specified statistical properties by solving either a sequence of non-linear optimization models (one at each node of the scenario tree) or one large optimization problem. In the hybrid approach, the optimization problem is reduced in size by fixing price variables to values obtained by simulation. These procedures are backtested using historical data and computational results are presented.},
  citeulike-article-id = {14087933},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/s0165-1889(03)00113-1},
  groups               = {Scenario generation, PortfOptim_Scenario, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:15:48},
  timestamp            = {2020-02-29 18:42},
}

@Article{Engle-Roussellet-2016,
  author               = {Engle, Robert F. and Roussellet, Guillaume},
  date                 = {2016},
  journaltitle         = {SSRN e-Print},
  title                = {Scenario Generation for Long-Run Interest Rate Risk Assessment},
  doi                  = {10.2139/ssrn.2828236},
  issn                 = {1556-5068},
  abstract             = {We propose a statistical model of the term structure of sovereign yields tailored for long-term probability-based scenario generation and forecasts. While being simple to estimate, our model is able to reproduce simultaneously the positivity of the yield curve, high persistence, factor structure and time varying volatilities and correlations. It features a regime switching short rate model. A complete benchmark of the model following Diebold and Li is performed in terms of forecasting ability and coverage properties. We show that the proposed model improves performance relative to a standard model from the literature.},
  citeulike-article-id = {14332366},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2828236},
  groups               = {Scenario_Risk, FrcstQWIM_Rates, Real_Yield_Curve, Scenario_YieldCurve},
  posted-at            = {2017-04-05 14:32:29},
  timestamp            = {2020-02-29 18:42},
}

@Article{Friedman-Zhang-2014,
  author               = {Friedman, Craig A. and Zhang, Yangyong},
  date                 = {2014-01},
  journaltitle         = {SSRN e-Print},
  title                = {A Method to Find Diverse and Manageable Sets of Plausible Yet Severe Financial Scenarios},
  url                  = {https://ssrn.com/abstract=2379083},
  abstract             = {We introduce a new practical data-intensive method to generate/discover consistent finite representative collections of plausible yet severe macroprudential, microprudential, book-specific, and individual obligor/instrument scenarios. These scenarios are conditioned on current information (including current macroeconomic, index, industry and instrument/obligor-specific information), and can be conditioned on partial future scenario specifications as well (to accommodate regulatory stress testing requirements, for example, the CCAR requirements for banks, the projections of economists, or senior management). Our method is scalable, is designed to work with limited training data, can incorporate the fat-tailed and mutually dependent behavior that is characteristic of many financial quantities, and can reflect model misspecification risk.},
  citeulike-article-id = {14073679},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2379083},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2379083code533415.pdf?abstractid=2379083 and mirid=1},
  day                  = {15},
  groups               = {Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-06-22 07:07:59},
  timestamp            = {2020-02-29 18:42},
}

@Article{Gao-et-al-2017,
  author               = {Gao, Gelin and Mishra, Bud and Ramazzotti, Daniele},
  date                 = {2017-03},
  journaltitle         = {arXiv e-Print},
  title                = {Efficient Simulation of Financial Stress Testing Scenarios with Suppes-Bayes Causal Networks},
  eprint               = {1703.03076},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1703.03076},
  abstract             = {The most recent financial upheavals have cast doubt on the adequacy of some of the conventional quantitative risk management strategies, such as VaR (Value at Risk), in many common situations. Consequently, there has been an increasing need for verisimilar financial stress testings, namely simulating and analyzing financial portfolios in extreme, albeit rare scenarios. Unlike conventional risk management which exploits statistical correlations among financial instruments, here we focus our analysis on the notion of probabilistic causation, which is embodied by Suppes-Bayes Causal Networks (SBCNs), SBCNs are probabilistic graphical models that have many attractive features in terms of more accurate causal analysis for generating financial stress scenarios. In this paper, we present a novel approach for conducting stress testing of financial portfolios based on SBCNs in combination with classical machine learning classification tools. The resulting method is shown to be capable of correctly discovering the causal relationships among financial factors that affect the portfolios and thus, simulating stress testing scenarios with a higher accuracy and lower computational complexity than conventional Monte Carlo Simulations.},
  citeulike-article-id = {14381688},
  citeulike-linkout-0  = {http://arxiv.org/abs/1703.03076},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1703.03076},
  day                  = {8},
  groups               = {Test_Scenario, Proba_Bayes, Scenario_Risk},
  posted-at            = {2017-06-23 20:26:51},
  timestamp            = {2020-02-29 18:42},
}

@Article{Geyer-et-al-2010,
  author               = {Geyer, Alois and Hanke, Michael and Weissensteiner, Alex},
  date                 = {2010-11},
  journaltitle         = {European Journal of Operational Research},
  title                = {No-arbitrage conditions, scenario trees, and multi-asset financial optimization},
  doi                  = {10.1016/j.ejor.2010.03.022},
  issn                 = {0377-2217},
  number               = {3},
  pages                = {609--613},
  volume               = {206},
  abstract             = {Many numerical optimization methods use scenario trees as a discrete approximation for the true (multi-dimensional) probability distributions of the problem's random variables. Realistic specifications in financial optimization models can lead to tree sizes that quickly become computationally intractable. In this paper we focus on the two main approaches proposed in the literature to deal with this problem: scenario reduction and state aggregation. We first state necessary conditions for the node structure of a tree to rule out arbitrage. However, currently available scenario reduction algorithms do not take these conditions explicitly into account. State aggregation excludes arbitrage opportunities by relying on the risk-neutral measure. This is, however, only appropriate for pricing purposes but not for optimization. Both limitations are illustrated by numerical examples. We conclude that neither of these methods is suitable to solve financial optimization models in asset liability or portfolio management.},
  citeulike-article-id = {6880890},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2010.03.022},
  day                  = {15},
  groups               = {Scenario generation, PortfOptim_Scenario, Scenario_Risk},
  journal              = {European Journal of Operational Research},
  owner                = {cristi},
  posted-at            = {2016-03-27 04:39:18},
  timestamp            = {2020-02-29 18:42},
  year                 = {2010},
}

@Article{Hua-Xia-2014,
  author               = {Hua, Lei and Xia, Michelle},
  date                 = {2014-07},
  journaltitle         = {North American Actuarial Journal},
  title                = {Assessing High-Risk Scenarios by Full-Range Tail Dependence Copulas},
  doi                  = {10.1080/10920277.2014.888009},
  number               = {3},
  pages                = {363--378},
  volume               = {18},
  abstract             = {Copulas with a full-range tail dependence property can cover the widest range of positive dependence in the tail, so that a regression model can be built accounting for dynamic tail dependence patterns between variables. We propose a model that incorporates both regression on each marginal of bivariate response variables and regression on the dependence parameter for the response variables. An ACIG copula that possesses the full-range tail dependence property is implemented in the regression analysis. Comparisons between regression analysis based on ACIG and Gumbel copulas are conducted, showing that the ACIG copula is generally better than the Gumbel copula when there is intermediate upper tail dependence. A simulation study is conducted to illustrate that dynamic tail dependence structures between loss and ALAE can be captured by using the one-parameter ACIG copula. Finally, we apply the ACIG and Gumbel regression models for a dataset from the U.S. Medical Expenditure Panel Survey. The empirical analysis suggests that the regression model with the ACIG copula improves the assessment of high-risk scenarios, especially for aggregated dependent risks.},
  citeulike-article-id = {14148379},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/10920277.2014.888009},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/10920277.2014.888009},
  day                  = {3},
  groups               = {Scenario generation, Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-09-28 19:01:47},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 18:42},
}

@Article{Dempster-et-al-2010,
  author               = {Dempster, Michael A. H. and Medova, Elena A. and Villaverde, Michael},
  date                 = {2010},
  journaltitle         = {Journal of Asset Management},
  title                = {Long-term interest rates and consol bond valuation},
  doi                  = {10.1057/jam.2010.7},
  issn                 = {1470-8272},
  number               = {2-3},
  pages                = {113--135},
  volume               = {11},
  abstract             = {This article presents a Gaussian three-factor model of the term structure of interest rates which is Markov and time homogeneous. The model captures the whole term structure and is particularly useful in forward simulations for applications in long-term swap and bond pricing, risk management and portfolio optimization. Kalman filter parameter estimation uses EU swap rate data and is described in detail. The yield curve model is fitted to data up to 2002 and assessed by simulation of yield curve scenarios over the next 2 years. It is then applied to the valuation of callable floating rate consol bonds as recently issued by European banks to raise Tier 1 regulatory capital over the subsequent period from 2005 to 2007.},
  citeulike-article-id = {7616454},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2010.7},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2010/00000011/F0020002/art00004},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1057/jam.2010.7},
  groups               = {Real_Yield_Curve, Scenario_YieldCurve},
  posted-at            = {2017-03-19 19:09:46},
  publisher            = {Palgrave Macmillan UK},
  timestamp            = {2020-02-29 18:42},
}

@Article{DeVisser-Hamelink-2014,
  author               = {{De Visser}, Samuel and Hamelink, Foort},
  date                 = {2014-01},
  journaltitle         = {SSRN e-Print},
  title                = {Simulating Pension's Assets and Liabilities in a Regime Switching Framework},
  url                  = {https://ssrn.com/abstract=2381551},
  abstract             = {In this paper we build a simple ALM model where future scenarios are generated assuming a Markov regime switching framework. Using the Shiller database of monthly equity returns and interest rate data since 1870, two regimes are revealed by the data that clearly correspond to a normal regime where returns behave like expected from economic theory, and a high volatility regime we may also refer to as a crisis regime . Given the evidence of the non-stationarity of economic variables, we investigate the added value of reducing risk in the portfolio when the model indicates a high probability of a regime shift. The persistence of each of the regimes is high. This framework gives, each month and for each scenario, the probability of being in one of the two regimes, and hence the multivariate distribution of the simulated variables that pertains to the relevant regime. These variables are (1) equity returns, (2) long term (10-year) interest rates, (3) realized inflation, and (4) short term (6-month) interest rates. We then investigate a number of relevant statistics of the terminal wealth achieved after a 20-year period for two typical portfolios: a long-only portfolio well-diversified over stocks and bonds where the relevant metric is the portfolio's value (for instance, an endowment fund), and a pension fund's coverage ratio where the fund's liabilities are valued by a market interest rate curve. We show that both types of investors greatly benefit from adjusting their exposure to equities and interest rates conditionally on the expected risk regime. Finally, we show the consequence when both the endowment fund manager and the pension fund board members optimize their own reward/risk ratio from their job. We argue that in such a case they seek to minimize the probability of large losses (either in absolute terms or relative to the pension fund's liabilities), while maximizing the minimum level of wealth (or coverage ratio for the pension fund) achieved with a given (say 95 confidence level. We quantify the added value of the risk-regime depending allocations for such managers.},
  citeulike-article-id = {13988016},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2381551},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2381551code147508.pdf?abstractid=2381551 and mirid=1},
  day                  = {20},
  groups               = {Scenario generation, Regime based investing, Regime_Invest, PortfOptim_Regime, Real_Yield_Curve, Scenario_YieldCurve, [nbkcbu3:]},
  journal              = {SSRN Electronic Journal},
  owner                = {cristi},
  posted-at            = {2016-03-25 15:21:40},
  timestamp            = {2020-02-29 18:42},
  year                 = {2014},
}

@Book{Dupacova-et-al-2002,
  author               = {Dupacova, Jitka and Hurt, J. and Stepan, J.},
  date                 = {2002-08},
  title                = {Stochastic Modeling in Economics and Finance},
  edition              = {2002},
  isbn                 = {1402008406},
  publisher            = {Springer},
  url                  = {https://www.springer.com/gp/book/9781402008405},
  abstract             = {In Part I, the fundamentals of financial thinking and elementary mathematical methods of finance are presented. The method of presentation is simple enough to bridge the elements of financial arithmetic and complex models of financial math developed in the later parts. It covers characteristics of cash flows, yield curves, and valuation of securities. Part II is devoted to the allocation of funds and risk management: classics (Markowitz theory of portfolio), capital asset pricing model, arbitrage pricing theory, asset and liability management, value at risk. The method explanation takes into account the computational aspects. Part III explains modeling aspects of multistage stochastic programming on a relatively accessible level. It includes a survey of existing software, links to parametric, multiobjective and dynamic programming, and to probability and statistics. It focuses on scenario-based problems with the problems of scenario generation and output analysis discussed in detail and illustrated within a case study.},
  citeulike-article-id = {14171145},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20 and amp;path=ASIN/1402008406},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21 and amp;path=ASIN/1402008406},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21 and amp;path=ASIN/1402008406},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/1402008406},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/1402008406/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20 and path=ASIN/1402008406},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/1402008406},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN1402008406},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=1402008406 and index=books and linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/1402008406},
  day                  = {31},
  groups               = {Scenario_YieldCurve},
  howpublished         = {Hardcover},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:23:52},
  timestamp            = {2020-02-29 18:42},
}

@Article{Ferstl-Weissensteiner-2010,
  author               = {Ferstl, Robert and Weissensteiner, Alex},
  date                 = {2010-06},
  journaltitle         = {Journal of Asset Management},
  title                = {Backtesting short-term treasury management strategies based on multi-stage stochastic programming},
  doi                  = {10.1057/jam.2010.11},
  issn                 = {1470-8272},
  number               = {2-3},
  pages                = {94--112},
  volume               = {11},
  abstract             = {We show the practical viability of a short-term treasury management model which is formulated as a multi-stage stochastic linear program. A company minimises the Conditional Value at Risk of final wealth, subject to given future cash flows and the uncertain future development of interest rates and equity returns, choosing an asset allocation among cash, several bonds and an equity investment. The scenario generation procedure includes an estimation of the market price of risk and a change of the underlying probability measure. We provide an out-of-sample backtest for the proposed policy and compare the performance to alternative strategies.

Our approach shows a better risk-return trade-off for different aggregated risk measures. Further, we perform several numerical studies based on a real market data set to test for the sensitivity to changes in the input parameters, for example shifts of the yield curve, changes in the equity spread or the cash flows. The resulting portfolios are well-diversified and the impact on the asset allocation follows economic intuition.},
  citeulike-article-id = {7616455},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2010.11},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2010/00000011/F0020002/art00003},
  groups               = {Test_Scenario, Scenario_YieldCurve},
  owner                = {cristi},
  posted-at            = {2016-03-06 02:20:34},
  publisher            = {Palgrave Macmillan},
  timestamp            = {2020-02-29 18:42},
}

@Article{Irlam-2018,
  author         = {Irlam, Gordon},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Financial planning via deep reinforcement learning AI},
  doi            = {10.2139/ssrn.3201703},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3201703},
  abstract       = {This paper introduces AIPlanner, a financial planner based upon deep reinforcement learning. AIPlanner provides an investment and consumption strategy intended to optimize lifetime well-being. The results of AIPlanner are very close to the precise analytical solution, as well as to the precise solution computed using stochastic dynamic programming. Deep reinforcement learning is additionally capable of delivering results for far more complicated and realistic financial models that other approaches can't handle. As an example of this capability, a bond model that includes both a yield curve and time varying interest rates is employed. Compared to other popular approaches, in one reasonable scenario, AIPlanner was found to effectively deliver approximately 1,000 to 8,000 of additional consumption per year.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_YieldCurve, ML_ReinfoLrng},
  timestamp      = {2020-02-29 18:42},
}

@Article{Kopeliovich-et-al-2015,
  author       = {Yaacov Kopeliovich and Arcady Novosyolov and Daniel Satchkov and Barry Schachter},
  date         = {2015},
  journaltitle = {The Journal of Derivatives},
  title        = {Robust Risk Estimation and Hedging: A Reverse Stress Testing Approach},
  doi          = {10.3905/jod.2015.22.4.010},
  number       = {4},
  pages        = {10--25},
  url          = {https://jod.pm-research.com/content/22/4/10},
  volume       = {22},
  abstract     = {The stress test has become an increasingly important risk assessment and management tool. But while it is easy to imagine a stress scenario and to estimate its impact on the firm's financial condition, it is not so obvious how to select the most meaningful scenarios in the first place, either to get reasonable coverage of the space of stressful possibilities or even to focus on those that are most probable.

In this article, the authors approach the problem from the reverse direction. They begin with a specified level of loss and pick the most likely scenario that generates that loss. They then use principal components to construct a set of alternative scenarios that produce the same level of loss but in (maximally) different ways. This provides much greater insight into which sources of risk are the most important and the most stable across scenarios.},
  groups       = {Test_Scenario, StressTest_Reverse},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:43},
}

@Article{Mankai-Guesmi-2015,
  author       = {Mankai, Selim and Guesmi, Khaled},
  date         = {2015},
  journaltitle = {Bankers, Markets and Investors},
  title        = {Robust Portfolio Protection: A Scenarios-based Approach},
  number       = {138},
  pages        = {30--44},
  abstract     = {This paper constructs a portfolio protection model to deal with uncertain adverse returns. Our model considers an adjustable discrete uncertainty set to control the conservatism of the robust portfolio.

Without prior assumptions on the data generating process, we develop an a priori probabilistic guarantee of the robust portfolio. Unlike previous measures that depend solely on the uncertainty model, our measure also takes into account asset allocation and investment horizon. We provide an application of international portfolio protection covering the financial crisis period. Computational experiments and ex-post analysis provide evidence for the effectiveness of our model.},
  groups       = {Scenario generation, PortfOptim_Robust, PortfOptim_Scenario, Scenario_Portfolio, Scenario_Market, ExAnte_ExPost},
  keywords     = {Portfolio protection; Robust optimization; Multivariate tail dependence; Nonparametric predictive inference},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:44},
}

@Article{Nagpal-2017,
  author         = {Nagpal, Krishan Mohan},
  date           = {2017-11},
  journaltitle   = {Risk manag (Bas)},
  title          = {Designing stress scenarios for portfolios},
  doi            = {10.1057/s41283-017-0024-x},
  issn           = {1460-3799},
  number         = {4},
  pages          = {323--349},
  volume         = {19},
  abstract       = {Evaluating portfolio performance under different stress scenarios is an important risk management tool. Designing stress scenarios for portfolios can be complex as it involves determining potential market changes in different asset classes and risk factors in a coherent manner that are extreme but plausible. This paper describes an approach to design such stress scenarios where the portfolio performance depends on market changes in many risk factors and asset classes. The scenario design is customized for the portfolio and helps describe plausible market changes that would have the most adverse impact on the portfolio performance. The approach relies on historical data and derives the scenario based on market changes during historical periods that would have been the most stressful for the given portfolio. The proposed approach also allows one to adjust the level of severity and if desired incorporate any specific market conditions of concern (such as scenario design for increasing interest rate environment and/or certain level of unemployment rate, etc.). The main advantages of the proposed approach are (a) flexibility in scenario design with and without constraints on market conditions with adjustable levels of severities, (b) computational simplicity, (c) scalability to any number of market risk factors, (d) no need of prior assumptions on joint distribution of market risk factors, and (e) transparency of the results as they are developed from market changes during actual stressful historical periods.},
  f1000-projects = {QuantInvest},
  groups         = {PortfOptim_Scenario, Scenario_Market, Scenario_Risk, Scenario_Portfolio},
  timestamp      = {2020-02-29 18:44},
}

@Article{Johnson-West-2018,
  author         = {Johnson, Matthew C. and West, Mike},
  date           = {2018-03-06},
  journaltitle   = {arXiv e-Print},
  title          = {Bayesian Predictive Synthesis: Forecast Calibration and Combination},
  url            = {https://arxiv.org/abs/1803.01984},
  abstract       = {The combination of forecast densities, whether they result from a set of models, a group of consulted experts, or other sources, is becoming increasingly important in the fields of economics, policy and finance, among others. Requiring methodology that goes beyond standard Bayesian model uncertainty and model mixing - with its well-known limitations based on a clearly proscribed theoretical basis - multiple 'density combination' methods have been proposed. While some proposals have demonstrated empirical success, most apparently lack a core philosophical and theoretical foundation. Interesting recent examples generalize the common 'linear opinion pool' with flexible mixing weights that depend on the forecast variable itself - i.e., outcome-dependent mixing. Taking a foundational subjective Bayesian perspective, we show that such a density combination scheme is in fact justified as one example of Bayesian agent opinion analysis, or 'predictive synthesis'. This logically coherent framework clearly delineates the underlying assumptions as well as the theoretical constraints and limitations of many combination 'rules', defining a broad class of Bayesian models for the general problem. A number of examples, including an application to a set of predictive densities in foreign exchange, provide illustrations.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:44},
}

@Book{Kemp-2014,
  author    = {M. Kemp},
  date      = {2014},
  title     = {Extreme Events: Robust Portfolio Construction in the Presence of Fat Tails},
  publisher = {Wiley},
  url       = {https://www.wiley.com/en-us/Extreme+Events%3A+Robust+Portfolio+Construction+in+the+Presence+of+Fat+Tails-p-9781119962878},
  abstract  = {Taking due account of extreme events when constructing portfolios of assets or liabilities is a key discipline for market professionals. Extreme events are a fact of life in how markets operate. In Extreme Events: Robust Portfolio Construction in the Presence of Fat Tails, leading expert Malcolm Kemp shows readers how to analyse market data to uncover fat-tailed behaviour, how to incorporate expert judgement in the handling of such information, and how to refine portfolio construction methodologies to make portfolios less vulnerable to extreme events or to benefit more from them.

This is the only text that combines a comprehensive treatment of modern risk budgeting and portfolio construction techniques with the specific refinements needed for them to handle extreme events. It explains in a logical sequence what constitutes fat-tailed behaviour and why it arises, how we can analyse such behaviour, at aggregate, sector or instrument level, and how we can then take advantage of this analysis.

Along the way, it provides a rigorous, comprehensive and clear development of traditional portfolio construction methodologies applicable if fat-tails are absent. It then explains how to refine these methodologies to accommodate real world behaviour.

Throughout, the book highlights the importance of expert opinion, showing that even the most data-centric portfolio construction approaches ultimately depend on practitioner assumptions about how the world might behave.},
  groups    = {Scenario_ExpertView},
  owner     = {zkgst0c},
  timestamp = {2020-02-29 18:44},
}

@Article{Lichtendahl-et-al-2013,
  author               = {Lichtendahl, Kenneth C. and Grushka-Cockayne, Yael and Winkler, Robert L.},
  date                 = {2013-07},
  journaltitle         = {Management Science},
  title                = {Is It Better to Average Probabilities or Quantiles?},
  doi                  = {10.1287/mnsc.1120.1667},
  issn                 = {0025-1909},
  number               = {7},
  pages                = {1594--1611},
  volume               = {59},
  abstract             = {We consider two ways to aggregate expert opinions using simple averages: averaging probabilities and averaging quantiles. We examine analytical properties of these forecasts and compare their ability to harness the wisdom of the crowd. In terms of location, the two average forecasts have the same mean. The average quantile forecast is always sharper: it has lower variance than the average probability forecast. Even when the average probability forecast is overconfident, the shape of the average quantile forecast still offers the possibility of a better forecast. Using probability forecasts for gross domestic product growth and inflation from the Survey of Professional Forecasters, we present evidence that both when the average probability forecast is overconfident and when it is underconfident, it is outperformed by the average quantile forecast. Our results show that averaging quantiles is a viable alternative and indicate some conditions under which it is likely to be more useful than averaging probabilities.},
  citeulike-article-id = {14332370},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/mnsc.1120.1667},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-04-05 14:44:36},
  timestamp            = {2020-02-29 18:44},
}

@Article{Lorenz-et-al-2011,
  author               = {Lorenz, Jan and Rauhut, Heiko and Schweitzer, Frank and Helbing, Dirk},
  date                 = {2011},
  journaltitle         = {Proceedings of the National Academy of Sciences},
  title                = {How social influence can undermine the wisdom of crowd effect},
  doi                  = {10.1073/pnas.1008636108},
  issn                 = {1091-6490},
  number               = {22},
  pages                = {9020--9025},
  volume               = {108},
  abstract             = {Social groups can be remarkably smart and knowledgeable when their averaged judgements are compared with the judgements of individuals. Already Galton [Galton F (1907) Nature 75:7] found evidence that the median estimate of a group can be more accurate than estimates of experts. This wisdom of crowd effect was recently supported by examples from stock markets, political elections, and quiz shows [Surowiecki J (2004) The Wisdom of Crowds]. In contrast, we demonstrate by experimental evidence (N = 144) that even mild social influence can undermine the wisdom of crowd effect in simple estimation tasks. In the experiment, subjects could reconsider their response to factual questions after having received average or full information of the responses of other subjects. We compare subjects' convergence of estimates and improvements in accuracy over five consecutive estimation periods with a control condition, in which no information about others' responses was provided. Although groups are initially "wise", knowledge about estimates of others narrows the diversity of opinions to such an extent that it undermines the wisdom of crowd effect in three different ways. The "social influence effect" diminishes the diversity of the crowd without improvements of its collective error. The "range reduction effect" moves the position of the truth to peripheral regions of the range of estimates so that the crowd becomes less reliable in providing expertise for external observers. The "confidence effect" boosts individuals' confidence after convergence of their estimates despite lack of improved accuracy. Examples of the revealed mechanism range from misled elites to the recent global financial crisis.},
  citeulike-article-id = {9305976},
  citeulike-linkout-0  = {http://dx.doi.org/10.1073/pnas.1008636108},
  citeulike-linkout-1  = {http://www.pnas.org/content/early/2011/05/10/1008636108.abstract},
  citeulike-linkout-2  = {http://www.pnas.org/content/early/2011/05/10/1008636108.full.pdf},
  citeulike-linkout-3  = {http://www.pnas.org/cgi/content/abstract/108/22/9020},
  citeulike-linkout-4  = {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3107299/},
  citeulike-linkout-5  = {http://view.ncbi.nlm.nih.gov/pubmed/21576485},
  citeulike-linkout-6  = {http://www.hubmed.org/display.cgi?uids=21576485},
  day                  = {31},
  groups               = {Scenario_ExpertView},
  journal              = {Proceedings of the National Academy of Sciences},
  pmcid                = {PMC3107299},
  pmid                 = {21576485},
  posted-at            = {2017-12-08 01:54:56},
  publisher            = {National Academy of Sciences},
  timestamp            = {2020-02-29 18:44},
  year                 = {2011},
}

@Article{Lundberg-Lee-2017,
  author               = {Lundberg, Scott and Lee, Su-In},
  date                 = {2017-05},
  journaltitle         = {arXiv e-Print},
  title                = {A unified approach to interpreting model predictions},
  eprint               = {1705.07874},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1705.07874},
  abstract             = {Understanding why a model made a certain prediction is crucial in many applications. However, with large modern datasets the best accuracy is often achieved by complex models even experts struggle to interpret, such as ensemble or deep learning models. This creates a tension between accuracy and interpretability. In response, a variety of methods have recently been proposed to help users interpret the predictions of complex models. Here, we present a unified framework for interpreting predictions, namely SHAP (SHapley Additive exPlanations, which assigns each feature an importance for a particular prediction. The key novel components of the SHAP framework are the identification of a class of additive feature importance measures and theoretical results that there is a unique solution in this class with a set of desired properties. This class unifies six existing methods, and several recent methods in this class do not have these desired properties. This means that our framework can inform the development of new methods for explaining prediction models. We demonstrate that several new methods we presented in this paper based on the SHAP framework show better computational performance and better consistency with human intuition than existing methods.},
  citeulike-article-id = {14362088},
  citeulike-linkout-0  = {http://arxiv.org/abs/1705.07874},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1705.07874},
  day                  = {22},
  groups               = {ML_Interpretability, Scenario_ExpertView},
  posted-at            = {2017-05-23 21:53:34},
  timestamp            = {2020-02-29 18:44},
}

@Article{Luo-2016,
  author               = {Luo, Gang},
  date                 = {2016},
  journaltitle         = {Network Modeling Analysis in Health Informatics and Bioinformatics},
  title                = {A review of automatic selection methods for machine learning algorithms and hyper-parameter values},
  doi                  = {10.1007/s13721-016-0125-6},
  number               = {1},
  pages                = {1--16},
  volume               = {5},
  abstract             = {Machine learning studies automatic algorithms that improve themselves through experience. It is widely used for analyzing and extracting value from large biomedical data sets, or "big biomedical data,"advancing biomedical research, and improving healthcare. Before a machine learning model is trained, the user of a machine learning software tool typically must manually select a machine learning algorithm and set one or more model parameters termed hyper-parameters. The algorithm and hyper-parameter values used can greatly impact the resulting model's performance, but their selection requires special expertise as well as many labor-intensive manual iterations. To make machine learning accessible to layman users with limited computing expertise, computer science researchers have proposed various automatic selection methods for algorithms and/or hyper-parameter values for a given supervised machine learning problem. This paper reviews these methods, identifies several of their limitations in the big biomedical data environment, and provides preliminary thoughts on how to address these limitations. These findings establish a foundation for future research on automatically selecting algorithms and hyper-parameter values for analyzing big biomedical data.},
  booktitle            = {Network Modeling Analysis in Health Informatics and Bioinformatics},
  citeulike-article-id = {14177315},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s13721-016-0125-6},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s13721-016-0125-6},
  groups               = {Scenario_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-11-04 16:21:22},
  publisher            = {Springer Vienna},
  timestamp            = {2020-02-29 18:44},
}

@Article{Morlidge-2016,
  author               = {Morlidge, Steve},
  date                 = {2016},
  journaltitle         = {Foresight: international journal of applied forecasting},
  title                = {Using Error Analysis to Improve Forecast Performance},
  number               = {41},
  abstract             = {Over the last two years I have written a number of pieces for Foresight with proposals about how to improve the measurement of forecast error. In this article, I show how these and related ideas can be used to manage the performance of the forecast process. Drawing upon my own experience, I will show how to Measure and track trends in forecast performance Compare and benchmark forecasts Identify the drivers of forecast performance Routinely track down the root causes of performance failures Conduct periodic reviews of the performance of a portfolio of forecasts Measure the impact of avoidable forecast error on the company. Such analyses can be embedded in a control system to provide speedy, actionable feedback to forecasters and to their internal customers. They could also form the basis of an approach for consultants or internal experts to audit the quality of the forecasting process. This article furnishes examples of the practical application of this approach in the demandforecasting process of a typical consumer-goods business, selling product from stock.},
  citeulike-article-id = {14486408},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-11-30 18:30:04},
  timestamp            = {2020-02-29 18:44},
}

@Article{Nguyen-Chamroukhi-2018,
  author         = {Nguyen, Hien D. and Chamroukhi, Faicel},
  date           = {2018-07},
  journaltitle   = {WIREs Data Mining Knowl Discov},
  title          = {Practical and theoretical aspects of mixture-of-experts modeling: An overview},
  doi            = {10.1002/widm.1246},
  issn           = {1942-4787},
  number         = {4},
  pages          = {e1246},
  volume         = {8},
  abstract       = {Mixture-of-experts (MoE) models are a powerful paradigm for modeling data arising from complex data generating processes (DGPs). In this article, we demonstrate how different MoE models can be constructed to approximate the underlying DGPs of arbitrary types of data. Due to the probabilistic nature of MoE models, we propose the maximum quasi-likelihood (MQL) approach as a method for estimating MoE model parameters from data, and we provide conditions under which MQL estimators are consistent and asymptotically normal. The blockwise minorization-maximization (blockwise-MM) algorithm framework is proposed as an all-purpose method for constructing algorithms for obtaining MQL estimators. An example derivation of a blockwise-MM algorithm is provided. We then present a method for constructing information criteria for estimating the number of components in MoE models and provide justification for the classic Bayesian information criterion (BIC). We explain how MoE models can be used to conduct classification, clustering, and regression and illustrate these applications via two worked examples.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:44},
}

@InCollection{Kegel-et-al-2017,
  author         = {Kegel, Lars and Hahmann, Martin and Lehner, Wolfgang},
  booktitle      = {Proceedings of the 29th International Conference on Scientific and Statistical Database Management - '17},
  date           = {2017-06-27},
  title          = {Generating What-If Scenarios for Time Series Data},
  doi            = {10.1145/3085504.3085507},
  isbn           = {9781450352826},
  location       = {New York, New York, USA},
  pages          = {1--12},
  publisher      = {ACM Press},
  abstract       = {Time series data has become a ubiquitous and important data source in many application domains. Most companies and organizations strongly rely on this data for critical tasks like decision-making, planning, predictions, and analytics in general. While all these tasks generally focus on actual data representing organization and business processes, it is also desirable to apply them to alternative scenarios in order to prepare for developments that diverge from expectations or assess the robustness of current strategies. When it comes to the construction of such what-if scenarios, existing tools either focus on scalar data or they address highly specific scenarios. In this work, we propose a generally applicable and easy-to-use method for the generation of what-if scenarios on time series data. Our approach extracts descriptive features of a data set and allows the construction of an alternate version by means of filtering and modification of these features.},
  day            = {27},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:44},
}

@InCollection{Kegel-et-al-2018,
  author         = {Kegel, Lars and Hahmann, Martin and Lehner, Wolfgang},
  booktitle      = {Proceedings of the 30th International Conference on Scientific and Statistical Database Management - '18},
  date           = {2018-07-09},
  title          = {Feature-based comparison and generation of time series},
  doi            = {10.1145/3221269.3221293},
  isbn           = {9781450365055},
  location       = {New York, New York, USA},
  pages          = {1--12},
  publisher      = {ACM Press},
  abstract       = {For more than three decades, researchers have been developping generation methods for the weather, energy, and economic domain. These methods provide generated datasets for reasons like system evaluation and data availability. However, despite the variety of approaches, there is no comparative and cross-domain assessment of generation methods and their expressiveness. We present a similarity measure that analyzes generation methods regarding general time series features. By this means, users can compare generation methods and validate whether a generated dataset is considered similar to a given dataset. Moreover, we propose a feature-based generation method that evolves cross-domain time series datasets. This method outperforms other generation methods regarding the feature-based similarity.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:44},
}

@InCollection{Linardi-et-al-2018,
  author         = {Linardi, Michele and Zhu, Yan and Palpanas, Themis and Keogh, Eamonn},
  booktitle      = {Proceedings of the 2018 International Conference on Management of Data - SIGMOD '18},
  date           = {2018-06-10},
  title          = {VALMOD: A suite for easy and exact detection of variable length motifs in data series},
  doi            = {10.1145/3183713.3193556},
  isbn           = {9781450347037},
  location       = {New York, New York, USA},
  pages          = {1757--1760},
  publisher      = {ACM Press},
  abstract       = {Data series motif discovery represents one of the most useful primitives for data series mining, with applications to many domains, such as robotics, entomology, seismology, medicine, and climatology, and others. The state-of-the-art motif discovery tools still require the user to provide the motif length. Yet, in several cases, the choice of motif length is critical for their detection. Unfortunately, the obvious brute-force solution, which tests all lengths within a given range, is computationally untenable, and does not provide any support for ranking motifs at different resolutions (i.e., lengths). We demonstrate VALMOD, our scalable motif discovery algorithm that efficiently finds all motifs in a given range of lengths, and outputs a length-invariant ranking of motifs. Furthermore, we support the analysis process by means of a newly proposed meta-data structure that helps the user to select the most promising pattern length. This demo aims at illustrating in detail the steps of the proposed approach, showcasing how our algorithm and corresponding graphical insights enable users to efficiently identify the correct motifs.},
  day            = {10},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:44},
}

@Article{Mori-et-al-2016,
  author               = {Mori, Usue and Mendiburu, Alexander and Lozano, Jose A.},
  date                 = {2016},
  journaltitle         = {The R Journal},
  title                = {Distance Measures for Time Series in R: The TSdist Package},
  url                  = {https://journal.r-project.org/archive/2016/RJ-2016-058/RJ-2016-058.pdf},
  abstract             = {The definition of a distance measure between time series is crucial for many time series data mining tasks, such as clustering and classification. For this reason, a vast portfolio of time series distance measures has been published in the past few years. In this paper, the TSdist package is presented, a complete tool which provides a unified framework to calculate the largest variety of time series dissimilarity measures available in R at the moment, to the best of our knowledge. The package implements some popular distance measures which were not previously available in R, and moreover, it also provides wrappers for measures already included in other R packages. Additionally, the application of these distance measures to clustering and classification tasks is also supported in TSdist, directly enabling the evaluation and comparison of their performance within these two frameworks.},
  citeulike-article-id = {14435137},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-09-21 00:20:48},
  timestamp            = {2020-02-29 18:44},
}

@Article{Nicolae-et-al-2016,
  author               = {Nicolae, Maria-Irina and Gaussier, Eric and Habrard, Amaury and Sebban, Marc},
  date                 = {2016-10},
  journaltitle         = {arXiv e-Print},
  title                = {Similarity Learning for Time Series Classification},
  eprint               = {1610.04783},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1610.04783},
  abstract             = {Multivariate time series naturally exist in many fields, like energy, bioinformatics, signal processing, and finance. Most of these applications need to be able to compare these structured data. In this context, dynamic time warping (DTW) is probably the most common comparison measure. However, not much research effort has been put into improving it by learning. In this paper, we propose a novel method for learning similarities based on DTW, in order to improve time series classification. Making use of the uniform stability framework, we provide the first theoretical guarantees in the form of a generalization bound for linear classification. The experimental study shows that the proposed approach is efficient, while yielding sparse classifiers.},
  citeulike-article-id = {14167362},
  citeulike-linkout-0  = {http://arxiv.org/abs/1610.04783},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1610.04783},
  day                  = {15},
  groups               = {Scenario_TimeSeries},
  owner                = {cristi},
  posted-at            = {2016-10-29 22:25:52},
  timestamp            = {2020-02-29 18:44},
}

@Article{Otranto-2010a,
  author               = {Otranto, Edoardo},
  date                 = {2010-01},
  journaltitle         = {Computational Statistics \& Data Analysis},
  title                = {Identifying financial time series with similar dynamic conditional correlation},
  doi                  = {10.1016/j.csda.2009.07.026},
  issn                 = {0167-9473},
  number               = {1},
  pages                = {1--15},
  volume               = {54},
  abstract             = {One of the main problems in modelling multivariate conditional covariance time series is the parameterization of the correlation structure. If no constraints are imposed, it implies a large number of unknown coefficients. The most popular models propose parsimonious representations, imposing similar correlation structures to all the series or to groups of time series, but the choice of these groups is quite subjective. A statistical approach is proposed to detect groups of homogeneous time series in terms of correlation dynamics for one of the widely used models: the Dynamic Conditional Correlation model. The approach is based on a clustering algorithm, which uses the idea of distance between dynamic conditional correlations, and the classical Wald test, to compare the coefficients of two groups of dynamic conditional correlations. The proposed approach is evaluated in terms of simulation experiments and applied to a set of financial time series.},
  citeulike-article-id = {5381887},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=1621145.1621317},
  citeulike-linkout-1  = {http://dx.doi.org/10.1016/j.csda.2009.07.026},
  day                  = {01},
  groups               = {Scenario_TimeSeries},
  location             = {Amsterdam, The Netherlands, The Netherlands},
  owner                = {cristi},
  posted-at            = {2016-03-27 16:52:40},
  publisher            = {Elsevier Science Publishers B. V.},
  timestamp            = {2020-02-29 18:44},
}

@Article{Li-Floudas-2014,
  author               = {Li, Zukui and Floudas, Christodoulos A.},
  date                 = {2014-11},
  journaltitle         = {Computers and Chemical Engineering},
  title                = {Optimal scenario reduction framework based on distance of uncertainty distribution and output performance: I. Single reduction via mixed integer linear optimization},
  doi                  = {10.1016/j.compchemeng.2014.03.019},
  issn                 = {0098-1354},
  pages                = {50--66},
  volume               = {70},
  abstract             = {Proposed a novel scenario reduction algorithm considering both input and output space probabilistic distance and output performance of decision making. Minimizes the differences on the best, worst and expected performance of the output measure of the original and the reduced scenario distributions. Developed a novel mixed integer linear optimization based problem formulation. A number of case studies demonstrating and comparing the performance of the proposed method with existing tools. Realistic decision making involves consideration of uncertainty in various parameters. While large number of scenarios brings significant challenge to computations, the scenario reduction aims at selecting a small number of representative scenarios that can capture the wide range of possible scenarios. A novel scenario reduction algorithm is proposed in this paper to incorporate the consideration of both input data and output performance of decision making. The proposed optimal scenario reduction algorithm, OSCAR, is formulated as a mixed integer linear optimization problem. It minimizes not only the probabilistic distance between the original and reduced input scenario distribution, but also minimizes the differences between the best, worst and expected performances of the output measure of the original and the reduced scenario distributions. The proposed method leads to reduced distribution not only closer to the original distribution in terms of the transportation distance, but also captures the performance of the output.},
  citeulike-article-id = {14171316},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.compchemeng.2014.03.019},
  groups               = {Scenario_Best, Optimiz_MixInt},
  owner                = {cristi},
  posted-at            = {2016-10-25 05:19:48},
  timestamp            = {2020-02-29 18:45},
}

@Article{Jacobs-et-al-2005,
  author               = {Jacobs, Bruce I. and Levy, Kenneth N. and Markowitz, Harry M.},
  date                 = {2005-08},
  journaltitle         = {Operations Research},
  title                = {Portfolio Optimization with Factors, Scenarios, and Realistic Short Positions},
  doi                  = {10.1287/opre.1050.0212},
  issn                 = {0030-364X},
  number               = {4},
  pages                = {586--599},
  volume               = {53},
  abstract             = {This paper presents fast algorithms for calculating mean-variance efficient frontiers when the investor can sell securities short as well as buy long, and when a factor and/or scenario model of covariance is assumed. Currently, fast algorithms for factor, scenario, or mixed (factor and scenario) models exist, but (except for a special case of the results reported here) apply only to portfolios of long positions. Factor and scenario models are used widely in applied portfolio analysis, and short sales have been used increasingly as part of large institutional portfolios. Generally, the critical line algorithm (CLA) traces out mean-variance efficient sets when the investor's choice is subject to any system of linear equality or inequality constraints. Versions of CLA that take advantage of factor and/or scenario models of covariance gain speed by greatly simplifying the equations for segments of the efficient set. These same algorithms can be used, unchanged, for the long-short portfolio selection problem provided a certain condition on the constraint set holds. This condition usually holds in practice.},
  citeulike-article-id = {14310468},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/opre.1050.0212},
  groups               = {PortfOptim_FullScale, PortfOptim_Scenario, Scenario_Portfolio},
  posted-at            = {2017-03-14 02:50:31},
  timestamp            = {2020-02-29 18:45},
}

@Article{Koskosidis-Duarte-1997,
  author               = {Koskosidis, Yiannis A. and Duarte, Antonio M.},
  date                 = {1997},
  journaltitle         = {The Journal of Portfolio Management},
  title                = {A Scenario-Based Approach to Active Asset Allocation},
  number               = {2},
  pages                = {74-85+},
  url                  = {https://jpm.pm-research.com/content/23/2/74},
  volume               = {23},
  abstract             = {The systematic diversification of investment portfolios through proper asset allocation is widely recognized as an important strategy for risk reduction and return enhancement.

This article presents an optimization based asset allocation framework, which employs stochastic optimization and scenario analysis to handle uncertainties associated with forecasting the expected returns, volatilities and cross correlations of the assets. It enables investors to incorporate their forward views on market expected returns, in the optimization phase. Finally, it allows the optimization process to determine the degree of currency exposure, hedging, in multicurrency portfolios.

By using multiple scenarios, the model preserves the diversity of asset behavior, instead of flattening performance into some sort of aggregate average measurement. Scenario analysis helps investors to avoid pitfalls associated with single point forecasting, while it encompasses a wide variety of possible return outcomes so they can properly diversify their portfolios. The framework for blending forward views with historical scenarios represents a flexible tool that allows investors to overlay their future expectations on historical patterns of returns. Finally, the model incorporates currency hedging decisions for multicurrency portfolios directly into the optimization process.},
  citeulike-article-id = {7157953},
  citeulike-linkout-0  = {http://search.ebscohost.com/login.aspx?direct=true and 38;db=buh and 38;AN=11909984 and 38;site=ehost-live},
  groups               = {Scenario generation, PortfOptim_Scenario, Scenario_Risk, Scenario_Portfolio},
  isbn                 = {00954918},
  keywords             = {allocation, asset, capitalists, expected, finance, financiers, hedging, investment, management, mathematical, optimization, policy, portfolio, returns, risk},
  owner                = {zkgst0c},
  posted-at            = {2016-01-10 01:12:14},
  timestamp            = {2020-02-29 18:45},
}

@Article{Markowitz-Perold-1981,
  author       = {Markowitz, H.M and Perold, F.},
  date         = {1981},
  journaltitle = {Finance},
  title        = {Portfolio Analysis with Factors and Scenarios},
  groups       = {PortfOptim_Scenario, Scenario_Portfolio},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 18:45},
}

@Article{Menkens-2014,
  author               = {Menkens, Olaf},
  date                 = {2014-02},
  journaltitle         = {SSRN e-Print},
  title                = {Worst-Case Scenario Portfolio Optimization Given the Probability of a Crash},
  url                  = {https://ssrn.com/abstract=2397263},
  abstract             = {Korn and Wilmott (2002) introduced the worst-case scenario portfolio problem. Although Korn and Wilmott assume that the probability of a crash occurring is unknown, this paper analyses how the worst-case scenario portfolio problem is affected if the probability of a crash occurring is known. The result is that the additional information of the known probability is not used in the worst-case scenario. This leads to a q-quantile approach (instead of a worst-case), which is a Value at Risk-style approach in the optimal portfolio problem with respect to the potential crash. Finally, it will be shown that --- under suitable conditions --- every stochastic portfolio strategy has at least one superior deterministic portfolio strategy within this approach.},
  citeulike-article-id = {13997435},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2397263},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2577446code459356.pdf?abstractid=2397263 and mirid=1},
  day                  = {19},
  groups               = {Scenario generation, Bubble_Crash, Scenario_Risk, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-04-05 05:00:01},
  timestamp            = {2020-02-29 18:45},
}

@Article{Mausser-Romanko-2012,
  author               = {Mausser, Helmut and Romanko, Oleksandr},
  date                 = {2012-10},
  journaltitle         = {Optimization},
  title                = {Bias, exploitation and proxies in scenario-based risk minimization},
  doi                  = {10.1080/02331934.2012.684795},
  number               = {10},
  pages                = {1191--1219},
  volume               = {61},
  abstract             = {When minimizing a risk measure over a set of scenarios, solutions are often optimistic in the sense that the in-sample, or perceived, risk is much less than the out-of-sample risk. Optimism, which can be attributed to the bias of the risk estimator and the exploitation of the scenarios' idiosyncracies by the optimization, increases with the amount of sampling error inherent in the scenarios and the flexibility afforded by the problem's formulation. Minimizing a proxy, namely an estimator of a risk measure different from the one that is actually of interest, can reduce optimism and improve out-of-sample performance. The effectiveness of a proxy depends on the sizes of its perceived risk, bias and exploitation in relation to those of the estimator for the risk measure of interest.},
  citeulike-article-id = {13989286},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/02331934.2012.684795},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/02331934.2012.684795},
  day                  = {1},
  groups               = {Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-03-28 03:28:26},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-29 18:45},
}

@Article{Morris-2013,
  author               = {Morris, I.},
  date                 = {2013},
  journaltitle         = {SSRN e-Print},
  title                = {A Scenario-Based Approach to Asset Allocation},
  url                  = {https://www.investmentmagazine.com.au/wp-content/uploads/2013/05/5.-Ian-Morris-BLACKSTONE.pdf},
  abstract             = {Different approaches to asset allocation are available to practitioners. But reliance on selection of parametric return distributions, summary measures of risk and historical data as an indicator of the future remain widespread.

This research paper proposes an approach that makes more complete use of information available about the future, forcing consideration of different time frames, alternate outcomes, and tail risk. It does this not by forecasting the future but by describing what has the potential to make a significant difference to long-term investment outcomes.},
  citeulike-article-id = {13911262},
  groups               = {Scenario generation, Scenario_Risk, FrcstQWIM_MedLngTerm},
  howpublished         = {Available at http://investmentmagazine.com.au/wp-content/uploads/2013/05/5.-Ian-Morris-BLACKSTONE.pdf},
  owner                = {cristi},
  posted-at            = {2016-01-18 04:23:53},
  timestamp            = {2020-02-29 18:45},
}

@Article{Koo-et-al-2019,
  author         = {Koo, Bonsoo and La Vecchia, Davide and Linton, Oliver B.},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Estimation of a Nonparametric model for Bond Prices from Cross-section and Time series Information},
  doi            = {10.2139/ssrn.3341344},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3341344},
  urldate        = {2019-03-07},
  abstract       = {We develop estimation methodology for an additive nonparametric panel model that is suitable for capturing the pricing of coupon-paying government bonds followed over many time periods. We use our model to estimate the discount function and yield curve of nominally riskless government bonds. The novelty of our approach is the combination of two different techniques: cross-sectional nonparametric methods and kernel estimation for time varying dynamics in the time series context. The resulting estimator is used for predicting individual bond prices given the full schedule of their future payments. In addition, it is able to capture the yield curve shapes and dynamics commonly observed in the fixed income markets. We establish the consistency, the rate of convergence, and the asymptotic normality of the proposed estimator. A Monte Carlo exercise illustrates the good performance of the method under different scenarios. We apply our methodology to the daily CRSP bond market dataset, and compare ours with the popular Diebold and Li (2006) method.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_YieldCurve},
  timestamp      = {2020-02-29 18:46},
}

@Article{Lopes-Vazquez-2018,
  author         = {Lopes, Sara Dutra and Vazquez, Carlos},
  date           = {2018-07-09},
  journaltitle   = {Applied Mathematical Finance},
  title          = {Real-World Scenarios With Negative Interest Rates based on the LIBOR Market Model},
  doi            = {10.1080/{1350486X}.2018.1492348},
  issn           = {1350-{486X}},
  pages          = {1--17},
  abstract       = {In this article, we present a methodology to simulate the evolution of interest rates under real-world probability measure. More precisely, using the multidimensional Shifted Lognormal LIBOR market model and a specification of the market price of risk vector process, we explain how to perform simulations of the real-world forward rates in the future, using the Euler-Maruyama scheme with a predictor-corrector strategy. The proposed methodology allows for the presence of negative interest rates as currently observed in the markets.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {Real_Yield_Curve, Scenario_YieldCurve},
  timestamp      = {2020-02-29 18:46},
}

@Article{Panchekha-et-al-2018,
  author         = {Panchekha, Alexey and Tull, Robert and Bell, Matthew M.},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Ensemble Active Management},
  doi            = {10.2139/ssrn.3243578},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3243578},
  abstract       = {This White Paper questions the superiority of the traditional Active Management paradigm. Do stand-alone, -expert investment managers or management teams, with well-defined yet rigidly entrenched philosophies and methodologies, deliver optimal results? The conclusion, derived from a database reflecting 30,000 test portfolios and 165 million data points, was that they do not.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:48},
}

@Article{Satopaa-et-al-2014,
  author               = {Satopaa, Ville A. and Baron, Jonathan and Foster, Dean P. and Mellers, Barbara A. and Tetlock, Philip E. and Ungar, Lyle H.},
  date                 = {2014-04},
  journaltitle         = {International Journal of Forecasting},
  title                = {Combining multiple probability predictions using a simple logit model},
  doi                  = {10.1016/j.ijforecast.2013.09.009},
  issn                 = {0169-2070},
  number               = {2},
  pages                = {344--356},
  volume               = {30},
  abstract             = {This paper begins by presenting a simple model of the way in which experts estimate probabilities. The model is then used to construct a likelihood-based aggregation formula for combining multiple probability forecasts.

The resulting aggregator has a simple analytical form that depends on a single, easily-interpretable parameter. This makes it computationally simple, attractive for further development, and robust against overfitting.

Based on a large-scale dataset in which over 1300 experts tried to predict 69 geopolitical events, our aggregator is found to be superior to several widely-used aggregation algorithms.},
  citeulike-article-id = {13932966},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2013.09.009},
  groups               = {Scenario_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-02-15 08:18:56},
  timestamp            = {2020-02-29 18:48},
}

@Article{Silva-et-al-2017,
  author               = {Silva, Thuener and Pinheiro, Placido R. and Poggi, Marcus},
  date                 = {2017-01},
  journaltitle         = {European Journal of Operational Research},
  title                = {A more human-like portfolio optimization approach},
  doi                  = {10.1016/j.ejor.2016.06.018},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {252--260},
  volume               = {256},
  abstract             = {We propose a new method for constructing a portfolio aligned with the investor's profile and personal perspective. We propose a new way to create Black-Litterman views. Our method is more qualitative than other Black-Litterman variants and helps to quantify the subjective views. We analyze the sensitivity of the method through practical experiments. Black and Litterman proposed an improvement to the Markowitz portfolio optimization model. They suggested the construction of views to represent investor's opinion about the future of stocks' returns. However, conceiving these views can be quite confusing. It requires the investor to quantify several subjective parameters. In this article, we propose a new way of creating these views using Verbal Decision Analysis. Questionnaires were designed with the intent of making it easier for investors to express their vision about stocks. Following the ZAPROS methodology, the investor answers sets of questions allowing to determine a Formal Index of Quality (FIQ). The views are then derived from the resulting FIQ. Our approach was implemented and tested on data from the Brazilian Stocks. It allows investors to create a personal risk-return balanced portfolio without the help of an expert. The experiments show that the proposed method mitigates the impact of poor view estimation. Also, one must notice that the method is qualitative and its aim is to create a more efficient portfolio considering the investor's vision.},
  citeulike-article-id = {14336733},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2016.06.018},
  groups               = {Scenario_ExpertView},
  posted-at            = {2017-04-13 11:05:18},
  timestamp            = {2020-02-29 18:48},
}

@Article{Vlaev-et-al-2015,
  author               = {Vlaev, Ivo and Nieboer, Jeroen and Martin, Steve and Dolan, Paul},
  date                 = {2015-03},
  journaltitle         = {Journal of Financial Services Marketing},
  title                = {How behavioural science can improve financial advice services},
  doi                  = {10.1057/fsm.2015.1},
  issn                 = {1363-0539},
  number               = {1},
  pages                = {74--88},
  volume               = {20},
  abstract             = {Evidence from the behavioural sciences, notably economics and psychology, has profoundly changed the way policymakers and practitioners view expert advice to consumers. In this article, we take stock of the behavioural science evidence on financial advice and explore its implications for the profession.

We organise the evidence in a comprehensive theoretical framework that also serves a practical purpose: the design of behaviour change interventions. We suggest various ways in which financial advisers can use the insights from behavioural science to improve the take-up and effectiveness of their advice. Finally, we discuss ethical and practical considerations for the financial advisor wishing to put behavioural science knowledge to use.},
  citeulike-article-id = {13911281},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/fsm.2015.1},
  groups               = {Scenario_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-01-18 04:43:49},
  timestamp            = {2020-02-29 18:48},
}

@Article{Wood-2018,
  author         = {Wood, Michael},
  date           = {2018-03-15},
  journaltitle   = {arXiv e-Print},
  title          = {How sure are we? Two approaches to statistical inference},
  url            = {https://arxiv.org/abs/1803.06214},
  abstract       = {Suppose you are told that taking a statin will reduce your risk of a heart attack or stroke by 3\% in the next ten years, or that women have better emotional intelligence than men. You may wonder how accurate the 3\% is, or how confident we should be about the assertion about women's emotional intelligence, bearing in mind that these conclusions are only based on samples of data? My aim here is to present two statistical approaches to questions like these. Approach 1 is often called null hypothesis testing but I prefer the phrase "baseline hypothesis": this is the standard approach in many areas of inquiry but is fraught with problems. Approach 2 can be viewed as a generalisation of the idea of confidence intervals, or as the application of Bayes' theorem. Unlike Approach 1, Approach 2 provides a tentative estimate of the probability of hypotheses of interest. For both approaches, I explain, from first principles, building only on "common sense" statistical concepts like averages and randomness, both how to derive answers, and the rationale behind the answers. This is achieved by using computer simulation methods (resampling and bootstrapping using a spreadsheet available on the web) which avoid the use of probability distributions (t, normal, etc). Such a minimalist, but reasonably rigorous, analysis is particularly useful in a discipline like statistics which is widely used by people who are not specialists. My intended audience includes both statisticians, and users of statistical methods who are not statistical experts.},
  day            = {15},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ExpertView},
  timestamp      = {2020-02-29 18:48},
}

@Article{Zhou-et-al-2019a,
  author         = {Zhou, Feng and Zhou, Hao-min and Yang, Zhihua and Yang, Lihua},
  date           = {2019-01},
  journaltitle   = {Expert Systems with Applications},
  title          = {EMD2FNN: A strategy combining empirical mode decomposition and factorization machine based neural network for stock market trend prediction},
  doi            = {10.1016/j.eswa.2018.07.065},
  issn           = {0957-4174},
  pages          = {136--151},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0957417418304901},
  volume         = {115},
  abstract       = {Stock market forecasting is a vital component of financial systems. However, the stock prices are highly noisy and non-stationary due to the fact that stock markets are affected by a variety of factors. Predicting stock market trend is usually subject to big challenges. The goal of this paper is to introduce a new hybrid, end-to-end approach containing two stages, the Empirical Mode Decomposition and Factorization Machine based Neural Network (EMD2FNN), to predict the stock market trend. To illustrate the method, we apply EMD2FNN to predict the daily closing prices from the Shanghai Stock Exchange Composite () index, the National Association of Securities Dealers Automated Quotations (NASDAQ) index and the Standard \& Poor 500 Composite Stock Price Index (S\&P 500), which respectively exhibit oscillatory, upward and downward patterns. The results are compared with predictions obtained by other methods, including the neural network (NN) model, the factorization machine based neural network (FNN) model, the empirical mode decomposition based neural network (EMD2NN) model and the wavelet de-noising-based back propagation (WDBP) neural network model. Under the same conditions, the experiments indicate that the proposed methods perform better than the other ones according to the metrics of Mean Absolute Error (MAE), Root Mean Square Error (RMSE) and Mean Absolute Percentage Error (MAPE). Furthermore, we compute the profitability with a simple long-short trading strategy to examine the trading performance of our models in the metrics of Average Annual Return (AAR), Maximum Drawdown (MD), Sharpe Ratio (SR) and AAR/MD. The performances in two different scenarios, when taking or not taking the transaction cost into consideration, are found economically significant.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Hybrid, Scenario_ExpertView, Invest_Network},
  timestamp      = {2020-02-29 18:48},
}

@Article{Paparrizos-Gravano-2017,
  author               = {Paparrizos, John and Gravano, Luis},
  date                 = {2017-06},
  journaltitle         = {ACM Transactions on Database Systems},
  title                = {Fast and Accurate Time-Series Clustering},
  doi                  = {10.1145/3044711},
  issn                 = {0362-5915},
  number               = {2},
  volume               = {42},
  abstract             = {The proliferation and ubiquity of temporal data across many disciplines has generated substantial interest in the analysis and mining of time series. Clustering is one of the most popular data-mining methods, not only due to its exploratory power but also because it is often a preprocessing step or subroutine for other techniques. In this article, we present k-Shape and k-MultiShapes (k-MS), two novel algorithms for time-series clustering. k-Shape and k-MS rely on a scalable iterative refinement procedure. As their distance measure, k-Shape and k-MS use shape-based distance (SBD), a normalized version of the cross-correlation measure, to consider the shapes of time series while comparing them. Based on the properties of SBD, we develop two new methods, namely ShapeExtraction (SE) and MultiShapesExtraction (MSE), to compute cluster centroids that are used in every iteration to update the assignment of time series to clusters. k-Shape relies on SE to compute a single centroid per cluster based on all time series in each cluster. In contrast, k-MS relies on MSE to compute multiple centroids per cluster to account for the proximity and spatial distribution of time series in each cluster. To demonstrate the robustness of SBD, k-Shape, and k-MS, we perform an extensive experimental evaluation on 85 datasets against state-of-the-art distance measures and clustering methods for time series using rigorous statistical analysis. SBD, our efficient and parameter-free distance measure, achieves similar accuracy to Dynamic Time Warping (DTW), a highly accurate but computationally expensive distance measure that requires parameter tuning. For clustering, we compare k-Shape and k-MS against scalable and non-scalable partitional, hierarchical, spectral, density-based, and shapelet-based methods, with combinations of the most competitive distance measures. k-Shape outperforms all scalable methods in terms of accuracy. Furthermore, k-Shape also outperforms all non-scalable approaches, with one exception, namely k-medoids with DTW, which achieves similar accuracy. However, unlike k-Shape, this approach requires tuning of its distance measure and is significantly slower than k-Shape. k-MS performs similarly to k-Shape in comparison to rival methods, but k-MS is significantly more accurate than k-Shape. Beyond clustering, we demonstrate the effectiveness of k-Shape to reduce the search space of one-nearest-neighbor classifiers for time series. Overall, SBD, k-Shape, and k-MS emerge as domain-independent, highly accurate, and efficient methods for time-series comparison and clustering with broad applications.},
  citeulike-article-id = {14435136},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=3086510.3044711},
  citeulike-linkout-1  = {http://dx.doi.org/10.1145/3044711},
  groups               = {Scenario_TimeSeries},
  location             = {New York, NY, USA},
  posted-at            = {2017-09-21 00:10:48},
  publisher            = {ACM},
  timestamp            = {2020-02-29 18:48},
}

@Article{Phan-et-al-2018,
  author               = {Phan, Thi-Thu-Hong and Poisson Caillault, Emilie and Lefebvre, Alain and Bigand, Andre},
  date                 = {2018-08},
  journaltitle         = {Pattern Recognition Letters},
  title                = {Dynamic time warping-based imputation for univariate time series data},
  doi                  = {10.1016/j.patrec.2017.08.019},
  issn                 = {0167-8655},
  abstract             = {Time series with missing values occur in almost any domain of applied sciences. Ignoring missing values can lead to a loss of efficiency and unreliable results, especially for large missing sub-sequence(s). This paper proposes an approach to fill in large gap(s) within time series data under the assumption of effective information. To obtain the imputation of missing values, we find the most similar sub-sequence to the sub-sequence before (resp. after) the missing values, then complete the gap by the next (resp. previous) sub-sequence of the most similar one. Dynamic Time Warping algorithm is applied to compare sub-sequences, and combined with the shape-feature extraction algorithm for reducing insignificant solutions. Eight well-known and real-world data sets are used for evaluating the performance of the proposed approach in comparison with five other methods on different indicators. The obtained results proved that the performance of our approach is the most robust one in case of time series data having high auto-correlation and cross-correlation, strong seasonality, large gap(s), and complex distribution.},
  citeulike-article-id = {14499072},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patrec.2017.08.019},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-12-08 00:04:43},
  timestamp            = {2020-02-29 18:48},
}

@Article{Yeh-2018,
  author         = {Yeh, Chin-Chia Michael},
  date           = {2018-11-05},
  journaltitle   = {arXiv e-Print},
  title          = {Towards a Near Universal Time Series Data Mining Tool: Introducing the Matrix Profile},
  url            = {https://arxiv.org/abs/1811.03064},
  abstract       = {The last decade has seen a flurry of research on all-pairs-similarity-search (or, self-join) for text, DNA, and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. Surprisingly, however, little progress has been made on addressing this problem for time series subsequences. In this thesis, we have introduced a near universal time series data mining tool called matrix profile which solves the all-pairs-similarity-search problem and caches the output in an easy-to-access fashion. The proposed algorithm is not only parameter-free, exact and scalable, but also applicable for both single and multidimensional time series. By building time series data mining methods on top of matrix profile, many time series data mining tasks (e.g., motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering) can be efficiently solved. Because the same matrix profile can be shared by a diverse set of time series data mining methods, matrix profile is versatile and computed-once-use-many-times data structure. We demonstrate the utility of matrix profile for many time series data mining problems, including motif discovery, discord discovery, weakly labeled time series classification, and representation learning on domains as diverse as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We hope the matrix profile is not the end but the beginning of many more time series data mining projects.},
  day            = {5},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries, ML_ClustTimeSrs},
  timestamp      = {2020-02-29 18:48},
}

@Article{Yeh-et-al-2017,
  author         = {Yeh, Chin-Chia Michael and Kavantzas, Nickolas and Keogh, Eamonn},
  date           = {2017-08-01},
  journaltitle   = {Proceedings of the VLDB Endowment},
  title          = {Matrix profile IV},
  doi            = {10.14778/3137765.3137784},
  issn           = {2150-8097},
  number         = {12},
  pages          = {1802--1812},
  volume         = {10},
  abstract       = {In academic settings over the last decade, there has been significant progress in time series classification. However, much of this work makes assumptions that are simply unrealistic for deployed industrial applications. Examples of these unrealistic assumptions include the following: assuming that data subsequences have a single fixed-length, are precisely extracted from the data, and are correctly labeled according to their membership in a set of equal-size classes. In real-world industrial settings, these patterns can be of different lengths, the class annotations may only belong to a general region of the data, may contain errors, and finally, the class distribution is typically highly skewed. Can we learn from such weakly labeled data? In this work, we introduce SDTS, a scalable algorithm that can learn in such challenging settings. We demonstrate the utility of our ideas by learning from diverse datasets with millions of datapoints. As we shall demonstrate, our domain-agnostic parameter-free algorithm can be competitive with domain-specific algorithms used in neuroscience and entomology, even when those algorithms have been tuned by domain experts to incorporate domain knowledge.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:48},
}

@Article{Yeh-et-al-2017a,
  author         = {Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Zimmerman, Zachary and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn},
  date           = {2017-06-24},
  journaltitle   = {Data Min Knowl Discov},
  title          = {Time series joins, motifs, discords and shapelets: a unifying view that exploits the matrix profile},
  doi            = {10.1007/s10618-017-0519-9},
  issn           = {1384-5810},
  number         = {1},
  pages          = {1--41},
  volume         = {32},
  abstract       = {The last decade has seen a flurry of research on all-pairs-similarity-search (or similarity joins) for text, DNA and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. However, there has been surprisingly little progress made on similarity joins for time series subsequences. The lack of progress probably stems from the daunting nature of the problem. For even modest sized datasets the obvious nested-loop algorithm can take months, and the typical speed-up techniques in this domain (i.e., indexing, lower-bounding, triangular-inequality pruning and early abandoning) at best produce only one or two orders of magnitude speedup. In this work we introduce a novel scalable algorithm for time series subsequence all-pairs-similarity-search. For exceptionally large datasets, the algorithm can be trivially cast as an anytime algorithm and produce high-quality approximate solutions in reasonable time and/or be accelerated by a trivial porting to a GPU framework. The exact similarity join algorithm computes the answer to the time series motif and time series discord problem as a side-effect, and our algorithm incidentally provides the fastest known algorithm for both these extensively-studied problems. We demonstrate the utility of our ideas for many time series data mining problems, including motif discovery, novelty discovery, shapelet discovery, semantic segmentation, density estimation, and contrast set mining. Moreover, we demonstrate the utility of our ideas on domains as diverse as seismology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring and medicine.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:48},
}

@Article{Yin-Shang-2014,
  author               = {Yin, Yi and Shang, Pengjian},
  date                 = {2014-09},
  journaltitle         = {Chaos: An Interdisciplinary Journal of Nonlinear Science},
  title                = {Asymmetric multiscale detrended cross-correlation analysis of financial time series},
  doi                  = {10.1063/1.4893442},
  issn                 = {1054-1500},
  number               = {3},
  pages                = {032101+},
  volume               = {24},
  abstract             = {We propose the asymmetric multiscale detrended cross-correlation analysis (MS-ADCCA) method and apply MS-ADCCA method to explore the existence of asymmetric cross-correlation for daily price returns in US and Chinese stock markets and to assess the properties of these asymmetric cross-correlations. The results all show the existences of asymmetric cross-correlations, while small asymmetries at small scales and larger asymmetries at larger scales are also displayed. There is a strong similarity between S and amp;P500 and DJI, and we reveal that the asymmetries depend more on the cross-correlations of S and amp;P500 vs. DJI, S\&P 500 vs. NQCI, DJI vs. NQCI, and ShangZheng vs. ShenCheng when the market is falling than rising, respectively. By comparing the spectra of S\&P 500 vs. NQCI and DJI vs. NQCI with uptrends and downtrends, we detect some new characteristics which lead to some new conclusions. Likewise, some new conclusions also can be drawn by the new characteristics displayed through the comparison between the spectra of ShangZheng vs. HSI and ShenCheng vs. HSI. Obviously, we conclude that although the overall spectra are similar and one market has the same effect when it is rising and falling in the study of asymmetric cross-correlations between it and different markets, the cross-correlations and asymmetries on the trends of the different markets are all different. MS-ADCCA method can detect the differences on the asymmetric cross-correlations by different trends of markets. Moreover, the uniqueness of cross-correlation between NQCI and HSI can be detected in the study of the asymmetric cross-correlations, which confirms that HSI is unique in the Chinese stock markets and NQCI is unique in the US stock markets further.},
  citeulike-article-id = {13989271},
  citeulike-linkout-0  = {http://dx.doi.org/10.1063/1.4893442},
  day                  = {01},
  groups               = {Scenario_TimeSeries},
  owner                = {cristi},
  posted-at            = {2016-03-28 03:56:58},
  timestamp            = {2020-02-29 18:48},
}

@Article{Zhao-Itti-2018,
  author               = {Zhao, Jiaping and Itti, Laurent},
  date                 = {2018-02},
  journaltitle         = {Pattern Recognition},
  title                = {shapeDTW: Shape Dynamic Time Warping},
  doi                  = {10.1016/j.patcog.2017.09.020},
  issn                 = {0031-3203},
  pages                = {171--184},
  volume               = {74},
  abstract             = {Dynamic Time Warping (DTW) is an algorithm to align temporal sequences with possible local non-linear distortions, and has been widely applied to audio, video and graphics data alignments. DTW is essentially a point-to-point matching method under some boundary and temporal consistency constraints. Although DTW obtains a global optimal solution, it does not necessarily achieve locally sensible matchings. Concretely, two temporal points with entirely dissimilar local structures may be matched by DTW. To address this problem, we propose an improved alignment algorithm, named shape Dynamic Time Warping (shapeDTW), which enhances DTW by taking point-wise local structural information into consideration. shapeDTW is inherently a DTW algorithm, but additionally attempts to pair locally similar structures and to avoid matching points with distinct neighborhood structures. We apply shapeDTW to align audio signal pairs having ground-truth alignments, as well as artificially simulated pairs of aligned sequences, and obtain quantitatively much lower alignment errors than DTW and its two variants. When shapeDTW is used as a distance measure in a nearest neighbor classifier (NN-shapeDTW) to classify time series, it beats DTW on 64 out of 84 UCR time series datasets, with significantly improved classification accuracies. By using a properly designed local structure descriptor, shapeDTW improves accuracies by more than 10\% on 18 datasets. To the best of our knowledge, shapeDTW is the first distance measure under the nearest neighbor classifier scheme to significantly outperform DTW, which had been widely recognized as the best distance measure to date. Our code is publicly accessible at: https://github.com/jiapingz/shapeDTW.},
  citeulike-article-id = {14468784},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2017.09.020},
  groups               = {Scenario_TimeSeries},
  posted-at            = {2017-12-08 00:09:08},
  timestamp            = {2020-02-29 18:48},
}

@InCollection{Zhu-et-al-2017a,
  author         = {Zhu, Yan and Imamura, Makoto and Nikovski, Daniel and Keogh, Eamonn},
  booktitle      = {2017 IEEE International Conference on Data Mining (ICDM)},
  date           = {2017-11-18},
  title          = {Matrix profile VII: time series chains: A new primitive for time series data mining (best student paper award)},
  doi            = {10.1109/{ICDM}.2017.79},
  isbn           = {978-1-5386-3835-4},
  pages          = {695--704},
  publisher      = {IEEE},
  abstract       = {Since their introduction over a decade ago, time series motifs have become a fundamental tool for time series analytics, finding diverse uses in dozens of domains. In this work we introduce Time Series Chains, which are related to, but distinct from, time series motifs. Informally, time series chains are a temporally ordered set of subsequence patterns, such that each pattern is similar to the pattern that preceded it, but the first and last patterns are arbitrarily dissimilar. In the discrete space, this is similar to extracting the text chain "hit, hot, dot, dog" from a paragraph. The first and last words have nothing in common, yet they are connected by a chain of words with a small mutual difference. Time series chains can capture the evolution of systems, and help predict the future. As such, they potentially have implications for prognostics. In this work, we introduce a robust definition of time series chains, and a scalable algorithm that allows us to discover them in massive datasets.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:48},
}

@InCollection{Zhu-et-al-2018a,
  author         = {Zhu, Yan and Imamura, Makoto and Nikovski, Daniel and Keogh, Eamonn},
  booktitle      = {Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence},
  date           = {2018-07-13},
  title          = {Time series chains: A novel tool for time series data mining},
  doi            = {10.24963/ijcai.2018/764},
  editor         = {Lang, Jerome},
  isbn           = {9780999241127},
  location       = {California},
  pages          = {5414--5418},
  publisher      = {International Joint Conferences on Artificial Intelligence Organization},
  abstract       = {Since their introduction over a decade ago, time se-ries motifs have become a fundamental tool for time series analytics, finding diverse uses in dozens of domains. In this work we introduce Time Series Chains, which are related to, but distinct from, time series motifs. Informally, time series chains are a temporally ordered set of subsequence patterns, such that each pattern is similar to the pattern that preceded it, but the first and last patterns are arbi-trarily dissimilar. In the discrete space, this is simi-lar to extracting the text chain “hit, hot, dot, dog” from a paragraph. The first and last words have nothing in common, yet they are connected by a chain of words with a small mutual difference. Time Series Chains can capture the evolution of systems, and help predict the future. As such, they potentially have implications for prognostics. In this work, we introduce a robust definition of time series chains, and a scalable algorithm that allows us to discover them in massive datasets.},
  day            = {13},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:48},
}

@Article{Zhu-et-al-2018b,
  author         = {Zhu, Yan and Imamura, Makoto and Nikovski, Daniel and Keogh, Eamonn},
  date           = {2019},
  journaltitle   = {Knowledge and Information Systems},
  title          = {Introducing time series chains: a new primitive for time series data mining},
  doi            = {10.1007/s10115-018-1224-8},
  issn           = {0219-1377},
  pages          = {1135-11611--27},
  url            = {https://link.springer.com/article/10.1007/s10115-018-1224-8},
  volume         = {60},
  abstract       = {Time series motifs were introduced in 2002 and have since become a fundamental tool for time series analytics, finding diverse uses in dozens of domains. In this work, we introduce Time Series Chains, which are related to, but distinct from, time series motifs. Informally, time series chains are a temporally ordered set of subsequence patterns, such that each pattern is similar to the pattern that preceded it, but the first and last patterns can be arbitrarily dissimilar. In the discrete space, this is similar to extracting the text chain , date, cate, cade, code from text stream. The first and last words have nothing in common, yet they are connected by a chain of words with a small mutual difference. Time series chains can capture the evolution of systems, and help predict the future. As such, they potentially have implications for prognostics. In this work, we introduce two robust definitions of time series chains and scalable algorithms that allow us to discover them in massive complex datasets.},
  day            = {2},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 18:48},
}

@Article{Schottle-et-al-2010,
  author               = {Schottle, Katrin and Werner, Ralf and Zagst, Rudi},
  date                 = {2010-02},
  journaltitle         = {Mathematical Methods of Operations Research},
  title                = {Comparison and robustification of Bayes and Black-Litterman models},
  doi                  = {10.1007/s00186-010-0302-9},
  issn                 = {1432-2994},
  number               = {3},
  pages                = {453--475},
  volume               = {71},
  abstract             = {For determining an optimal portfolio allocation, parameters representing the underlying market characterized by expected asset returns and the covariance matrix are needed. Traditionally, these point estimates for the parameters are obtained from historical data samples, but as experts often have strong opinions about (some of) these values, approaches to combine sample information and experts' views are sought for.

The focus of this paper is on the two most popular of these frameworks the Black-Litterman model and the Bayes approach. We will prove that from the point of traditional portfolio optimization the Black-Litterman is just a special case of the Bayes approach. In contrast to this, we will show that the extensions of both models to the robust portfolio framework yield two rather different robustified optimization problems.},
  citeulike-article-id = {6844196},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00186-010-0302-9},
  citeulike-linkout-1  = {http://www.springerlink.com/content/53t3712240246234},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1007/s00186-010-0302-9},
  day                  = {25},
  groups               = {Black_Litterman, PortfOptim_Bayes, Scenario_ExpertView},
  owner                = {cristi},
  posted-at            = {2016-03-11 21:56:36},
  publisher            = {Springer-Verlag},
  timestamp            = {2020-02-29 18:49},
}

@Article{Ziemba-2013,
  author               = {Ziemba, William T.},
  date                 = {2013-12},
  journaltitle         = {Quantitative Finance Letters},
  title                = {The case for convex risk measures and scenario-dependent correlation matrices to replace VaR, C-VaR and covariance simulations for safer risk control of portfolios},
  number               = {1},
  pages                = {47--54},
  volume               = {1},
  abstract             = {Value at risk (VaR) is the most popular risk measure and is enshrined in various regulations. It postulates that portfolio losses are less than some prescribed amount most of the time. Therefore a loss of 10 million is the same as a loss of 5 billion. C-VaR tries to correct this by linearly penalizing the loss so a loss of 20 million is twice as damaging as that of 10 million with the same probability. T

his is an improvement but is not enough of a penalty to force investment portfolios to be structured to avoid these losses. The author has used convex risk measures since 1974 in various asset liability management (ALM) models such as the Russell Yasuda Kasai and the Vienna InnoALM. They penalize losses at a much greater rate than linear rate so that double or triple losses are more than two or three times as undesirable.

Also scenario-dependent correlation matrices are very important in model applications because ordinary average correlations tend to work when you do not need them and fail by giving misleading results when you need them. For example, in stock market crash situations, bonds and stocks are no longer positively correlated. Adding these two features to stochastic asset liability planning models is a big step towards improving risk control and performance.},
  citeulike-article-id = {13922880},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/21649502.2013.803757},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/21649502.2013.803757},
  day                  = {1},
  groups               = {Scenario generation, Scenario_Risk},
  journal              = {Quantitative Finance Letters},
  owner                = {zkgst0c},
  posted-at            = {2016-01-31 17:45:47},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-29 18:51},
  year                 = {2013},
}

@Article{Philps-et-al-2018,
  author         = {Philps, Daniel and Weyde, Tillman and Garcez, Artur d'Avila and Batchelor, Roy},
  date           = {2018-12-06},
  journaltitle   = {arXiv e-Print},
  title          = {Continual Learning Augmented Investment Decisions},
  url            = {https://arxiv.org/abs/1812.02340},
  urldate        = {2019-09-06},
  abstract       = {Investment decisions can benefit from incorporating an accumulated knowledge of the past to drive future decision making. We introduce Continual Learning Augmentation (CLA) which is based on an explicit memory structure and a feed forward neural network (FFNN) base model and used to drive long term financial investment decisions. We demonstrate that our approach improves accuracy in investment decision making while memory is addressed in an explainable way. Our approach introduces novel remember cues, consisting of empirically learned change points in the absolute error series of the FFNN. Memory recall is also novel, with contextual similarity assessed over time by sampling distances using dynamic time warping (DTW). We demonstrate the benefits of our approach by using it in an expected return forecasting task to drive investment decisions. In an investment simulation in a broad international equity universe between 2003-2017, our approach significantly outperforms FFNN base models. We also illustrate how CLA's memory addressing works in practice, using a worked example to demonstrate the explainability of our approach.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {ML_ForcstTimeSrs, ML_Text_QWIM},
  timestamp      = {2020-02-29 19:14},
}

@Article{Ramponi-et-al-2018,
  author         = {Ramponi, Giorgia and Protopapas, Pavlos and Brambilla, Marco and Janssen, Ryan},
  date           = {2018-11-20},
  journaltitle   = {arXiv e-Print},
  title          = {T-CGAN: Conditional Generative Adversarial Network for Data Augmentation in Noisy Time Series with Irregular Sampling},
  url            = {https://arxiv.org/abs/1811.08295},
  urldate        = {2019-04-10},
  abstract       = {In this paper we propose a data augmentation method for time series with irregular sampling, Time-Conditional Generative Adversarial Network (T-CGAN). Our approach is based on Conditional Generative Adversarial Networks (CGAN), where the generative step is implemented by a deconvolutional NN and the discriminative step by a convolutional NN. Both the generator and the discriminator are conditioned on the sampling timestamps, to learn the hidden relationship between data and timestamps, and consequently to generate new time series. We evaluate our model with synthetic and real-world datasets. For the synthetic data, we compare the performance of a classifier trained with T-CGAN-generated data, against the performance of the same classifier trained on the original data. Results show that classifiers trained on T-CGAN-generated data perform the same as classifiers trained on real data, even with very short time series and small training sets. For the real world datasets, we compare our method with other techniques of data augmentation for time series, such as time slicing and time warping, over a classification problem with unbalanced datasets. Results show that our method always outperforms the other approaches, both in case of regularly sampled and irregularly sampled time series. We achieve particularly good performance in case with a small training set and short, noisy, irregularly-sampled time series.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:14},
}

@Article{Saxena-Stubbs-2015,
  author               = {Saxena, Anureet and Stubbs, Robert A.},
  date                 = {2015},
  journaltitle         = {Journal of Investment Management},
  title                = {Augmented Risk Models to Mitigate Factor Alignment Problems},
  number               = {3},
  url                  = {https://www.joim.com/augmented-risk-models-to-mitigate-factor-alignment-problems/},
  volume               = {13},
  abstract             = {Construction of optimized portfolios entails a complex interaction between three key entities, namely, the risk factors, the alpha factors and the constraints. The problems that arise due to mutual misalignment between these three entities are collectively referred to as Factor Alignment Problems (FAP). Examples of FAP include risk u underestimation of optimized portfolios, undesirable exposures to factors with hidden and unaccounted systematic risk, consistent failure in achieving ex-ante performance targets, and inability to harvest high quality alphas into above-average IR. In this paper, we give a detailed analysis of FAP and discuss solution approaches based on augmenting the user risk model with a single additional factor y. For the case of unconstrained mean-variance optimization (MVO) problems, we develop a generic analytical framework to analyze the ex-post utility function of the corresponding optimal portfolios, derive a closed-form expression of the optimal factor volatility value and compare the solutions for various choices of y culminating with a closed-form expression for the optimal choice of y. Augmented risk models not only correct for risk underestimation bias of optimal portfolios but also push the ex-post efficient frontier upward thereby empowering a portfolio manager (PM) to access portfolios that lie above the traditional risk-return frontier. We corroborate our theoretical results by extensive computational experiments, and discuss market conditions under which augmented risk models are likely to be most beneficial.},
  citeulike-article-id = {14486843},
  groups               = {PortfOptim_Factor, ExAnte_ExPost},
  posted-at            = {2017-12-01 19:53:53},
  timestamp            = {2020-02-29 19:14},
}

@Article{Taylor-Nitschke-2017,
  author         = {Taylor, Luke and Nitschke, Geoff},
  date           = {2017-08-20},
  journaltitle   = {arXiv e-Print},
  title          = {Improving Deep Learning using Generic Data Augmentation},
  url            = {https://arxiv.org/abs/1708.06020v1},
  urldate        = {2019-04-10},
  abstract       = {Deep artificial neural networks require a large corpus of training data in order to effectively learn, where collection of such training data is often expensive and laborious. Data augmentation overcomes this issue by artificially inflating the training set with label preserving transformations. Recently there has been extensive use of generic data augmentation to improve Convolutional Neural Network (CNN) task performance. This study benchmarks various popular data augmentation schemes to allow researchers to make informed decisions as to which training methods are most appropriate for their data sets. Various geometric and photometric schemes are evaluated on a coarse-grained data set using a relatively simple CNN. Experimental results, run using 4-fold cross-validation and reported in terms of Top-1 and Top-5 accuracy, indicate that cropping in geometric augmentation significantly increases CNN task performance.},
  day            = {20},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:14},
}

@Article{Tran-et-al-2017c,
  author         = {Tran, Toan and Pham, Trung and Carneiro, Gustavo and Palmer, Lyle and Reid, Ian},
  date           = {2017-10-29},
  journaltitle   = {arXiv e-Print},
  title          = {A Bayesian Data Augmentation Approach for Learning Deep Models},
  url            = {https://arxiv.org/abs/1710.10564v1},
  urldate        = {2019-04-10},
  abstract       = {Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm --- generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above --- the results also show that our approach produces better classification results than similar GAN models.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:14},
}

@Article{Volpi-et-al-2018,
  author         = {Volpi, Riccardo and Namkoong, Hongseok and Sener, Ozan and Duchi, John and Murino, Vittorio and Savarese, Silvio},
  date           = {2018-05-30},
  journaltitle   = {arXiv e-Print},
  title          = {Generalizing to Unseen Domains via Adversarial Data Augmentation},
  url            = {https://arxiv.org/abs/1805.12018v2},
  urldate        = {2019-04-10},
  abstract       = {We are concerned with learning models that generalize well to different {unseen} domains. We consider a worst-case formulation over data distributions that are near the source domain in the feature space. Only using training data from a single source distribution, we propose an iterative procedure that augments the dataset with examples from a fictitious target domain that is "hard" under the current model. We show that our iterative scheme is an adaptive data augmentation method where we append adversarial examples at each iteration. For softmax losses, we show that our method is a data-dependent regularization scheme that behaves differently from classical regularizers that regularize towards zero (e.g., ridge or lasso). On digit recognition and semantic segmentation tasks, our method learns models improve performance across a range of a priori unknown target domains.},
  day            = {30},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:14},
}

@Article{Wang-et-al-2019a,
  author         = {Wang, Yuexi and Polson, Nicholas G. and Sokolov, Vadim O.},
  date           = {2019-03-22},
  journaltitle   = {arXiv e-Print},
  title          = {Scalable Data Augmentation for Deep Learning},
  url            = {https://arxiv.org/abs/1903.09668v1},
  urldate        = {2019-04-10},
  abstract       = {Scalable Data Augmentation (SDA) provides a framework for training deep learning models using auxiliary hidden layers. Scalable MCMC is available for network training and inference. SDA provides a number of computational advantages over traditional algorithms, such as avoiding backtracking, local modes and can perform optimization with stochastic gradient descent (SGD) in TensorFlow. Standard deep neural networks with logit, ReLU and SVM activation functions are straightforward to implement. To illustrate our architectures and methodology, we use Polya-Gamma logit data augmentation for a number of standard datasets. Finally, we conclude with directions for future research.},
  day            = {22},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:14},
}

@Article{Warner-2019,
  author         = {Warner, Mike},
  date           = {2019-03-31},
  journaltitle   = {The Journal of Investing},
  title          = {Satori:toward a global augmented intelligence architecture},
  doi            = {10.3905/joi.2019.28.3.088},
  issn           = {1068-0896},
  number         = {3},
  pages          = {88--99},
  urldate        = {2019-04-29},
  volume         = {28},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:14},
}

@Article{Xie-et-al-2019,
  author         = {Xie, Qizhe and Dai, Zihang and Hovy, Eduard and Luong, Minh-Thang and Le, Quoc V.},
  date           = {2019-04-29},
  journaltitle   = {arXiv e-Print},
  title          = {Unsupervised Data Augmentation},
  url            = {https://arxiv.org/abs/1904.12848},
  urldate        = {2019-05-04},
  abstract       = {Despite its success, deep learning still needs large labeled datasets to succeed. Data augmentation has shown much promise in alleviating the need for more labeled data, but it so far has mostly been applied in supervised settings and achieved limited gains. In this work, we propose to apply data augmentation to unlabeled data in a semi-supervised learning setting. Our method, named Unsupervised Data Augmentation or UDA, encourages the model predictions to be consistent between an unlabeled example and an augmented unlabeled example. Unlike previous methods that use random noise such as Gaussian noise or dropout noise, UDA has a small twist in that it makes use of harder and more realistic noise generated by state-of-the-art data augmentation methods. This small twist leads to substantial improvements on six language tasks and three vision tasks even when the labeled set is extremely small. For example, on the IMDb text classification dataset, with only 20 labeled examples, UDA outperforms the state-of-the-art model trained on 25,000 labeled examples. On standard semi-supervised learning benchmarks, CIFAR-10 with 4,000 examples and SVHN with 1,000 examples, UDA outperforms all previous approaches and reduces more than 30\% of the error rates of state-of-the-art methods: going from 7.66\% to 5.27\% and from 3.53\% to 2.46\% respectively. UDA also works well on datasets that have a lot of labeled data. For example, on ImageNet, with 1.3M extra unlabeled data, UDA improves the top-1/top-5 accuracy from 78.28/94.36\% to 79.04/94.45\% when compared to AutoAugment.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:14},
}

@Article{Yamaguchi-et-al-2019,
  author         = {Yamaguchi, Shin'ya and Kanai, Sekitoshi and Eda, Takeharu},
  date           = {2019-12-25},
  journaltitle   = {arXiv e-Print},
  title          = {Effective Data Augmentation with Multi-Domain Learning GANs},
  url            = {https://arxiv.org/abs/1912.11597v1},
  urldate        = {2020-01-01},
  abstract       = {For deep learning applications, the massive data development (e.g., collecting, labeling), which is an essential process in building practical applications, still incurs seriously high costs. In this work, we propose an effective data augmentation method based on generative adversarial networks (GANs), called Domain Fusion. Our key idea is to import the knowledge contained in an outer dataset to a target model by using a multi-domain learning GAN. The multi-domain learning GAN simultaneously learns the outer and target dataset and generates new samples for the target tasks. The simultaneous learning process makes GANs generate the target samples with high fidelity and variety. As a result, we can obtain accurate models for the target tasks by using these generated samples even if we only have an extremely low volume target dataset. We experimentally evaluate the advantages of Domain Fusion in image classification tasks on 3 target datasets: CIFAR-100, FGVC-Aircraft, and Indoor Scene Recognition. When trained on each target dataset reduced the samples to 5,000 images, Domain Fusion achieves better classification accuracy than the data augmentation using fine-tuned GANs. Furthermore, we show that Domain Fusion improves the quality of generated samples, and the improvements can contribute to higher accuracy.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:14},
}

@Article{Zens-Bock-2019,
  author         = {Zens, Gregor and Bock, Maximilian},
  date           = {2019-04-30},
  journaltitle   = {arXiv e-Print},
  title          = {A Factor-Augmented Markov Switching (FAMS) Model},
  url            = {https://arxiv.org/abs/1904.13194},
  urldate        = {2019-05-04},
  abstract       = {This paper investigates the role of high-dimensional information sets in the context of Markov switching models with time varying transition probabilities. Markov switching models are commonly employed in empirical macroeconomic research and policy work. However, the information used to model the switching process is usually limited drastically to ensure stability of the model. Increasing the number of included variables to enlarge the information set might even result in decreasing precision of the model. Moreover, it is often not clear a priori which variables are actually relevant when it comes to informing the switching behavior. Building strongly on recent contributions in the field of dynamic factor analysis, we introduce a general type of Markov switching autoregressive models for non-linear time series analysis. Large numbers of time series are allowed to inform the switching process through a factor structure. This factor-augmented Markov switching (FAMS) model overcomes estimation issues that are likely to arise in previous assessments of the modeling framework. More accurate estimates of the switching behavior as well as improved model fit result. The performance of the FAMS model is illustrated in a simulated data example as well as in an US business cycle application.},
  day            = {30},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:14},
}

@Article{Zhang-et-al-2018i,
  author         = {Zhang, Xiaofeng and Wang, Zhangyang and Liu, Dong and Ling, Qing},
  date           = {2018-08-29},
  journaltitle   = {arXiv e-Print},
  title          = {DADA: Deep Adversarial Data Augmentation for Extremely Low Data Regime Classification},
  url            = {https://arxiv.org/abs/1809.00981v1},
  urldate        = {2019-04-10},
  abstract       = {Deep learning has revolutionized the performance of classification, but meanwhile demands sufficient labeled data for training. Given insufficient data, while many techniques have been developed to help combat overfitting, the challenge remains if one tries to train deep networks, especially in the ill-posed extremely low data regimes: only a small set of labeled data are available, and nothing -- including unlabeled data -- else. Such regimes arise from practical situations where not only data labeling but also data collection itself is expensive. We propose a deep adversarial data augmentation (DADA) technique to address the problem, in which we elaborately formulate data augmentation as a problem of training a class-conditional and supervised generative adversarial network (GAN). Specifically, a new discriminator loss is proposed to fit the goal of data augmentation, through which both real and augmented samples are enforced to contribute to and be consistent in finding the decision boundaries. Tailored training techniques are developed accordingly. To quantitatively validate its effectiveness, we first perform extensive simulations to show that DADA substantially outperforms both traditional data augmentation and a few GAN-based options. We then extend experiments to three real-world small labeled datasets where existing data augmentation and/or transfer learning strategies are either less effective or infeasible. All results endorse the superior capability of DADA in enhancing the generalization ability of deep networks trained in practical extremely low data regimes. Source code is available at this https URL.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment, ML_TransferLrng},
  timestamp      = {2020-02-29 19:14},
}

@Article{Zhang-et-al-2019u,
  author         = {Zhang, Xinyu and Wang, Qiang and Zhang, Jian and Zhong, Zhao},
  date           = {2019-12-24},
  journaltitle   = {arXiv e-Print},
  title          = {Adversarial AutoAugment},
  url            = {https://arxiv.org/abs/1912.11188v1},
  urldate        = {2020-01-19},
  abstract       = {Data augmentation (DA) has been widely utilized to improve generalization in training deep neural networks. Recently, human-designed data augmentation has been gradually replaced by automatically learned augmentation policy. Through finding the best policy in well-designed search space of data augmentation, AutoAugment can significantly improve validation accuracy on image classification tasks. However, this approach is not computationally practical for large-scale problems. In this paper, we develop an adversarial method to arrive at a computationally-affordable solution called Adversarial AutoAugment, which can simultaneously optimize target related object and augmentation policy search loss. The augmentation policy network attempts to increase the training loss of a target network through generating adversarial augmentation policies, while the target network can learn more robust features from harder examples to improve the generalization. In contrast to prior work, we reuse the computation in target network training for policy evaluation, and dispense with the retraining of the target network. Compared to AutoAugment, this leads to about 12x reduction in computing cost and 11x shortening in time overhead on ImageNet. We show experimental results of our approach on CIFAR-10/CIFAR-100, ImageNet, and demonstrate significant performance improvements over state-of-the-art. On CIFAR-10, we achieve a top-1 test error of 1.36\%, which is the currently best performing single model. On ImageNet, we achieve a leading performance of top-1 accuracy 79.40\% on ResNet-50 and 80.00\% on ResNet-50-D without extra data.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:14},
}

@Article{Kim-2014,
  author               = {Kim, Jae H.},
  date                 = {2014-03},
  journaltitle         = {Journal of Empirical Finance},
  title                = {Predictive regression: An improved augmented regression method},
  doi                  = {10.1016/j.jempfin.2014.01.002},
  issn                 = {0927-5398},
  pages                = {13--25},
  volume               = {26},
  abstract             = {This paper proposes three modifications to the augmented regression method. These modifications include improved bias-correction and stationarity-correction. The matrix formula is introduced for covariance matrix estimation. It is found that these modifications deliver substantial gain in small samples. The improved method is applied to monthly US dividend yield and stock return.

This paper proposes three modifications to the augmented regression method (ARM) for bias-reduced estimation and statistical inference in the predictive regression. They are in relation to improved bias-correction, stationarity-correction, and the use of matrix formulae for bias-correction and covariance matrix estimation. The improved ARM parameter estimators are unbiased to the order of n, and always satisfy the condition of stationarity. With the matrix formulae, the improved ARM can easily be implemented for a high order model with multiple predictors.

From an extensive Monte Carlo experiment, it is found that the improved ARM delivers substantial gain in parameter estimation, statistical inference, and out-of-sample forecasting in small samples. As an application, the improved ARM is applied to monthly US stock return data to evaluate the predictive power of dividend yield in univariate and bivariate predictive models.},
  citeulike-article-id = {13932392},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jempfin.2014.01.002},
  groups               = {Regression_Estim},
  owner                = {cristi},
  posted-at            = {2016-02-13 18:00:53},
  timestamp            = {2020-02-29 19:15},
}

@Article{Kuchnik-Smith-2018,
  author         = {Kuchnik, Michael and Smith, Virginia},
  date           = {2018-10-11},
  journaltitle   = {arXiv e-Print},
  title          = {Efficient Augmentation via Data Subsampling},
  url            = {https://arxiv.org/abs/1810.05222v2},
  abstract       = {Data augmentation is commonly used to encode invariances in learning methods. However, this process is often performed in an inefficient manner, as artificial examples are created by applying a number of transformations to all points in the training set. The resulting explosion of the dataset size can be an issue in terms of storage and training costs, as well as in selecting and tuning the optimal set of transformations to apply. In this work, we demonstrate that it is possible to significantly reduce the number of data points included in data augmentation while realizing the same accuracy and invariance benefits of augmenting the entire dataset. We propose a novel set of subsampling policies, based on model influence and loss, that can achieve a 90\% reduction in augmentation set size while maintaining the accuracy gains of standard data augmentation.},
  day            = {11},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:15},
}

@Article{Lee-et-al-2019e,
  author         = {Lee, Hankook and Hwang, Sung Ju and Shin, Jinwoo},
  date           = {2019-10-14},
  journaltitle   = {arXiv e-Print},
  title          = {Rethinking Data Augmentation: Self-Supervision and Self-Distillation},
  url            = {https://arxiv.org/abs/1910.05872v1},
  urldate        = {2019-10-18},
  abstract       = {Data augmentation techniques, e.g., flipping or cropping, which systematically enlarge the training dataset by explicitly generating more training samples, are effective in improving the generalization performance of deep neural networks. In the supervised setting, a common practice for data augmentation is to assign the same label to all augmented samples of the same source. However, if the augmentation results in large distributional discrepancy among them (e.g., rotations), forcing their label invariance may be too difficult to solve and often hurts the performance. To tackle this challenge, we suggest a simple yet effective idea of learning the joint distribution of the original and self-supervised labels of augmented samples. The joint learning framework is easier to train, and enables an aggregated inference combining the predictions from different augmented samples for improving the performance. Further, to speed up the aggregation process, we also propose a knowledge transfer technique, self-distillation, which transfers the knowledge of augmentation into the model itself. We demonstrate the effectiveness of our data augmentation framework on various fully-supervised settings including the few-shot and imbalanced classification scenarios.},
  day            = {14},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Lim-et-al-2019b,
  author         = {Lim, Sungbin and Kim, Ildoo and Kim, Taesup and Kim, Chiheon and Kim, Sungwoong},
  date           = {2019-05-01},
  journaltitle   = {arXiv e-Print},
  title          = {Fast AutoAugment},
  url            = {https://arxiv.org/abs/1905.00397},
  abstract       = {Data augmentation is an indispensable technique to improve generalization and also to deal with imbalanced datasets. Recently, AutoAugment has been proposed to automatically search augmentation policies from a dataset and has significantly improved performances on many image recognition tasks. However, its search method requires thousands of GPU hours to train even in a reduced setting. In this paper, we propose Fast AutoAugment algorithm that learns augmentation policies using a more efficient search strategy based on density matching. In comparison to AutoAugment, the proposed algorithm speeds up the search time by orders of magnitude while maintaining the comparable performances on the image recognition tasks with various models and datasets including CIFAR-10, CIFAR-100, and ImageNet.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Lin-et-al-2019d,
  author         = {Lin, Yijiong and Huang, Jiancong and Zimmer, Matthieu and Rojas, Juan and Weng, Paul},
  date           = {2019-10-19},
  journaltitle   = {arXiv e-Print},
  title          = {Towards More Sample Efficiency in Reinforcement Learning with Data Augmentation},
  url            = {https://arxiv.org/abs/1910.09959},
  urldate        = {2019-10-24},
  abstract       = {Deep reinforcement learning (DRL) is a promising approach for adaptive robot control, but its current application to robotics is currently hindered by high sample requirements. We propose two novel data augmentation techniques for DRL in order to reuse more efficiently observed data. The first one called Kaleidoscope Experience Replay exploits reflectional symmetries, while the second called Goal-augmented Experience Replay takes advantage of lax goal definitions. Our preliminary experimental results show a large increase in learning speed.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@InCollection{Liu-et-al-2018g,
  author         = {Liu, Bo and Wang, Xudong and Dixit, Mandar and Kwitt, Roland and Vasconcelos, Nuno},
  booktitle      = {2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  date           = {2018-06-18},
  title          = {Feature space transfer for data augmentation},
  doi            = {10.1109/{CVPR}.2018.00947},
  isbn           = {978-1-5386-6420-9},
  pages          = {9090--9098},
  publisher      = {IEEE},
  abstract       = {The problem of data augmentation in feature space is considered. A new architecture, denoted the FeATure TransfEr Network (FATTEN), is proposed for the modeling of feature trajectories induced by variations of object pose. This architecture exploits a parametrization of the pose manifold in terms of pose and appearance. This leads to a deep encoder/decoder network architecture, where the encoder factors into an appearance and a pose predictor. Unlike previous attempts at trajectory transfer, FATTEN can be efficiently trained end-to-end, with no need to train separate feature transfer functions. This is realized by supplying the decoder with information about a target pose and the use of a multi-task loss that penalizes category- and pose-mismatches. In result, FATTEN discourages discontinuous or non-smooth trajectories that fail to capture the structure of the pose manifold, and generalizes well on object recognition tasks involving large pose variation. Experimental results on the artificial ModelNet database show that it can successfully learn to map source features to target features of a desired pose, while preserving class identity. Most notably, by using feature space transfer for data augmentation (w.r.t. pose and depth) on SUN-RGBD objects, we demonstrate considerable performance improvements on one/few-shot object recognition in a transfer learning setup, compared to current state-of-the-art methods.},
  day            = {18},
  f1000-projects = {QuantInvest},
  groups         = {ML_TransferLrng},
  timestamp      = {2020-02-29 19:15},
}

@Article{Mollaysa-et-al-2019,
  author         = {Mollaysa, Amina and Kalousis, Alexandros and Bruno, Eric and Diephuis, Maurits},
  date           = {2019-10-15},
  journaltitle   = {Proceedings of Machine Learning Research},
  title          = {Learning to Augment with Feature Side-information},
  url            = {http://proceedings.mlr.press/v101/mollaysa19a.html},
  urldate        = {2019-12-26},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Mounsaveng-et-al-2019,
  author         = {Mounsaveng, Saypraseuth and Vazquez, David and Ayed, Ismail Ben and Pedersoli, Marco},
  date           = {2019-09-21},
  journaltitle   = {arXiv e-Print},
  title          = {Adversarial Learning of General Transformations for Data Augmentation},
  url            = {https://arxiv.org/abs/1909.09801},
  urldate        = {2019-10-18},
  abstract       = {Data augmentation (DA) is fundamental against overfitting in large convolutional neural networks, especially with a limited training dataset. In images, DA is usually based on heuristic transformations, like geometric or color transformations. Instead of using predefined transformations, our work learns data augmentation directly from the training data by learning to transform images with an encoder-decoder architecture combined with a spatial transformer network. The transformed images still belong to the same class but are new, more complex samples for the classifier. Our experiments show that our approach is better than previous generative data augmentation methods, and comparable to predefined transformation methods when training an image classifier.},
  day            = {21},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Nalepa-et-al-2019,
  author         = {Nalepa, Jakub and Myller, Michal and Kawulok, Michal},
  date           = {2019-03-13},
  journaltitle   = {arXiv e-Print},
  title          = {Hyperspectral Data Augmentation},
  url            = {https://arxiv.org/abs/1903.05580},
  urldate        = {2019-04-10},
  abstract       = {Data augmentation is a popular technique which helps improve generalization capabilities of deep neural networks. It plays a pivotal role in remote-sensing scenarios in which the amount of high-quality ground truth data is limited, and acquiring new examples is costly or impossible. This is a common problem in hyperspectral imaging, where manual annotation of image data is difficult, expensive, and prone to human bias. In this letter, we propose online data augmentation of hyperspectral data which is executed during the inference rather than before the training of deep networks. This is in contrast to all other state-of-the-art hyperspectral augmentation algorithms which increase the size (and representativeness) of training sets. Additionally, we introduce a new principal component analysis based augmentation. The experiments revealed that our data augmentation algorithms improve generalization of deep networks, work in real-time, and the online approach can be effectively combined with offline techniques to enhance the classification accuracy.},
  day            = {13},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:15},
}

@Article{Ohno-Ando-2014,
  author               = {Ohno, Saburo and Ando, Tomohiro},
  date                 = {2014-10},
  journaltitle         = {Econometric Reviews},
  title                = {Stock Return Predictability: A Factor-Augmented Predictive Regression System with Shrinkage Method},
  doi                  = {10.1080/07474938.2014.977086},
  pages                = {1--43},
  abstract             = {To predict stock market behaviors, we use a factor-augmented predictive regression with shrinkage to incorporate the information available across literally thousands of financial and economic variables. The system is constructed in terms of both expected returns and the tails of the return distribution. We develop the variable selection consistency and asymptotic normality of the estimator. To select the regularization parameter, we employ the prediction error, with the aim of predicting the behavior of the stock market. Through analysis of the Tokyo Stock Exchange, we find that a large number of variables provide useful information for predicting stock market behaviors.},
  citeulike-article-id = {14167260},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/07474938.2014.977086},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/07474938.2014.977086},
  day                  = {20},
  groups               = {Predictability_FinInfo},
  owner                = {cristi},
  posted-at            = {2016-10-18 09:05:36},
  publisher            = {Taylor and Francis},
  timestamp            = {2020-02-29 19:15},
}

@Article{Demetrescu-HaciogluHoke-2019,
  author         = {Demetrescu, Matei and Hacioglu Hoke, Sinem},
  date           = {2019-01},
  journaltitle   = {International Journal of Forecasting},
  title          = {Predictive regressions under asymmetric loss: Factor augmentation and model selection},
  doi            = {10.1016/j.ijforecast.2018.07.013},
  issn           = {0169-2070},
  number         = {1},
  pages          = {80--99},
  volume         = {35},
  abstract       = {The paper discusses the specifics of forecasting with factor-augmented predictive regressions under general loss functions. In line with the literature, we employ principal component analysis to extract factors from the set of predictors. We additionally extract information on the volatility of the series to be predicted, since volatility is forecast-relevant under non-quadratic loss functions. To ensure asymptotic unbiasedness of forecasts under the relevant loss, we estimate the predictive regression by minimizing the in-sample average loss. Finally, to select the most promising predictors for the series to be forecast, we employ an information criterion tailored to the relevant loss. Using a large monthly data set for the US economy, we assess the proposed adjustments in a pseudo out-of-sample forecasting exercise for various variables. As expected, the use of estimation under the relevant loss is effective. Using an additional volatility proxy as predictor and conducting model selection tailored to the relevant loss function enhances forecast performance significantly.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@MastersThesis{Djodikromo-2016,
  author               = {Djodikromo, J. A. L.},
  date                 = {2016},
  institution          = {Erasmus University},
  title                = {The Stochastic Factor-Augmented Nelson-Siegel Model},
  abstract             = {Accurately modeling and forecasting the term structure of interest rates is relevant for both academics and practitioners in the industry. The dynamic Nelson-Siegel model is suitable for this. Although the amount of research on the in-sample fit of the Nelson-Siegel model and its extensions is substantial, the number of studies examining the out-of-sample performance of these models is relatively little, particularly for the nonlinear class of Nelson-Siegel models. For this reason, the focus of this thesis is twofold. First, I examine the predictive performance of various extensions in the Nelson-Siegel framework relative to the standard dynamic Nelson-Siegel model. Second, I study the differences between the extended Kalman filter and the unscented Kalman filter in the context of nonlinear Nelson- Siegel models. I find that the results are maturity- and subsample-dependent. The greatest gain in predictive accuracy is found for the stochastic factor augmented Nelson-Siegel model, for which the improvement over the standard model attains values of a 28 percent decrease in the RMSPE when the yields are relatively volatile. Furthermore, the findings indicate that the use of the unscented Kalman filter rather than the extended Kalman filter for fitting nonlinear Nelson-Siegel models is beneficial for both ends of the yield curve and could have a positive impact on the accuracy in the predictive framework in some cases.},
  citeulike-article-id = {14344166},
  posted-at            = {2017-04-24 23:32:37},
  timestamp            = {2020-02-29 19:15},
}

@Article{Dunkler-et-al-2014,
  author         = {Dunkler, Daniela and Plischke, Max and LeffondrE, Karen and Heinze, Georg},
  date           = {2014-11-21},
  journaltitle   = {PLOS One},
  title          = {Augmented backward elimination: a pragmatic and purposeful way to develop statistical models.},
  doi            = {10.1371/journal.pone.0113677},
  number         = {11},
  pages          = {e113677},
  volume         = {9},
  abstract       = {Statistical models are simple mathematical rules derived from empirical data describing the association between an outcome and several explanatory variables. In a typical modeling situation statistical analysis often involves a large number of potential explanatory variables and frequently only partial subject-matter knowledge is available. Therefore, selecting the most suitable variables for a model in an objective and practical manner is usually a non-trivial task. We briefly revisit the purposeful variable selection procedure suggested by Hosmer and Lemeshow which combines significance and change-in-estimate criteria for variable selection and critically discuss the change-in-estimate criterion. We show that using a significance-based threshold for the change-in-estimate criterion reduces to a simple significance-based selection of variables, as if the change-in-estimate criterion is not considered at all. Various extensions to the purposeful variable selection procedure are suggested. We propose to use backward elimination augmented with a standardized change-in-estimate criterion on the quantity of interest usually reported and interpreted in a model for variable selection. Augmented backward elimination has been implemented in a SAS macro for linear, logistic and Cox proportional hazards regression. The algorithm and its implementation were evaluated by means of a simulation study. Augmented backward elimination tends to select larger models than backward elimination and approximates the unselected model up to negligible differences in point estimates of the regression coefficients. On average, regression coefficients obtained after applying augmented backward elimination were less biased relative to the coefficients of correctly specified models than after backward elimination. In summary, we propose augmented backward elimination as a reproducible variable selection algorithm that gives the analyst more flexibility in adopting model selection to a specific statistical modeling situation.},
  day            = {21},
  f1000-projects = {QuantInvest},
  pmcid          = {PMC4240713},
  pmid           = {25415265},
  timestamp      = {2020-02-29 19:15},
}

@Article{Exterkate-et-al-2013,
  author               = {Exterkate, Peter and Dijk, Dick V. and Heij, Christiaan and Groenen, Patrick J. F.},
  date                 = {2013-04},
  journaltitle         = {Journal of Forecasting},
  title                = {Forecasting the Yield Curve in a Data-Rich Environment Using the Factor-Augmented Nelson-Siegel Model},
  doi                  = {10.1002/for.1258},
  number               = {3},
  pages                = {193--214},
  volume               = {32},
  abstract             = {This paper compares various ways of extracting macroeconomic information from a data-rich environment for forecasting the yield curve using the Nelson-Siegel model. Five issues in extracting factors from a large panel of macro variables are addressed; namely, selection of a subset of the available information, incorporation of the forecast objective in constructing factors, specification of a multivariate forecast objective, data grouping before constructing factors, and selection of the number of factors in a data-driven way. Our empirical results show that each of these features helps to improve forecast accuracy, especially for the shortest and longest maturities.

Factor-augmented methods perform well in relatively volatile periods, including the crisis period in 2008-9, when simpler models do not suffice. The macroeconomic information is exploited best by partial least squares methods, with principal component methods ranking second best. Reductions of mean squared prediction errors of 20-30 percent are attained, compared to the Nelson-Siegel model without macro factors.},
  citeulike-article-id = {13935017},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.1258},
  day                  = {1},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:15:43},
  timestamp            = {2020-02-29 19:15},
}

@Article{Fawaz-et-al-2018a,
  author         = {Fawaz, Hassan Ismail and Forestier, Germain and Weber, Jonathan and Idoumghar, Lhassane and Muller, Pierre-Alain},
  date           = {2018-08-07},
  journaltitle   = {arXiv e-Print},
  title          = {Data augmentation using synthetic data for time series classification with deep residual networks},
  url            = {https://arxiv.org/abs/1808.02455},
  abstract       = {Data augmentation in deep neural networks is the process of generating artificial data in order to reduce the variance of the classifier with the goal to reduce the number of errors. This idea has been shown to improve deep neural network's generalization capabilities in many computer vision tasks such as image recognition and object localization. Apart from these applications, deep Convolutional Neural Networks (CNNs) have also recently gained popularity in the Time Series Classification (TSC) community. However, unlike in image recognition problems, data augmentation techniques have not yet been investigated thoroughly for the TSC task. This is surprising as the accuracy of deep learning models for TSC could potentially be improved, especially for small datasets that exhibit overfitting, when a data augmentation method is adopted. In this paper, we fill this gap by investigating the application of a recently proposed data augmentation technique based on the Dynamic Time Warping distance, for a deep learning model for TSC. To evaluate the potential of augmenting the training set, we performed extensive experiments using the UCR TSC benchmark. Our preliminary experiments reveal that data augmentation can drastically increase deep CNN's accuracy on some datasets and significantly improve the deep model's accuracy when the method is used in an ensemble approach.},
  day            = {7},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:15},
}

@Article{Ferreira-Serpa-2016,
  author               = {Ferreira, Wallace and Serpa, Alberto},
  date                 = {2016},
  journaltitle         = {Structural and Multidisciplinary Optimization},
  title                = {Ensemble of metamodels: the augmented least squares approach},
  doi                  = {10.1007/s00158-015-1366-1},
  number               = {5},
  pages                = {1019--1046},
  volume               = {53},
  abstract             = {In this work we present an approach to create ensemble of metamodels (or weighted averaged surrogates) based on least squares (LS) approximation. The LS approach is appealing since it is possible to estimate the ensemble weights without using any explicit error metrics as in most of the existent ensemble methods. As an additional feature, the LS based ensemble of metamodels has a prediction variance function that enables the extension to the efficient global optimization. The proposed LS approach is a variation of the standard LS regression by augmenting the matrices in such a way that minimizes the effects of multicollinearity inherent to calculation of the ensemble weights. We tested and compared the augmented LS approach with different LS variants and also with existent ensemble methods, by means of analytical and real-world functions from two to forty-four variables. The augmented least squares approach performed with good accuracy and stability for prediction purposes, in the same level of other ensemble methods and has computational cost comparable to the faster ones.},
  citeulike-article-id = {14332013},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00158-015-1366-1},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s00158-015-1366-1},
  groups               = {Data_MultiCollinear},
  posted-at            = {2017-04-05 00:37:50},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-29 19:15},
}

@InProceedings{Forestier-et-al-2017,
  author         = {Forestier, Germain and Petitjean, Francois and Dau, Hoang Anh and Webb, Geoffrey I. and Keogh, Eamonn},
  booktitle      = {2017 IEEE International Conference on Data Mining (ICDM)},
  date           = {2017-11-18},
  title          = {Generating synthetic time series to augment sparse datasets},
  doi            = {10.1109/{ICDM}.2017.106},
  isbn           = {978-1-5386-3835-4},
  pages          = {865--870},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/8215569/},
  urldate        = {2019-10-09},
  abstract       = {In machine learning, data augmentation is the process of creating synthetic examples in order to augment a dataset used to learn a model. One motivation for data augmentation is to reduce the variance of a classifier, thereby reducing error. In this paper, we propose new data augmentation techniques specifically designed for time series classification, where the space in which they are embedded is induced by Dynamic Time Warping (DTW). The main idea of our approach is to average a set of time series and use the average time series as a new synthetic example. The proposed methods rely on an extension of DTW Barycentric Averaging (DBA), the averaging technique that is specifically developed for DTW. In this paper, we extend DBA to be able to calculate a weighted average of time series under DTW. In this case, instead of each time series contributing equally to the final average, some can contribute more than others. This extension allows us to generate an infinite number of new examples from any set of given time series. To this end, we propose three methods that choose the weights associated to the time series of the dataset. We carry out experiments on the 85 datasets of the UCR archive and demonstrate that our method is particularly useful when the number of available examples is limited (e.g. 2 to 6 examples per class) using a 1-NN DTW classifier. Furthermore, we show that augmenting full datasets is beneficial in most cases, as we observed an increase of accuracy on 56 datasets, no effect on 7 and a slight decrease on only 22.},
  day            = {18},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Gennatas-et-al-2019,
  author         = {Gennatas, E. D. and Friedman, J. H. and Ungar, L. H. and Pirracchio, R. and Eaton, E. and Reichman, L. and Interian, Y. and Simone, C. B. and Auerbach, A. and Delgado, E. and Van der Laan, M. J. and Solberg, T. D. and Valdes, G.},
  date           = {2019-03-22},
  journaltitle   = {arXiv e-Print},
  title          = {Expert-Augmented Machine Learning},
  url            = {https://arxiv.org/abs/1903.09731},
  abstract       = {Machine Learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption by the level of trust that models afford users. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of man and machine. Here we present Expert-Augmented Machine Learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We use a large dataset of intensive care patient data to predict mortality and show that we can extract expert knowledge using an online platform, help reveal hidden confounders, improve generalizability on a different population and learn using less data. EAML presents a novel framework for high performance and dependable machine learning in critical applications.},
  day            = {22},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Giovannetti-2013,
  author               = {Giovannetti, Bruno C.},
  date                 = {2013-01},
  journaltitle         = {Journal of Forecasting},
  title                = {Nonlinear Forecasting Using Factor-Augmented Models},
  doi                  = {10.1002/for.1248},
  number               = {1},
  pages                = {32--40},
  volume               = {32},
  abstract             = {Using factors in forecasting exercises reduces the dimensionality of the covariates set and, therefore, allows the forecaster to explore possible nonlinearities in the model. For an American macroeconomic dataset, I present evidence that the employment of nonlinear estimation methods can improve the out-of-sample forecasting accuracy for some macroeconomic variables, such as industrial production, employment, and Fed fund rate.},
  citeulike-article-id = {13935020},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.1248},
  day                  = {1},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:18:56},
  timestamp            = {2020-02-29 19:15},
}

@Article{Hataya-et-al-2019,
  author         = {Hataya, Ryuichiro and Zdenek, Jan and Yoshizoe, Kazuki and Nakayama, Hideki},
  date           = {2019-11-16},
  journaltitle   = {arXiv e-Print},
  title          = {Faster AutoAugment: Learning Augmentation Strategies using Backpropagation},
  url            = {https://arxiv.org/abs/1911.06987v1},
  urldate        = {2019-12-15},
  abstract       = {Data augmentation methods are indispensable heuristics to boost the performance of deep neural networks, especially in image recognition tasks. Recently, several studies have shown that augmentation strategies found by search algorithms outperform hand-made strategies. Such methods employ black-box search algorithms over image transformations with continuous or discrete parameters and require a long time to obtain better strategies. In this paper, we propose a differentiable policy search pipeline for data augmentation, which is much faster than previous methods. We introduce approximate gradients for several transformation operations with discrete parameters as well as the differentiable mechanism for selecting operations. As the objective of training, we minimize the distance between the distributions of augmented data and the original data, which can be differentiated. We show that our method, Faster AutoAugment, achieves significantly faster searching than prior work without a performance drop.},
  day            = {16},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{He-et-al-2019d,
  author         = {He, Zhuoxun and Xie, Lingxi and Chen, Xin and Zhang, Ya and Wang, Yanfeng and Tian, Qi},
  date           = {2019-09-19},
  journaltitle   = {arXiv e-Print},
  title          = {Data Augmentation Revisited: Rethinking the Distribution Gap between Clean and Augmented Data},
  url            = {https://arxiv.org/abs/1909.09148v1},
  urldate        = {2019-10-02},
  abstract       = {Data augmentation has been widely applied as an effective methodology to prevent over-fitting in particular when training very deep neural networks. The essential benefit comes from introducing additional priors in visual invariance, and thus generate images in different appearances but containing the same semantics. Recently, researchers proposed a few powerful data augmentation techniques which indeed improved accuracy, yet we notice that these methods have also caused a considerable gap between clean and augmented data. This paper revisits this problem from an analytical perspective, for which we estimate the upper-bound of testing loss using two terms, named empirical risk and generalization error, respectively. Data augmentation significantly reduces the generalization error, but meanwhile leads to a larger empirical risk, which can be alleviated by a simple algorithm, i.e. using less-augmented data to refine the model trained on fully-augmented data. We validate our approach on a few popular image classification datasets including CIFAR and ImageNet, and demonstrate consistent accuracy gain. We also conjecture that this simple strategy implies a generalized approach to circumvent local minima, which is of value to future research on model optimization.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{HernandezGarcia-Konig-2018,
  author         = {Hernandez-Garcia, Alex and Konig, Peter},
  date           = {2018},
  journaltitle   = {arXiv e-Print},
  title          = {Data augmentation instead of explicit regularization},
  url            = {https://arxiv.org/abs/1806.03852v3},
  abstract       = {Modern deep artificial neural networks have achieved impressive results through models with a very large number of parameters---compared to the number of training examples---that control overfitting with the help of regularization. Regularization can be implicit, as is the case of stochastic gradient descent and parameter sharing in convolutional layers, or explicit. Explicit regularization techniques, most common forms are weight decay and dropout, reduce the effective capacity of the model and typically require the use of deeper and wider architectures to compensate for the reduced capacity. Although these techniques have proven successful in terms of improved generalization, they seem to waste some capacity. In contrast, data augmentation techniques rely on increasing the number of training examples to improve generalization without reducing the effective capacity. Unlike weight decay and dropout, data augmentation is independent of the specific network architecture, since it is applied on the training data. In this paper we systematically compare data augmentation and explicit regularization on some popular architectures and data sets. Our results demonstrate that data augmentation alone can achieve the same performance or higher as regularized models and exhibits much higher adaptability to changes in the architecture and the amount of training data.},
  day            = {11},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:15},
}

@Article{Hoffmann-et-al-2018,
  author         = {Hoffmann, Jordan and Bar-Sinai, Yohai and Lee, Lisa and Andrejevic, Jovana and Mishra, Shruti and Rubinstein, Shmuel M. and Rycroft, Chris H.},
  date           = {2018-07-04},
  journaltitle   = {arXiv e-Print},
  title          = {Machine Learning in a data-limited regime: Augmenting experiments with synthetic data uncovers order in crumpled sheets},
  url            = {https://arxiv.org/abs/1807.01437},
  abstract       = {Machine learning has gained widespread attention as a powerful tool to identify structure in complex, high-dimensional data. However, these techniques are ostensibly inapplicable for experimental systems with limited data acquisition rates, due to the restricted size of the dataset. Here we introduce a strategy to resolve this impasse by augmenting the experimental dataset with synthetically generated data of a much simpler sister system. Specifically, we study spontaneously emerging local order in crease networks of crumpled thin sheets, a paradigmatic example of spatial complexity, and show that machine learning techniques can be effective even in a data-limited regime. This is achieved by augmenting the scarce experimental dataset with inexhaustible amounts of simulated data of flat-folded sheets, which are simple to simulate. This significantly improves the predictive power in a test problem of pattern completion and demonstrates the usefulness of machine learning in bench-top experiments where data is good but scarce.},
  day            = {4},
  f1000-projects = {QuantInvest},
  groups         = {Regime_Identif, Data_Augment},
  timestamp      = {2020-02-29 19:15},
}

@Article{Hsu-2017,
  author               = {Hsu, Daniel},
  date                 = {2017-07-06},
  journaltitle         = {arXiv e-Print},
  title                = {Time Series Forecasting Based on Augmented Long Short-Term Memory},
  eprint               = {1707.00666},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1707.00666},
  abstract             = {In this paper, we use recurrent autoencoder model to predict the time series in single and multiple steps ahead. Previous prediction methods, such as recurrent neural network (RNN) and deep belief network (DBN) models, cannot learn long term dependencies. And conventional long short-term memory (LSTM) model doesn't remember recent inputs. Combining LSTM and autoencoder (AE), the proposed model can capture long-term dependencies across data points and uses features extracted from recent observations for augmenting LSTM at the same time. Based on comprehensive experiments, we show that the proposed methods significantly improves the state-of-art performance on chaotic time series benchmark and also has better performance on real-world data. Both single-output and multiple-output predictions are investigated.},
  citeulike-article-id = {14503534},
  citeulike-linkout-0  = {http://arxiv.org/abs/1707.00666},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1707.00666},
  day                  = {6},
  groups               = {FrcstQWIM_TimeSrs},
  posted-at            = {2017-12-15 18:04:36},
  timestamp            = {2020-02-29 19:15},
}

@Article{Huber-Fischer-2017,
  author         = {Huber, Florian and Fischer, Manfred M.},
  date           = {2017-12-28},
  journaltitle   = {Oxford Bulletin of Economics and Statistics},
  title          = {A Markov Switching Factor-Augmented VAR Model for Analyzing US Business Cycles and Monetary Policy},
  doi            = {10.1111/obes.12227},
  issn           = {0305-9049},
  number         = {3},
  pages          = {575--604},
  volume         = {80},
  abstract       = {This paper develops a multivariate regime switching monetary policy model for the US economy. To exploit a large dataset we use a factor-augmented VAR with discrete regime shifts, capturing distinct business cycle phases. The transition probabilities are modelled as time-varying, depending on a broad set of indicators that influence business cycle movements. The model is used to investigate the relationship between business cycle phases and monetary policy. Our results indicate that the effects of monetary policy are stronger in recessions, whereas the responses are more muted in expansionary phases. Moreover, lagged prices serve as good predictors for business cycle transitions. (authors' abstract)},
  day            = {28},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Antoniou-Storkey-2019,
  author         = {Antoniou, Antreas and Storkey, Amos},
  date           = {2019-02-26},
  journaltitle   = {arXiv e-Print},
  title          = {Assume, Augment and Learn: Unsupervised Few-Shot Meta-Learning via Random Labels and Data Augmentation},
  url            = {https://arxiv.org/abs/1902.09884},
  urldate        = {2019-10-18},
  abstract       = {The field of few-shot learning has been laboriously explored in the supervised setting, where per-class labels are available. On the other hand, the unsupervised few-shot learning setting, where no labels of any kind are required, has seen little investigation. We propose a method, named Assume, Augment and Learn or AAL, for generating few-shot tasks using unlabeled data. We randomly label a random subset of images from an unlabeled dataset to generate a support set. Then by applying data augmentation on the support set's images, and reusing the support set's labels, we obtain a target set. The resulting few-shot tasks can be used to train any standard meta-learning framework. Once trained, such a model, can be directly applied on small real-labeled datasets without any changes or fine-tuning required. In our experiments, the learned models achieve good generalization performance in a variety of established few-shot learning tasks on Omniglot and Mini-Imagenet.},
  day            = {26},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Banerjee-et-al-2014,
  author               = {Banerjee, Anindya and Marcellino, Massimiliano and Masten, Igor},
  date                 = {2014-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Forecasting with factor-augmented error correction models},
  doi                  = {10.1016/j.ijforecast.2013.01.009},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {589--612},
  volume               = {30},
  abstract             = {As a generalization of the factor-augmented VAR (FAVAR) and of the Error Correction Model (ECM), Banerjee and Marcellino (2009) introduced the Factor-augmented Error Correction Model (FECM). The FECM combines error-correction, cointegration and dynamic factor models, and has several conceptual advantages over the standard ECM and FAVAR models. In particular, it uses a larger dataset than the ECM and incorporates the long-run information which the FAVAR is missing because of its specification in differences.

In this paper, we examine the forecasting performance of the FECM by means of an analytical example, Monte Carlo simulations and several empirical applications. We show that FECM generally offers a higher forecasting precision relative to the FAVAR, and marks a useful step forward for forecasting with large datasets.},
  citeulike-article-id = {13932967},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2013.01.009},
  owner                = {cristi},
  posted-at            = {2016-02-15 08:19:43},
  timestamp            = {2020-02-29 19:15},
}

@InCollection{Banerjee-et-al-2016,
  author               = {Banerjee, Anindya and Marcellino, Massimiliano and Masten, Igor},
  booktitle            = {Dynamic Factor Models},
  date                 = {2016-01},
  title                = {An Overview of the Factor-augmented Error-Correction Model},
  doi                  = {10.1108/s0731-905320150000035001},
  editor               = {Hillebrand, Eric and Koopman, Siem J.},
  isbn                 = {978-1-78560-353-2},
  pages                = {3--41},
  publisher            = {Emerald Group Publishing Limited},
  volume               = {35},
  abstract             = {The Factor-augmented Error-Correction Model (FECM) generalizes the factor-augmented VAR (FAVAR) and the Error-Correction Model (ECM), combining error-correction, cointegration and dynamic factor models. It uses a larger set of variables compared to the ECM and incorporates the long-run information lacking from the FAVAR because of the latter's specification in differences. In this paper, we review the specification and estimation of the FECM, and illustrate its use for forecasting and structural analysis by means of empirical applications based on Euro Area and US data.},
  citeulike-article-id = {14072070},
  citeulike-linkout-0  = {http://dx.doi.org/10.1108/s0731-905320150000035001},
  day                  = {06},
  owner                = {cristi},
  posted-at            = {2016-06-19 16:28:43},
  timestamp            = {2020-02-29 19:15},
}

@Article{Bundoo-2008,
  author       = {Bundoo, Sunil K.},
  date         = {2008-12},
  journaltitle = {Applied Economics Letters},
  title        = {An augmented Fama and French three-factor model: new evidence from an emerging stock market},
  doi          = {10.1080/13504850601018049},
  issn         = {1350-4851},
  number       = {15},
  pages        = {1213--1218},
  volume       = {15},
  abstract     = {There were forty equity stocks listed on the stock exchange of Mauritius as at end of December 2004. Fama and French (1993) posit that a possible explanation for the size and book-to-market equity effects could be due to other risk factors not captured in a standard capital asset pricing model. This paper therefore investigates whether on the stock exchange of Mauritius, when taking into account the time variation in risk (as measured by time-varying betas), the two additional factors are still priced. The paper presents an augmented model, which takes into account the time variation in beta, in addition to the size and book-to-market equity factors. It is found that the coefficients for the size effect and the book-to-market equity effect are all significant at the one percent level and with the expected signs. These effects do not disappear. This shows that the Fama and French three factor model is robust to taking into account time-varying betas.},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:15},
}

@Article{Cheng-Hansen-2015,
  author               = {Cheng, Xu and Hansen, Bruce E.},
  date                 = {2015-06},
  journaltitle         = {Journal of Econometrics},
  title                = {Forecasting with factor-augmented regression: A frequentist model averaging approach},
  doi                  = {10.1016/j.jeconom.2015.02.010},
  issn                 = {0304-4076},
  number               = {2},
  pages                = {280--293},
  volume               = {186},
  abstract             = {This paper considers forecast combination with factor-augmented regression. In this framework, a large number of forecasting models are available, varying by the choice of factors and the number of lags. We investigate forecast combination across models using weights that minimize the Mallows and the leave-hh-out cross validation criteria. The unobserved factor regressors are estimated by principle components of a large panel with NN predictors over TT periods. With these generated regressors, we show that the Mallows and leave-hh-out cross validation criteria are asymptotically unbiased estimators of the one-step-ahead and multi-step-ahead mean squared forecast errors, respectively, provided that . (However, the paper does not establish any optimality properties for the methods.) In contrast to well-known results in the literature, this result suggests that the generated-regressor issue can be ignored for forecast combination, without restrictions on the relation between NN and TT. Simulations show that the Mallows model averaging and leave-hh-out cross-validation averaging methods yield lower mean squared forecast errors than alternative model selection and averaging methods such as AIC, BIC, cross validation, and Bayesian model averaging. We apply the proposed methods to the US macroeconomic data set in Stock and Watson (2012) and find that they compare favorably to many popular shrinkage-type forecasting methods.},
  citeulike-article-id = {14072060},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jeconom.2015.02.010},
  groups               = {Proba_Freq},
  owner                = {cristi},
  posted-at            = {2016-06-19 16:02:48},
  timestamp            = {2020-02-29 19:15},
}

@Article{Cheung-2009,
  author       = {Cheung, Wing},
  date         = {2009},
  journaltitle = {SSRN e-Print},
  title        = {Generalised Factor View Blending: Augmented Black-Litterman in Non-Normal Financial Markets with Non-Linear Instruments},
  abstract     = {The augmented Black-Litterman (ABL) model is an elegant view processor, as well as a natural, robust and unified allocation framework suitable for multiple investment styles (Cheung, 2009B and C). In this paper, we extend the model into a generalised factor view blending (GFVB) framework, suitable for tail risk-aware allocation in non-normal markets with non-linear instruments, factor structures and views. We highlight the following features: (1) Freedom in considering any market factor structure with any security and factor distributions, (2) Generic prior distribution without normality restrictions, (3) Freedom in forming non-linear, non-normal views, (4) View blending strictly based on the Bayes' Rule, (5) A structural approach to constructing portfolio of exotic products.},
  groups       = {Black_Litterman},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=1395283},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:15},
}

@Article{Cheung-2010a,
  author       = {Cheung, Wing},
  date         = {2010},
  journaltitle = {SSRN e-Print},
  title        = {The Intrinsic Logic of the Augmented Black-Litterman Model},
  abstract     = {Portfolio management is an art as well as science. We argue that portfolio managers eventually face two fundamental scientific challenges: (a) how to allocate; and (b) how to mimic. These require scientific answers. Other challenges in portfolio management are arts where different tastes, preferences, judgments and beliefs should be allowed. A general portfolio management framework should provide principles for solving the scientific problems, but remain neutral and facilitate interactions with the art part. The Augmented Black-Litterman (ABL) model rightly meets these requirements. This article reveals its intrinsic logic. It features:

- An exploration of the motivation and general requirements of portfolio construction; - A dissection of the ABL allocation model; - Three ABL endogenous techniques and their efficiency evidences; - The intrinsic logic of the ABL model; and - Justifications of the model as a unified Bayesian allocation framework.

Practitioners can consider this framework a unified allocation theory, which can be used to understand, evaluate and improve existing portfolio construction practices.},
  groups       = {Black_Litterman},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=1457010},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:15},
}

@Article{Cheung-2013,
  author       = {Cheung, Wing},
  date         = {2013},
  journaltitle = {Quantitative Finance},
  title        = {The Augmented Black-Litterman Model: A Ranking-Free Approach to Factor-Based Portfolio Construction and Beyond},
  number       = {2},
  volume       = {13},
  abstract     = {The Fama and French factor-ranking approach (1992, 1993, etc.) has been extensively applied in quantitative fund management. However, this approach suffers from hidden factor view, information inefficiency, etc. issues. Based on the Black-Litterman model (1992; as explained in Cheung 2010b), we develop a technique that endogenizes the ranking process and elegantly resolves these issues. This model explicitly seeks forward-looking factor views and smoothly blends them to deliver robust allocation to securities. Our numerical experiments show this is an intuitive and practical framework for factor-based portfolio construction, and beyond.

This article features: (1) a new and unified framework for strategy combination, factor mimicking and security-specific bets; (2) an elegant and ranking-free approach to factor style construction; (3) worked examples based on the FTSE EUROTOP 100 universe; (4) insight into the classic issue of confidence parameter setting; and (5) implementation guidance in an appendix.},
  groups       = {Black_Litterman},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=1347648},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:15},
}

@Article{Coulombe-2018,
  author         = {Coulombe, Claude},
  date           = {2018-12-05},
  journaltitle   = {arXiv e-Print},
  title          = {Text Data Augmentation Made Simple By Leveraging NLP Cloud APIs},
  url            = {https:://arxiv.org/abs/1812.04718},
  urldate        = {2019-04-10},
  abstract       = {In practice, it is common to find oneself with far too little text data to train a deep neural network. This "Big Data Wall" represents a challenge for minority language communities on the Internet, organizations, laboratories and companies that compete the GAFAM (Google, Amazon, Facebook, Apple, Microsoft). While most of the research effort in text data augmentation aims on the long-term goal of finding end-to-end learning solutions, which is equivalent to "using neural networks to feed neural networks", this engineering work focuses on the use of practical, robust, scalable and easy-to-implement data augmentation pre-processing techniques similar to those that are successful in computer vision. Several text augmentation techniques have been experimented. Some existing ones have been tested for comparison purposes such as noise injection or the use of regular expressions. Others are modified or improved techniques like lexical replacement. Finally more innovative ones, such as the generation of paraphrases using back-translation or by the transformation of syntactic trees, are based on robust, scalable, and easy-to-use NLP Cloud APIs. All the text augmentation techniques studied, with an amplification factor of only 5, increased the accuracy of the results in a range of 4.3\% to 21.6\%, with significant statistical fluctuations, on a standardized task of text polarity prediction. Some standard deep neural network architectures were tested: the multilayer perceptron (MLP), the long short-term memory recurrent network (LSTM) and the bidirectional LSTM (biLSTM). Classical XGBoost algorithm has been tested with up to 2.5\% improvements.},
  day            = {5},
  f1000-projects = {QuantInvest},
  groups         = {Data_Augment},
  timestamp      = {2020-02-29 19:15},
}

@Article{Cubadda-Guardabascio-2019,
  author         = {Cubadda, Gianluca and Guardabascio, Barbara},
  date           = {2019-01},
  journaltitle   = {International Journal of Forecasting},
  title          = {Representation, estimation and forecasting of the multivariate index-augmented autoregressive model},
  doi            = {10.1016/j.ijforecast.2018.08.002},
  issn           = {0169-2070},
  number         = {1},
  pages          = {67--79},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0169207018301316},
  urldate        = {2019-09-01},
  volume         = {35},
  abstract       = {We examine the conditions under which each individual series that is generated by a vector autoregressive model can be represented as an autoregressive model that is augmented with the lags of few linear combinations of all the variables in the system. We call this modelling Multivariate Index-Augmented Autoregression (MIAAR). We show that the parameters of the MIAAR can be estimated by a switching algorithm that increases the Gaussian likelihood at each iteration. Since maximum likelihood estimation may perform poorly when the number of parameters gets larger, we propose a regularized version of our algorithm to handle a medium-large number of time series. We illustrate the usefulness of the MIAAR modelling both by empirical applications and simulations.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Cubuk-et-al-2019,
  author         = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
  date           = {2019-09-30},
  journaltitle   = {arXiv e-Print},
  title          = {RandAugment: Practical data augmentation with no separate search},
  url            = {https://arxiv.org/abs/1909.13719},
  urldate        = {2019-10-05},
  abstract       = {Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, learned augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. One obstacle to a large-scale adoption of these methods is a separate search phase which significantly increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these learned augmentation approaches are unable to adjust the regularization strength based on model or dataset size. Learned augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment may be trained on the model and dataset of interest with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous learned augmentation approaches on CIFAR-10, CIFAR-100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size.},
  day            = {30},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:15},
}

@Article{Banbura-et-al-2015,
  author               = {Banbura, Marta and Giannone, Domenico and Lenza, Michele},
  date                 = {2015-07},
  journaltitle         = {International Journal of Forecasting},
  title                = {Conditional forecasts and scenario analysis with vector autoregressions for large cross-sections},
  doi                  = {10.1016/j.ijforecast.2014.08.013},
  issn                 = {0169-2070},
  number               = {3},
  pages                = {739--756},
  volume               = {31},
  abstract             = {This paper describes an algorithm for computing the distribution of conditional forecasts, i.e.,projections of a set of variables of interest on future paths of some other variables, in dynamic systems. The algorithm is based on Kalman filtering methods and is computationally viable for large models that can be cast in a linear state space representation. We build large vector autoregressions (VARs) and a large dynamic factor model (DFM) for a quarterly data set of 26 euro area macroeconomic and financial indicators. The two approaches deliver similar forecasts and scenario assessments. In addition, conditional forecasts shed light on the stability of the dynamic relationships in the euro area during the recent episodes of financial turmoil, and indicate that only a small number of sources drive the bulk of the fluctuations in the euro area economy.},
  citeulike-article-id = {13995883},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ijforecast.2014.08.013},
  owner                = {cristi},
  posted-at            = {2016-04-04 01:31:06},
  timestamp            = {2020-02-29 19:20},
}

@Article{BarraMontevechi-et-al-2017,
  author         = {Barra Montevechi, J. A. and da Silva Costa, R F and de Pinho, A F and de Carvalho Miranda, R},
  date           = {2017-05},
  journaltitle   = {Journal of Simulation},
  title          = {A simulation-based approach to perform economic evaluation scenarios},
  doi            = {10.1057/jos.2016.2},
  issn           = {1747-7778},
  number         = {2},
  pages          = {185--192},
  volume         = {11},
  abstract       = {The interest in the joint use of discrete event simulation (DES) with design of experiments (DOE), activity-based costing (ABC) and net present value (NPV) in order to aid in decision making in manufacturing systems has been on the rise in the past decade. Traditionally, the literature has presented research combining simulation with one or two of the mentioned techniques. In this paper, all four mentioned techniques are integrated into one methodology in order to economically evaluate simulation model scenarios for manufacturing systems. Scenarios are simulated and analysed through the application of ABC and DOE. Then, the significant scenarios are analysed through NPV. Through application of the methodology in a production cell, it is shown that it is viable to utilize these four techniques together. This research contributes to current literature through the development of a methodology that combines DES, ABC, DOE and NPV, in a decision-making structure.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:20},
}

@Article{Bernard-et-al-2014,
  author               = {Bernard, Carole and Chen, Jit S. and Vanduffel, Steven},
  date                 = {2014-04},
  journaltitle         = {Quantitative Finance},
  title                = {Optimal portfolios under worst-case scenarios},
  doi                  = {10.1080/14697688.2013.836282},
  number               = {4},
  pages                = {657--671},
  volume               = {14},
  abstract             = {In standard portfolio theories such as Mean?Variance optimization, expected utility theory, rank dependent utility heory, Yaari?s dual theory and cumulative prospect theory, the worst outcomes for optimal strategies occur when the market declines (e.g. during crises), which is at odds with the needs of many investors. Hence, we depart from the traditional settings and study optimal strategies for investors who impose additional constraints on their final wealth in the states corresponding to a stressed financial market. We provide a framework that maintains the stylized features of the SP/A theory while dealing with the goal of security in a more flexible way. Preferences become state-dependent, and we assess the impact of these preferences on trading decisions. We construct optimal strategies explicitly and show how they outperform traditional diversified strategies under worst-case scenarios.},
  citeulike-article-id = {13988610},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2013.836282},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2013.836282},
  day                  = {1},
  groups               = {ProspectThry_Invest, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-26 18:58:16},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 19:20},
}

@Article{Bogin-Doerner-2014,
  author         = {Bogin, Alexander and Doerner, William},
  date           = {2014-11-21},
  journaltitle   = {The Journal of Risk Finance},
  title          = {Generating historically-based stress scenarios using parsimonious factorization},
  doi            = {10.1108/{JRF}-03-2014-0036},
  issn           = {1526-5943},
  number         = {5},
  pages          = {591--611},
  volume         = {15},
  day            = {21},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:20},
}

@Article{Bouhalleb-Smida-2018,
  author         = {Bouhalleb, Arafet and Smida, Ali},
  date           = {2018-07},
  journaltitle   = {Journal of Forecasting},
  title          = {Scenario planning: An investigation of the construct and its measurement},
  doi            = {10.1002/for.2515},
  issn           = {0277-6693},
  number         = {4},
  pages          = {489--505},
  urldate        = {2019-09-01},
  volume         = {37},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:20},
}

@Article{Braouezec-Wagalath-2018,
  author         = {Braouezec, Yann and Wagalath, Lakshithe},
  date           = {2018-03-01},
  journaltitle   = {Review of Finance},
  title          = {Risk-Based Capital Requirements and Optimal Liquidation in a Stress Scenario},
  doi            = {10.1093/rof/rfw067},
  issn           = {1572-3097},
  number         = {2},
  pages          = {747--782},
  url            = {https://academic.oup.com/rof/article/22/2/747/2996608},
  urldate        = {2019-09-01},
  volume         = {22},
  abstract       = {We develop a simple yet realistic framework to analyze the impact of an exogenous shock on a bank balance-sheet and its optimal response when it is constrained to maintain its risk-based capital ratio above a regulatory threshold. We show that in a stress scenario, capital requirements may force the bank to shrink the size of its assets and we exhibit the bank optimal strategy as a function of regulatory risk-weights, asset market liquidity, and shock size. When financial markets are perfectly competitive, we show that the bank is always able to restore its capital ratio above the required one. However, for banks constrained to sell their loans at a discount and/or with a positive price impact when selling their marketable assets (large banks) we exhibit situations in which the deleveraging process generates a death spiral. We then show how to calibrate our model using annual reports of banks and study in detail the case of the French bank BNP Paribas. Finally, we suggest how our simple framework can be used to design a systemic capital surcharge.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:20},
}

@Article{Calfa-et-al-2014,
  author               = {Calfa, B. A. and Agarwal, A. and Grossmann, I. E. and Wassick, J. M.},
  date                 = {2014-09},
  journaltitle         = {Computers and Chemical Engineering},
  title                = {Data-driven multi-stage scenario tree generation via statistical property and distribution matching},
  doi                  = {10.1016/j.compchemeng.2014.04.012},
  issn                 = {0098-1354},
  pages                = {7--23},
  volume               = {68},
  abstract             = {Optimization matching for generating scenario trees that reduces under-specification Data-driven method that does not require strict assumptions on probability distribution. Two approaches for multi-stage scenario trees based on time series and forecasting. Application in production planning with uncertain yield and correlated product demands. This paper brings systematic methods for scenario tree generation to the attention of the Process Systems Engineering community. We focus on a general, data-driven optimization-based method for generating scenario trees that does not require strict assumptions on the probability distributions of the uncertain parameters. Using as a basis the Moment Matching Problem (MMP), originally proposed by Hyland and Wallace (2001), we propose matching marginal (Empirical) Cumulative Distribution Function information of the uncertain parameters in order to cope with potentially under-specified MMP formulations. The new method gives rise to a Distribution Matching Problem (DMP) that is aided by predictive analytics. We present two approaches for generating multi-stage scenario trees by considering time series modeling and forecasting. The aforementioned techniques are illustrated with a production planning problem with uncertainty in production yield and correlated product demands.},
  citeulike-article-id = {14072116},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.compchemeng.2014.04.012},
  owner                = {cristi},
  posted-at            = {2016-06-19 19:19:52},
  timestamp            = {2020-02-29 19:20},
}

@Article{Cambou-Filipovic-2017,
  author               = {Cambou, Mathieu and Filipovic, Damir},
  date                 = {2017-04},
  journaltitle         = {Mathematical Finance},
  title                = {Model uncertainty and scenario aggregation},
  doi                  = {10.1111/mafi.12097},
  issn                 = {0960-1627},
  number               = {2},
  pages                = {534--567},
  volume               = {27},
  abstract             = {This paper provides a coherent method for scenario aggregation addressing model uncertainty. It is based on divergence minimization from a reference probability measure subject to scenario constraints. An example from regulatory practice motivates the definition of five fundamental criteria that serve as a basis for our method. Standard risk measures, such as value-at-risk and expected shortfall, are shown to be robust with respect to minimum divergence scenario aggregation. Various examples illustrate the tractability of our method.},
  citeulike-article-id = {14334802},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/mafi.12097},
  posted-at            = {2017-04-09 23:03:16},
  timestamp            = {2020-02-29 19:20},
}

@Article{Chen-et-al-2015f,
  author               = {Chen, Michael and Mehrotra, Sanjay and Papp, David},
  date                 = {2015},
  journaltitle         = {Computational Optimization and Applications},
  title                = {Scenario generation for stochastic optimization problems via the sparse grid method},
  doi                  = {10.1007/s10589-015-9751-7},
  number               = {3},
  pages                = {669--692},
  volume               = {62},
  abstract             = {We study the use of sparse grids in the scenario generation (or discretization) problem in stochastic programming problems where the uncertainty is modeled using a continuous multivariate distribution. We show that, under a regularity assumption on the random function involved, the sequence of optimal objective function values of the sparse grid approximations converges to the true optimal objective function values as the number of scenarios increases. The rate of convergence is also established. We treat separately the special case when the underlying distribution is an affine transform of a product of univariate distributions, and show how the sparse grid method can be adapted to the distribution by the use of quadrature formulas tailored to the distribution. We numerically compare the performance of the sparse grid method using different quadrature rules with classic quasi-Monte Carlo (QMC) methods, optimal rank-one lattice rules, and Monte Carlo (MC) scenario generation, using a series of utility maximization problems with up to 160 random variables. The results show that the sparse grid method is very efficient, especially if the integrand is sufficiently smooth. In such problems the sparse grid scenario generation method is found to need several orders of magnitude fewer scenarios than MC and QMC scenario generation to achieve the same accuracy. It is indicated that the method scales well with the dimension of the distributionespecially when the underlying distribution is an affine transform of a product of univariate distributions, in which case the method appears scalable to thousands of random variables.},
  citeulike-article-id = {14087945},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10589-015-9751-7},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10589-015-9751-7},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:27:08},
  publisher            = {Springer US},
  timestamp            = {2020-02-29 19:20},
}

@Article{Cheng-Planchet-2018,
  author         = {Cheng, Po-Keng and Planchet, Frederic},
  date           = {2018-06-08},
  journaltitle   = {arXiv e-Print},
  title          = {Stochastic Deflator for an Economic Scenario Generator with Five Factors},
  url            = {https:://arxiv.org/abs/1806.02991},
  urldate        = {2019-03-07},
  abstract       = {In this paper, we implement a stochastic deflator with five economic and financial risk factors: interest rates, market price of risk, stock prices, default intensities, and convenience yields. We examine the deflator with different financial assets, such as stocks, zero-coupon bonds, vanilla options, and corporate coupon bonds. We find required regularity conditions to implement our stochastic deflator. Our numerical results show the reliability of the deflator approach in pricing financial derivatives.},
  day            = {8},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:20},
}

@Article{Chen-Xu-2014,
  author               = {Chen, Zhiping and Xu, Daobao},
  date                 = {2014-05},
  journaltitle         = {Applied Stochastic Models in Business and Industry},
  title                = {Knowledge-based scenario tree generation methods and application in multiperiod portfolio selection problem},
  doi                  = {10.1002/asmb.1970},
  number               = {3},
  pages                = {240--257},
  volume               = {30},
  abstract             = {A scenario tree is an efficient way to represent a stochastic data process in decision problems under uncertainty. This paper addresses how to efficiently generate appropriate scenario trees. A knowledge-based scenario tree generation method is proposed; the new method is further improved by accounting for subjective judgements or expectations about the random future. Compared with existing approaches, complicated mathematical models and time-consuming estimation, simulation and optimization problem solution are avoided in our knowledge-based algorithms, and large-scale scenario trees can be quickly generated. To show the advantages of the new algorithms, a multiperiod portfolio selection problem is considered, and a dynamic risk measure is adopted to control the intermediate risk, which is superior to the single-period risk measure used in the existing literature. A series of numerical experiments are carried out by using real trading data from the Shanghai stock market. The results show that the scenarios generated by our algorithms can properly represent the underlying distribution; our algorithms have high performance, say, a scenario tree with up to 10,000 scenarios can be generated in less than a half minute. The applications in the multiperiod portfolio management problem demonstrate that our scenario tree generation methods are stable, and the optimal trading strategies obtained with the generated scenario tree are reasonable, efficient and robust.},
  citeulike-article-id = {13988704},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/asmb.1970},
  day                  = {1},
  groups               = {Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 02:01:29},
  timestamp            = {2020-02-29 19:20},
}

@Article{Chen-Yan-2018,
  author               = {Chen, Zhiping and Yan, Zhe},
  date                 = {2018-10},
  journaltitle         = {Applied Stochastic Models in Business and Industry},
  title                = {Practical arbitrage-free scenario tree reduction methods and their applications in financial optimization},
  doi                  = {10.1002/asmb.2290},
  issn                 = {1524-1904},
  abstract             = {We construct an arbitrage-free scenario tree reduction model, from which some arbitrage-free scenario tree reduction algorithms are designed. They ensure that the reduced scenario trees are arbitrage free. Numerical results show the practicality and efficiency of the proposed algorithms. Results for multistage portfolio selection problems demonstrate the necessity and importance for guaranteeing that the reduced scenario trees are arbitrage free, as well as the practicality of the proposed arbitrage-free scenario tree reduction algorithms for financial optimization.},
  citeulike-article-id = {14500722},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/asmb.2290},
  day                  = {27},
  posted-at            = {2017-12-11 08:20:42},
  timestamp            = {2020-02-29 19:20},
}

@Article{Consigli-et-al-2012a,
  author               = {Consigli, Giorgio and Iaquinta, Gaetano and Moriggia, Vittorio},
  date                 = {2012-08},
  journaltitle         = {Quantitative Finance},
  title                = {Path-dependent scenario trees for multistage stochastic programmes in finance},
  doi                  = {10.1080/14697688.2010.518154},
  number               = {8},
  pages                = {1265--1281},
  volume               = {12},
  abstract             = {The formulation of dynamic stochastic programmes for financial applications generally requires the definition of a risk?reward objective function and a financial stochastic model to represent the uncertainty underlying the decision problem. The solution of the optimization problem and the quality of the resulting strategy will depend critically on the adopted financial model and its consistency with observed market dynamics. We present a recursive scenario approximation approach suitable for financial management problems, leading to a minimal yet sufficient representation of the randomness underlying the decision problem. The method relies on the definition of a benchmark probability space generated through Monte Carlo simulation and the implementation of a scenario reduction scheme. The procedure is tested on an interest rate vector process capturing market and credit risk dynamics in the fixed income market. The collected results show that a limited number of scenarios is sufficient to capture the exposure of the decision maker to interest rate and default risk.},
  citeulike-article-id = {14171152},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2010.518154},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2010.518154},
  day                  = {1},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:39:23},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 19:20},
}

@Article{DeGenaro-2016,
  author               = {{De Genaro}, Alan},
  date                 = {2016-06},
  journaltitle         = {Journal of Banking and Finance},
  title                = {Systematic multi-period stress scenarios with an application to CCP risk management},
  doi                  = {10.1016/j.jbankfin.2015.12.011},
  issn                 = {0378-4266},
  pages                = {119--134},
  volume               = {67},
  abstract             = {In the aftermath of the financial crisis of 2007-2008 regulators in multiple jurisdictions have laid the foundation of new regulatory standards aiming at strengthening systemic resilience. Among different initiatives, mandatory central clearing of standardized OTC derivatives has been one of the most prominent. Because OTC derivatives entail default management procedures that are far more complex than listed derivatives, risk management procedures have to follow suit. The recent paper by Vicente et al. (2015) propose an innovative way to calculate margin requirements by using multi-period robust optimization (RO) methods that accounts for important differences between OTC and listed derivatives default procedures. Motivated by this methodology, this paper proposes a hybrid framework to construct discrete uncertainty sets, in which each element of this set can be seen as multi-period stress scenarios, which are necessary to solve the RO problem faced by the Central Counterparty (CCP). When applied to determine the margin requirements, the present method provides both qualitative and quantitative results that outperform other robust optimization models such as Ben-Tal and Nemirovski (2000) and Bertsimas and Pachamanova (2008).},
  citeulike-article-id = {14087927},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.jbankfin.2015.12.011},
  groups               = {Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:00:49},
  timestamp            = {2020-02-29 19:21},
}

@Article{DeMeo-et-al-2019,
  author         = {{De Meo}, Emanuele},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Scenario Design for Macro-Financial Stress Testing},
  doi            = {10.2139/ssrn.3493554},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3493554},
  urldate        = {2019-12-26},
  abstract       = {The goal of this paper is to provide a possible approach to Scenario Design for selecting a stress scenario on economic growth, inflation and long-term interest rates in Italy. The Scenario Design framework belongs to the class of Second Generation Stress Tests and is composed of a few building blocks. First, multiple scenarios on the risk factors are generated simulating a Large Bayesian VAR for the Italian economy. Second, we take the perspective of a representative investor who aims to select a severe yet plausible scenario on the systematic risk factors follwing a factor investing strategy. Moreover, we compare the stress scenarios selected under two different approaches to measure plausibility: the Mahalanobis distance and Entropy pooling under three alternative subjective views with a clear economic narrative. We give evidence that our framework is suitable for the selection of a proper forward-looking severe yet plausible stress scenario.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@InCollection{deOliveira-Filomena-2016,
  author               = {{de Oliveira}, Alan D. and Filomena, Tiago P.},
  booktitle            = {XXXVI Congresso da Sociedade Brasileira de Computacao},
  date                 = {2016},
  title                = {Stochastic scenario generation: An empirical approach},
  abstract             = {We briefly discuss the differences among several methods to generate a scenario tree for stochastic optimization. First, the Monte Carlo Random sampling is presented, followed by the Fitting of the First Two Moments sampling, and lastly the Michaud sampling. Literature results are reviewed, taking into account distinctive features of each kind of methodology. According to the literature results, it is fundamental to consider the problem's unique characteristics to make the more appropriate choice on sampling method.},
  citeulike-article-id = {14500718},
  posted-at            = {2017-12-11 08:06:26},
  timestamp            = {2020-02-29 19:21},
}

@Article{Duarte-Rajagopal-1999,
  author         = {Duarte, Antonio Marcos and Rajagopal, Ram},
  date           = {1999-07-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {A Scenario-Based Approach to Optimal Currency Overlay},
  doi            = {10.3905/jpm.1999.319758},
  issn           = {0095-4918},
  number         = {4},
  pages          = {51--59},
  volume         = {25},
  abstract       = {Currency risk is a measure of a portfolio's potential losses due to changes in the relative value securities denominated in different currencies. Currency risk can be minimized using hedging techniques. The optimal currency overlay techniques proposed in the finance literature are based on Markowitz's mean-variance framework, which has shortcomings for both general asset allocation problems and currency hedging. Practitioners have begun to adopt scenario-based methodologies for asset allocation as a substitute for the mean-variance framework. This article presents a scenario-based approach for the optimal currency overlay. Three numerical examples illustrate its practical use and show that the choice of a loss function can produce quite different optimal currency allocations.},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{Edesess-Hambrecht-1980,
  author         = {Edesess, Michael and Hambrecht, George A.},
  date           = {1980-04-30},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Scenario forecasting},
  doi            = {10.3905/jpm.1980.408751},
  issn           = {0095-4918},
  number         = {3},
  pages          = {10--15},
  volume         = {6},
  day            = {30},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{Facchinato-Pola-2014,
  author       = {Simone Facchinato and Gianni Pola},
  date         = {2014},
  journaltitle = {SSRN e-Print},
  title        = {Managing uncertainty with diversification across macroeconomic scenarios (DAMS): from asset segmentation to portfolio},
  url          = {http://research-center.amundi.com/page/Publications/Discussion-Paper/2014/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  abstract     = {Recent history has provided an excellent laboratory to test the robustness of investment processes. Despite claims of diversification, most balanced portfolios and pension funds were concentrated on equity risk and, consequently, key investment decisions ultimately consisted in a single binary bet: buy or sell equity. This led to pro-cyclical returns and generated a broad debate on the effectiveness of active management in generating performance in difficult market conditions.

In 2011 AMUNDI Italy decided to revise the asset allocation process starting with a reinterpretation of portfolio diversification in terms of Diversification Across Macroeconomic Scenarios (DAMS). The main ambitions of DAMS are: (i) to explain complex patterns of large investment universes in terms of a limited number of factors and (ii) to catch up the market risk premium without being exposed to specific macroeconomic dynamics and asset idiosyncratic risk. In a previous study we illustrated the DAMS principle and implications in terms of asset segmentation.

The aim of this paper is to move towards a new framework for multi-asset portfolio management, what we call DAMS second generation. DAMS first generation is enriched with new concepts and tools that enable us (i) to infer market expectations on relevant macroeconomic factors (growth and inflation) and global risk premium, and (ii) to properly manage portfolios via strategic and tactical asset allocation.},
  groups       = {Diversified_Invest, Scenario_Market, Scenario_Portfolio, Invest_Diversif},
  howpublished = {Available at http://research-center.amundi.com/page/Publications/Discussion-Paper/Managing-uncertainty-with-Diversification-Across-Macroeconomic-Scenarios-DAMS-from-asset-segmentation-to-portfolio},
  organization = {Amundi},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:21},
}

@InProceedings{Feng-et-al-2018e,
  author         = {Feng, Jun and Li, Heng and Huang, Minlie and Liu, Shichen and Ou, Wenwu and Wang, Zhirong and Zhu, Xiaoyan},
  booktitle      = {Proceedings of the 2018 World Wide Web Conference on World Wide Web - WWW '18},
  date           = {2018-04-23},
  title          = {Learning to Collaborate: Multi-Scenario Ranking via Multi-Agent Reinforcement Learning},
  doi            = {10.1145/3178876.3186165},
  isbn           = {9781450356398},
  location       = {New York, New York, USA},
  pages          = {1939--1948},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3178876.3186165},
  urldate        = {2019-10-13},
  abstract       = {Ranking is a fundamental and widely studied problem in scenarios such as search, advertising, and recommendation. However, joint optimization for multi-scenario ranking, which aims to improve the overall performance of several ranking strategies in different scenarios, is rather untouched. Separately optimizing each individual strategy has two limitations. The first one is lack of collaboration between scenarios meaning that each strategy maximizes its own objective but ignores the goals of other strategies, leading to a sub-optimal overall performance. The second limitation is the inability of modeling the correlation between scenarios meaning that independent optimization in one scenario only uses its own user data but ignores the context in other scenarios. In this paper, we formulate multi-scenario ranking as a fully cooperative, partially observable, multi-agent sequential decision problem. We propose a novel model named Multi-Agent Recurrent Deterministic Policy Gradient (MA-RDPG) which has a communication component for passing messages, several private actors (agents) for making actions for ranking, and a centralized critic for evaluating the overall performance of the co-working actors. Each scenario is treated as an agent (actor). Agents collaborate with each other by sharing a global action-value function (the critic) and passing messages that encodes historical information across scenarios. The model is evaluated with online settings on a large E-commerce platform. Results show that the proposed model exhibits significant improvements against baselines in terms of the overall performance.},
  day            = {23},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@InProceedings{Fremont-et-al-2019,
  author         = {Fremont, Daniel J. and Dreossi, Tommaso and Ghosh, Shromona and Yue, Xiangyu and Sangiovanni-Vincentelli, Alberto L. and Seshia, Sanjit A.},
  booktitle      = {Proceedings of the 40th ACM SIGPLAN Conference on Programming Language Design and Implementation - PLDI 2019},
  date           = {2019-06-22},
  title          = {Scenic: a language for scenario specification and scene generation},
  doi            = {10.1145/3314221.3314633},
  isbn           = {9781450367127},
  location       = {New York, New York, USA},
  pages          = {63--78},
  publisher      = {ACM Press},
  url            = {http://dl.acm.org/citation.cfm?doid=3314221.3314633},
  urldate        = {2020-01-17},
  day            = {22},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@InProceedings{Garatti-Campi-2019,
  author         = {Garatti, Simone and Campi, Marco C.},
  booktitle      = {2019 18th European Control Conference (ECC)},
  date           = {2019-06-25},
  title          = {Complexity-based modulation of the data-set in scenario optimization},
  doi            = {10.23919/{ECC}.2019.8796160},
  isbn           = {978-3-907144-00-8},
  pages          = {1386--1391},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8796160/},
  urldate        = {2019-10-13},
  abstract       = {The scenario approach is a broad methodology for data-driven optimization that has found numerous applications in systems and control design. It consists in making a decision that is optimal with respect to a given criterion, while also being consistent with a sample of observations that are called the . More precisely, each scenario corresponds to a constraint and the solution is sought in the domain of feasibility of all scenario constraints. The level of robustness of the scenario solution is quantified by the , which is the probability that the scenario solution is not consistent with a new, out-of-sample, scenario. Recent studies have unveiled a profound link between the risk and the complexity of the solution (defined as the minimum amount of scenarios that is needed to reconstruct the solution). In this work, we leverage these results to introduce a new learning scheme where the size of the scenario sample is iteratively learned during optimization as a function of the complexity of the current solution. This new scheme implies a better exploitation of the information, so that one achieves a prescribed level of risk while saving many data as compared to standard scenario schemes. This paper presents the theoretical study that proves this result and illustrates it through a numerical example.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{Geyer-et-al-2011,
  author               = {Geyer, Alois and Hanke, Michael and Weissensteiner, Alex},
  date                 = {2011-09},
  journaltitle         = {SSRN e-Print},
  title                = {No-Arbitrage Bounds for Scenarios and Financial Optimization},
  url                  = {https://ssrn.com/abstract=1927222},
  abstract             = {We derive no-arbitrage bounds for expected excess returns to generate scenarios used in financial optimization. The bounds allow to distinguish three regions: one where arbitrage opportunities will never exist, a second where arbitrage may be present, and a third, where arbitrage opportunities will always exist. No-arbitrage bounds are derived in closed form for a given covariance matrix using the least possible number of scenarios. The same setting is also used in an algorithm to generate discrete scenarios and trees. Numerical results from solving two-stage asset allocation problems indicate that even for minimal tree size very accurate results can be obtained.},
  citeulike-article-id = {13911272},
  citeulike-linkout-0  = {http://ssrn.com/abstract=1927222},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2380096code138605.pdf?abstractid=1927222 and mirid=1},
  day                  = {15},
  groups               = {Scenario generation},
  howpublished         = {Available at SSRN: ssrn.com/abstract=1927222},
  owner                = {cristi},
  posted-at            = {2016-01-18 04:32:36},
  timestamp            = {2020-02-29 19:21},
}

@Article{Geyer-et-al-2013,
  author               = {Geyer, Alois and Hanke, Michael and Weissensteiner, Alex},
  date                 = {2013-09},
  journaltitle         = {Operations Research Letters},
  title                = {Scenario tree generation and multi-asset financial optimization problems},
  doi                  = {10.1016/j.orl.2013.06.003},
  issn                 = {0167-6377},
  number               = {5},
  pages                = {494--498},
  volume               = {41},
  abstract             = {We compare two popular scenario tree generation methods in the context of financial optimization: moment matching and scenario reduction. Using a simple problem with a known analytic solution, moment matching when ensuring absence of arbitrage replicates this solution precisely. On the other hand, even if the scenario trees generated by scenario reduction are arbitrage-free, the solutions are biased and highly variable. These results hold for correlated and uncorrelated asset returns, as well as for normal and non-normal returns.},
  citeulike-article-id = {13988767},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.orl.2013.06.003},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-03-27 04:38:33},
  timestamp            = {2020-02-29 19:21},
}

@Article{Geyer-et-al-2014a,
  author               = {Geyer, Alois and Hanke, Michael and Weissensteiner, Alex},
  date                 = {2014-07},
  journaltitle         = {European Journal of Operational Research},
  title                = {No-arbitrage bounds for financial scenarios},
  doi                  = {10.1016/j.ejor.2014.01.027},
  issn                 = {0377-2217},
  number               = {2},
  pages                = {657--663},
  volume               = {236},
  abstract             = {We derive no-arbitrage bounds for expected excess returns for financial scenarios. The bounds show whether arbitrage will never exist, may exist or will always exist. The bounds are derived in closed form for the least possible number of scenarios. Empirical examples illustrate the practical potential of knowing these bounds. We derive no-arbitrage bounds for expected excess returns to generate scenarios used in financial applications. The bounds allow to distinguish three regions: one where arbitrage opportunities will never exist, a second where arbitrage may be present, and a third, where arbitrage opportunities will always exist. No-arbitrage bounds are derived in closed form for a given covariance matrix using the least possible number of scenarios. Empirical examples illustrate the practical potential of knowing these bounds.},
  citeulike-article-id = {13988766},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2014.01.027},
  owner                = {cristi},
  posted-at            = {2016-03-27 04:37:20},
  timestamp            = {2020-02-29 19:21},
}

@Article{Grinold-1999,
  author         = {Grinold, Richard C.},
  date           = {1999-01-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Mean-Variance and Scenario-Based Approaches to Portfolio Selection},
  doi            = {10.3905/jpm.1999.319732},
  issn           = {0095-4918},
  number         = {2},
  pages          = {10--22},
  urldate        = {2019-06-29},
  volume         = {25},
  abstract       = {In this cautionary tale, the sorcerer's apprentice uses his mentor's magic before he knows how to control it. Matters quickly get out of hand. The author argues that the scenario-based expected utility maximization approach to portfolio optimization presents similar opportunities for misadventure. He shows how to avoid the danger, Alas, as with all sorcery, when the illusion is stripped away one sees that there is less there than initially supposed. The conventional mean-variance approach gives comparable answers with less bother and peril.},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{GroweKuska-Heitsch-2003,
  author         = {Growe-Kuska, N and Heitsch, H},
  date           = {2003},
  journaltitle   = {Power tech conference},
  title          = {Scenario reduction and scenario tree construction for power management problems},
  f1000-projects = {QuantInvest},
  publisher      = {ieeexplore.ieee.org},
  timestamp      = {2020-02-29 19:21},
}

@Article{Guerrero-et-al-2014,
  author               = {Guerrero, Vctor M. and Silva, Eliud and Gomez, Nicolas},
  date                 = {2014-01},
  journaltitle         = {Journal of Forecasting},
  title                = {Building Scenarios of Multiple Time Series that Take into Account the Effects of an Expected Intervention},
  doi                  = {10.1002/for.2271},
  number               = {1},
  pages                = {32--46},
  volume               = {33},
  abstract             = {We consider a forecasting problem that arises when an intervention is expected to occur on an economic system during the forecast horizon. The time series model employed is seen as a statistical device that serves to capture the empirical regularities of the observed data on the variables of the system without relying on a particular theoretical structure. Either the deterministic or the stochastic structure of a vector autoregressive error correction model of the system is assumed to be affected by the intervention. The information about the intervention effect is just provided by some linear restrictions imposed on the future values of the variables involved.

Formulas for restricted forecasts with intervention effects and their mean squared errors are derived as a particular case of Catlin's static updating theorem. An empirical illustration uses Mexican macroeconomic data on five variables and the restricted forecasts consider targets for years 2011-2014.},
  citeulike-article-id = {13935009},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2271},
  day                  = {1},
  groups               = {FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:09:42},
  timestamp            = {2020-02-29 19:21},
}

@InCollection{Hanke-Weissensteiner-2014,
  author               = {Hanke, Michael and Weissensteiner, Alex},
  booktitle            = {Wiley StatsRef: Statistics Reference Online},
  date                 = {2014-04},
  title                = {Arbitrage-Free Scenario Generation in Financial Optimization},
  doi                  = {10.1002/9781118445112.stat07925},
  editor               = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  isbn                 = {9781118445112},
  location             = {Chichester, UK},
  pages                = {1--6},
  publisher            = {John Wiley \& Sons, Ltd},
  abstract             = {Absence of arbitrage is an essential requirement for asset return scenarios used as the basis for financial optimization. We describe two approaches that can be used to arrive at arbitrage-free scenarios, together with theoretical results on their feasibility or existence in the form of restrictions on the sample size.},
  citeulike-article-id = {14500724},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/9781118445112.stat07925},
  day                  = {14},
  posted-at            = {2017-12-11 08:27:18},
  timestamp            = {2020-02-29 19:21},
}

@Article{Haugh-RuizLacedelli-2019,
  author         = {Haugh, Martin Brendan and Ruiz Lacedelli, Octavio},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {Scenario analysis for derivatives portfolios via dynamic factor models},
  doi            = {10.2139/ssrn.3424127},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3424127},
  urldate        = {2019-08-24},
  abstract       = {A classic approach to financial risk management is the use of scenario analysis to stress test portfolios. In the case of an SandP 500 options portfolio, for example, a scenario analysis might report a P&L of -1m in the event the SandP 500 falls 5\% and its implied volatility surface increases by 3 percentage points. But how accurate is this reported value of -1m? Such a number is typically computed under the (implicit) assumption that all other risk factors are set to zero. But this assumption is generally not justified as it ignores the often substantial statistical dependence among the risk factors. In particular, the expected values of the non-stressed factors conditional on the values of the stressed factors are generally non-zero. Moreover, even if the non-stressed factors were set to their conditional expected values rather than zero, the reported P&L might still be inaccurate due to convexity effects, particularly in the case of derivatives portfolios. A further weakness of this standard approach to scenario analysis is that the reported P&L numbers are generally not back-tested so their accuracy is not subjected to any statistical tests. There are many reasons for this but perhaps the main one is that scenario analysis for derivatives portfolios is typically conducted without having a probabilistic model for the underlying dynamics of the risk factors under the physical measure P. In this paper we address these weaknesses by embedding the scenario analysis within a dynamic factor model for the underlying risk factors. Such an approach typically requires multivariate state-space models that can model the real-world behavior of financial markets where risk factors are often latent, and that are sufficiently tractable so that we can compute (or simulate from) the conditional distribution of unstressed risk factors. We demonstrate how this can be done for observable as well as latent risk factors in examples drawn from options and fixed income markets. We show how the two forms of scenario analysis can lead to dramatically different results particularly in the case of portfolios that have been designed to be neutral to a subset of the risk factors.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{Hill-Vaysman-1998,
  author         = {Hill, Charles F. and Vaysman, Simon},
  date           = {1998-01-31},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {An approach to scenario hedging},
  doi            = {10.3905/jpm.24.2.83},
  issn           = {0095-4918},
  number         = {2},
  pages          = {83--92},
  urldate        = {2019-06-30},
  volume         = {24},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@InCollection{Hochreiter-2009,
  author               = {Hochreiter, Ronald},
  booktitle            = {Proceedings of the 2007 EvoWorkshops 2007 on EvoCoMnet, EvoFIN, EvoIASP,EvoINTERACTION, EvoMUSART, EvoSTOC and EvoTransLog: Applications of Evolutionary Computing},
  date                 = {2007},
  title                = {An Evolutionary Computation Approach to Scenario-Based Risk-Return Portfolio Optimization for General Risk Measures},
  doi                  = {10.1007/978-3-540-71805-5\_22},
  isbn                 = {978-3-540-71804-8},
  location             = {Valencia, Spain},
  pages                = {199--207},
  publisher            = {Springer-Verlag},
  abstract             = {Due to increasing complexity and non-convexity of financial engineering problems, biologically inspired heuristic algorithms gained significant importance especially in the area of financial decision optimization. In this paper, the stochastic scenario-based risk-return portfolio optimization problem is analyzed and solved with an evolutionary computation approach. The advantage of applying this approach is the creation of a common framework for an arbitrary set of loss distribution-based risk measures, regardless of their underlying structure. Numerical results for three of the most commonly used risk measures conclude the paper.},
  address              = {Berlin, Heidelberg},
  citeulike-article-id = {14160233},
  citeulike-linkout-0  = {http://portal.acm.org/citation.cfm?id=1574763},
  citeulike-linkout-1  = {http://dx.doi.org/10.1007/978-3-540-71805-522},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 18:22:46},
  timestamp            = {2020-02-29 19:21},
}

@Article{Izmirlian-2018,
  author               = {Izmirlian, Grant},
  date                 = {2018-01-11},
  journaltitle         = {arXiv e-Print},
  title                = {Average Power and lambda-power in Multiple Testing Scenarios when the Benjamini-Hochberg False Discovery Rate Procedure is Used},
  eprint               = {1801.03989},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1801.03989},
  abstract             = {We discuss several approaches to defining power in studies designed around the Benjamini-Hochberg (BH) false discovery rate (FDR) procedure. We focus primarily on the and the lambda, which are the expected true positive fraction and the probability that the true positive fraction exceeds lambda, respectively. We show that the average power converges as the number of simultaneous tests tends to infinity, to a limit that is nearly equivalent to the power introduced independently by JungSH:2005 and by LiuP:2007. Furthermore, we prove a CLT which allows asymptotic approximation to the lambda-power. Moreover, we prove SLLNs and CLTs for all quantities connected to the BH-FDR procedure: the positive call fraction, true positive fraction, and false discovery fraction, with full characterization of almost sure limits and limits in distribution. We discuss ramifications of the CLT for the false discovery fraction, introducing a procedure which allows tighter control of the false discovery fraction than the BH-FDR that can be used at the design and analysis steps. We conduct a large simulation study covering a fairly substantial portion of the space of possible inputs. We show its application in design of a biomarker study, a micro-array experiment and a GWAS study.},
  citeulike-article-id = {14518478},
  citeulike-linkout-0  = {http://arxiv.org/abs/1801.03989},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1801.03989},
  day                  = {11},
  groups               = {Test_Scenario, Proba_Test},
  posted-at            = {2018-01-17 20:49:56},
  timestamp            = {2020-02-29 19:21},
}

@Article{Jakobsons-2016,
  author         = {Jakobsons, Edgars},
  date           = {2016-01-01},
  journaltitle   = {Statistics \& Risk Modeling},
  title          = {Scenario aggregation method for portfolio expectile optimization},
  doi            = {10.1515/strm-2016-0008},
  issn           = {2193-1402},
  number         = {1-2},
  volume         = {33},
  abstract       = {The statistical functional expectile has recently attracted the attention of researchers in the area of risk management, because it is the only risk measure that is both coherent and elicitable. In this article, we consider the portfolio optimization problem with an expectile objective. Portfolio optimization problems corresponding to other risk measures are often solved by formulating a linear program (LP) that is based on a sample of asset returns. We derive three different LP formulations for the portfolio expectile optimization problem, which can be considered as counterparts to the LP formulations for the Conditional Value-at-Risk (CVaR) objective in the works of Rockafellar and Uryasev [43], Ogryczak and Sliwinski [41] and Espinoza and Moreno [21]. When the LPs are based on a simulated sample of the true (assumed continuous) asset returns distribution, the portfolios obtained from the LPs are only approximately optimal. We conduct a numerical case study estimating the suboptimality of the approximate portfolios depending on the sample size, number of assets, and tail-heaviness of the asset returns distribution. Further, the computation times using the three LP formulations are analyzed, showing that the formulation that is based on a scenario aggregation approach is considerably faster than the two alternatives.},
  day            = {1},
  f1000-projects = {QuantInvest},
  groups         = {PortfOptim_Scenario, Scenario_Risk, Scenario_Portfolio},
  timestamp      = {2020-02-29 19:21},
}

@InCollection{Jamshidian-Zhu-2008,
  author         = {Jamshidian, Farshid and Zhu, Yu},
  booktitle      = {Encyclopedia of quantitative risk analysis and assessment},
  date           = {2008-09-15},
  title          = {Scenario simulation method for risk management},
  doi            = {10.1002/9780470061596.risk0643},
  editor         = {Melnick, Edward L. and Everitt, Brian S.},
  isbn           = {9780470035498},
  location       = {Chichester, UK},
  publisher      = {John Wiley \& Sons, Ltd},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{Kaut-2013,
  author               = {Kaut, Michal},
  date                 = {2014},
  journaltitle         = {Computational Management Science},
  title                = {A copula-based heuristic for scenario generation},
  doi                  = {10.1007/s10287-013-0184-4},
  number               = {4},
  pages                = {503--516},
  volume               = {11},
  abstract             = {This paper presents a new heuristic for generating scenarios for two-stage stochastic programs. The method uses copulas to describe the dependence between the marginal distributions, instead of the more common correlations. The heuristic is then tested on a simple portfolio-selection model, and compared to two other scenario-generation methods.},
  citeulike-article-id = {13988910},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10287-013-0184-4},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10287-013-0184-4},
  owner                = {cristi},
  posted-at            = {2016-03-27 15:12:30},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-29 19:21},
}

@Article{Kaut-Lium-2015,
  author               = {Kaut, Michal and Lium, Arnt-Gunnar},
  date                 = {2015-01},
  journaltitle         = {Kybernetika},
  title                = {Scenario generation with distribution functions and correlations},
  doi                  = {10.14736/kyb-2014-6-1049},
  issn                 = {0023-5954},
  pages                = {1049--1064},
  abstract             = {In this paper, we present a method for generating scenarios for two-stage stochastic programs, using multivariate distributions specified by their marginal distributions and the correlation matrix. The margins are described by their cumulative distribution functions and we allow each margin to be of different type. We demonstrate the method on a model from stochastic service network design and show that it improves the stability of the scenario-generation process, compared to both sampling and a method that matches moments and correlations.},
  citeulike-article-id = {13988913},
  citeulike-linkout-0  = {http://dx.doi.org/10.14736/kyb-2014-6-1049},
  day                  = {05},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-03-27 15:17:12},
  timestamp            = {2020-02-29 19:21},
}

@Article{Lau-2014,
  author               = {Lau, Jonathan},
  date                 = {2014},
  journaltitle         = {SSRN e-Print},
  title                = {Economic Scenario Generator and Stochastic Modelling},
  citeulike-article-id = {13931097},
  groups               = {Scenario generation},
  howpublished         = {Available at http://www.actuariesindia.org/(S(oneht0qjkc5i4zmc0eqdp5vm))/CILA/CBLI2014/ESGJonathanLau!.pdf},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:38:55},
  timestamp            = {2020-02-29 19:21},
}

@Article{Li-Gao-2019,
  author         = {Li, Qiao and Gao, David Wenzhong},
  date           = {2019-08-30},
  journaltitle   = {arXiv e-Print},
  title          = {Fast Scenario Reduction for Power Systems by Deep Learning},
  url            = {https://arxiv.org/abs/1908.11486},
  urldate        = {2019-09-06},
  abstract       = {Scenario reduction is an important topic in stochastic programming problems. Due to the random behavior of load and renewable energy, stochastic programming becomes a useful technique to optimize power systems. Thus, scenario reduction gets more attentions in recent years. Many scenario reduction methods have been proposed to reduce the scenario set in a fast speed. However, the speed of scenario reduction is still very slow, in which it takes at least several seconds to several minutes to finish the reduction. This limitation of speed prevents stochastic programming to be implemented in real-time optimal control problems. In this paper, a fast scenario reduction method based on deep learning is proposed to solve this problem. Inspired by the deep learning based image process, recognition and generation methods, the scenario data are transformed into a 2D image-like data and then to be fed into a deep convolutional neural network (DCNN). The output of the DCNN will be an "image" of the reduced scenario set. Since images can be processed in a very high speed by neural networks, the scenario reduction by neural network can also be very fast. The results of the simulation show that the scenario reduction with the proposed DCNN method can be completed in very high speed.},
  day            = {30},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@Article{Lohndorf-2016,
  author               = {Lohndorf, Nils},
  date                 = {2016-11},
  journaltitle         = {European Journal of Operational Research},
  title                = {An empirical analysis of scenario generation methods for stochastic optimization},
  doi                  = {10.1016/j.ejor.2016.05.021},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {121--132},
  volume               = {255},
  abstract             = {This work presents an empirical analysis of popular scenario generation methods for stochastic optimization, including quasi-Monte Carlo, moment matching, and methods based on probability metrics, as well as a new method referred to as Voronoi cell sampling. Solution quality is assessed by measuring the error that arises from using scenarios to solve a multi-dimensional newsvendor problem, for which analytical solutions are available. In addition to the expected value, the work also studies scenario quality when minimizing the expected shortfall using the conditional value-at-risk. To quickly solve problems with millions of random parameters, a reformulation of the risk-averse newsvendor problem is proposed which can be solved via Benders decomposition. The empirical analysis identifies Voronoi cell sampling as the method that provides the lowest errors, with particularly good results for heavy-tailed distributions. A controversial finding concerns evidence for the ineffectiveness of widely used methods based on minimizing probability metrics under high-dimensional randomness.},
  citeulike-article-id = {14500717},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2016.05.021},
  posted-at            = {2017-12-11 07:58:42},
  timestamp            = {2020-02-29 19:21},
}

@TechReport{Marchioro-2013,
  author               = {Marchioro, Marco},
  date                 = {2013},
  institution          = {StatPro},
  title                = {Portfolio risk management with efficiently simulated scenarios},
  abstract             = {We describe a method-used, among others, by financial-software firm StatPro-to perform portfolio risk analysis based on a two-tier client/server approach. The risk server computes the numerical simulations of single-asset prices for a wide universe of investable instruments. The risk clients, using the server outcome, compute portfolio cash risk scenarios, stress-test simulations, and bid/ask liquidity spreads. We focus on the risk-client implementation and describe the details needed to compute the different scenario types for a portfolio of heterogeneous assets. It is also shown how it is necessary to treat differently bond-like instruments that always have a positive quote, futures that are settled on a margin account, and swap-like contracts that may have a positive or a negative net-present value. Finally we show how the computation of daily simulations can be used to estimate risk for longer time horizon, even when the financial instrument considered have special bounding constraints.},
  citeulike-article-id = {14387309},
  posted-at            = {2017-07-03 19:13:35},
  timestamp            = {2020-02-29 19:21},
}

@Article{Mehrotra-Papp-2013,
  author               = {Mehrotra, Sanjay and Papp, David},
  date                 = {2013-05},
  journaltitle         = {SIAM Journal on Optimization},
  title                = {Generating Moment Matching Scenarios Using Optimization Techniques},
  doi                  = {10.1137/110858082},
  issn                 = {1052-6234},
  number               = {2},
  pages                = {963--999},
  volume               = {23},
  abstract             = {An optimization based method is proposed to generate moment matching scenarios for numerical integration and its use in stochastic programming. The main advantage of the method is its flexibility: it can generate scenarios matching any prescribed set of moments of the underlying distribution rather than matching all moments up to a certain order, and the distribution can be defined over an arbitrary set. This allows for a reduction in the number of scenarios and allows the scenarios to be better tailored to the problem at hand. The method is based on a semi-infinite linear programming formulation of the problem that is shown to be solvable with polynomial iteration complexity. A practical column generation method is implemented. The column generation subproblems are polynomial optimization problems; however, they need not be solved to optimality. It is found that the columns in the column generation approach can be efficiently generated by random sampling. The number of scenarios generated matches a lower bound of Tchakaloff's. The rate of convergence of the approximation error is established for continuous integrands, and an improved bound is given for smooth integrands. Extensive numerical experiments are presented in which variants of the proposed method are compared to Monte Carlo and quasi-Monte Carlo methods on both numerical integration problems and stochastic optimization problems. The benefits of being able to match any prescribed set of moments, rather than all moments up to a certain order, is also demonstrated using optimization problems with 100-dimensional random vectors. Empirical results show that the proposed approach outperforms Monte Carlo and quasi-Monte Carlo based approaches on the tested problems.},
  citeulike-article-id = {14087940},
  citeulike-linkout-0  = {http://dx.doi.org/10.1137/110858082},
  day                  = {16},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:23:33},
  timestamp            = {2020-02-29 19:21},
}

@Article{Meucci-2010c,
  author               = {Meucci, Attilio},
  date                 = {2010-11},
  journaltitle         = {SSRN e-Print},
  title                = {Historical Scenarios with Fully Flexible Probabilities},
  url                  = {https://ssrn.com/abstract=1696802},
  abstract             = {After reviewing the parametric and scenario-based approaches to risk management, we discuss a methodology to enhance the flexibility of the scenario-based approach. We change the probability of each scenario, and then we compute the ensuing p and l distribution and all relevant statistics such as VaR and volatility. The probabilities can be changed to reflect specific market conditions, advanced estimation techniques, or partial information, using the entropy-based Fully Flexible Views technique. The implementation of this approach is trivial, as no costly repricing is needed. Commented code is available at symmys.com.},
  citeulike-article-id = {14160260},
  citeulike-linkout-0  = {http://ssrn.com/abstract=1696802},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID1843925code403805.pdf?abstractid=1696802 and mirid=1},
  day                  = {14},
  groups               = {Scenario generation, Scenario_Market, Scenario_Risk},
  owner                = {zkgst0c},
  posted-at            = {2016-10-12 19:43:00},
  timestamp            = {2020-02-29 19:21},
}

@Article{Mokinski-2017,
  author         = {Mokinski, Frieder},
  date           = {2017},
  journaltitle   = {SSRN e-Print},
  title          = {A Severity Function Approach to Scenario Selection},
  url            = {https://ssrn.com/abstract=3083989},
  abstract       = {The severity function approach (abbreviated SFA) is a method of selecting adverse scenarios from a multivariate density. It requires the scenario user (e.g. an agency that runs banking sector stress tests) to specify a "severity function", which maps candidate scenarios into a scalar severity metric. The higher the value of this metric, the more harmful a scenario is. In selecting a scenario the SFA proceeds as follows: First, it isolates a set of equally severe scenario candidates. This set is determined by the condition that more severe scenarios only occur with some user-specified probability. Second, from this set it selects the candidate with the highest probability density, i.e. the most plausible scenario. The approach hence operationalizes the mantra that "scenarios should be severe yet plausible".},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:21},
}

@InCollection{Moudiki-Planchet-2016,
  author               = {Moudiki, Thierry and Planchet, Frederic},
  booktitle            = {Modelling in Life Insurance - A Management Perspective},
  date                 = {2016},
  title                = {Economic Scenario Generators},
  doi                  = {10.1007/978-3-319-29776-7\_4},
  editor               = {Laurent, Jean-Paul and Norberg, Ragnar and Planchet, Frederic},
  pages                = {81--104},
  publisher            = {Springer International Publishing},
  series               = {EAA Series},
  abstract             = {The projection of economic and financial risk factors is a key element of prospective analyzes made by life insurers, both for the calculation of reserves under Solvency 2 and for the asset allocation and management of financial risks. This projection is achieved in practice through - economic scenario generators- (ESG), which are inputs for the calculus of the economic value of assets and liabilities and the analysis of the distribution of this value. The calculation of economic values is based on the - no free lunch- assumption and therefore leads to model the risk factors in a riskneutral probability, while the analysis of the distribution of these values requires the projection of these factors under the historical probability. Therefore, the insurer must handle different representations of the risk factors, which requires looking at the characteristics of a risk neutral ESG, those of an - historical- one and the possible need for coherence between these two representations. This is what we propose to do in this chapter.},
  citeulike-article-id = {14071351},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-319-29776-74},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-319-29776-74},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-06-18 08:17:12},
  timestamp            = {2020-02-29 19:21},
  year                 = {2016},
}

@Article{Mulvey-et-al-1999,
  author               = {Mulvey, John M. and Rosenbaum, Daniel P. and Shetty, Bala},
  date                 = {1999-11},
  journaltitle         = {European Journal of Operational Research},
  title                = {Parameter estimation in stochastic scenario generation systems},
  doi                  = {10.1016/s0377-2217(98)90323-x},
  issn                 = {0377-2217},
  number               = {3},
  pages                = {563--577},
  volume               = {118},
  abstract             = {Scenario analysis offers an effective tool for addressing the stochastic elements in multi-period financial planning models. Critical to any scenario generation process is the estimation of the input parameters of the underlying stochastic model for economic factors. In this paper, we propose a new approach for estimation, known as the integrated parameter estimation (IPE). This approach combines the significant features of other well-known estimation techniques within a non-convex multiple objective optimization framework, with the objective weights controlling the relative importance of the features. We solve the non-convex optimization problem using adaptive memory programming a variation of tabu search. Based on a short interest rate model using UK treasury rates from 1980 to 1995, the integrated approach compares favorably with maximum likelihood and the generalized method of moments. We also evaluate performance with Towers Perrin's CAP:Link scenario generation system.},
  citeulike-article-id = {14087996},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/s0377-2217(98)90323-x},
  owner                = {cristi},
  posted-at            = {2016-07-01 12:05:18},
  timestamp            = {2020-02-29 19:21},
}

@Conference{OrtecFinance-2014,
  author               = {{Ortec Finance}},
  booktitle            = {CFA Society Switzerland},
  date                 = {2014},
  title                = {Ex ante risk management with scenarios},
  abstract             = {Ex-ante risk management with help of scenarios including important information such as the impact of the business cycle is an innovative methodology used by sophisticated managers gaining worldwide popularity. Advanced stochastic scenario analysis supports financial decision-making when dealing with uncertainty through accounting for structural changes impacting trend values as well as short term events influencing the market momentum. The extent to which investor objectives will be achieved without violating risk limits is influenced by the path that financial and economic risk drivers will follow. A frequency domain based approach differs from traditional simulation approaches through the creation of plausible and relevant scenarios.

The Dynamic Scenario Generator (DSG) employs a unique combination of statistical and econometric techniques. The methodology is a mixture of frequency domain techniques, dynamic factor models and special approaches for dealing with non-normal distribution characteristics such as skewness and fat tails. The main purpose of the central frequency domain methodology is to describe all empirical features ( stylized facts ) of the time series behavior of financial and economic variables simultaneously, rather than focusing on only one or a few aspects in isolation.},
  citeulike-article-id = {13931099},
  groups               = {Scenario generation, Scenario_Risk, [nbkcbu3:]},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:46:25},
  timestamp            = {2020-02-29 19:21},
}

@Article{Ortega-et-al-2009,
  author               = {Ortega, Juan-Pablo and Pullirsch, Rainer and Teichmann, Josef and Wergieluk, Julian},
  date                 = {2009-08},
  journaltitle         = {arXiv e-Print},
  title                = {A new approach for scenario generation in Risk management},
  eprint               = {0904.0624},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/0904.0624},
  abstract             = {We provide a new dynamic approach to scenario generation for the purposes of risk management in the banking industry. We connect ideas from conventional techniques -- like historical and Monte Carlo simulation -- and we come up with a hybrid method that shares the advantages of standard procedures but eliminates several of their drawbacks. Instead of considering the static problem of constructing one or ten day ahead distributions for vectors of risk factors, we embed the problem into a dynamic framework, where any time horizon can be consistently simulated. Additionally, we use standard models from mathematical finance for each risk factor, whence bridging the worlds of trading and risk management.

Our approach is based on stochastic differential equations (SDEs), like the HJM-equation or the Black-Scholes equation, governing the time evolution of risk factors, on an empirical calibration method to the market for the chosen SDEs, and on an Euler scheme (or high-order schemes) for the numerical evaluation of the respective SDEs. The empirical calibration procedure presented in this paper can be seen as the SDE-counterpart of the so called Filtered Historical Simulation method; the behavior of volatility stems in our case out of the assumptions on the underlying SDEs. Furthermore, we are able to easily incorporate middle-size and large-size events within our framework always making a precise distinction between the information obtained from the market and the one coming from the necessary a-priori intuition of the risk manager.

Results of one concrete implementation are provided.},
  citeulike-article-id = {4518561},
  citeulike-linkout-0  = {http://arxiv.org/abs/0904.0624},
  citeulike-linkout-1  = {http://arxiv.org/pdf/0904.0624},
  day                  = {19},
  groups               = {Scenario generation, Scenario_Risk},
  howpublished         = {Available at Arxiv: http://arxiv.org/abs/0904.0624},
  owner                = {cristi},
  posted-at            = {2016-01-29 19:52:34},
  timestamp            = {2020-02-29 19:21},
}

@Article{Patomaki-2010,
  author               = {Patomaki, Heikki},
  date                 = {2010-06},
  journaltitle         = {Globalizations},
  title                = {What Next? An Explanation of the 2008 2009 Slump and Two Scenarios of the Shape of Things to Come},
  doi                  = {10.1080/14747731003593174},
  number               = {1-2},
  pages                = {67--85},
  volume               = {7},
  abstract             = {In order to build scenarios of possible futures and grasp the structural liabilities and tendencies of global financial markets, we do not need just historical analogies to past crises and collapses but also a conceptual-theoretical model that explains the characteristic mechanisms of financial markets. Firstly, I summarise the neoclassical understanding of financial markets and its characteristic effects. This understanding gave ex post legitimisation to the re-emergence of global finance in the early 1970s, and has subsequently justified and encouraged its rise to predominance in the world economy. I provide reasons to suspect that the orthodox account is misleading not only because it has been unable to anticipate the 2008?2009 crisis (or any other major crisis) but more fundamentally because it lacks insight even into the basic operations of financial markets. Secondly, I sketch an explanatory model of the 2008?2009 financial crisis, based on Keynes and Minsky as well as on concepts derived from Schumpeter, chaos theory and theory of collective action and rationality. This explanation provides the basis for two short-term scenarios of future developments, involving the possibility of a major crash in the late 2010s or around 2020; (also pathological) learning; and the emergence of green global-Keynesian policies and institutions. I conclude by suggesting that the era of neoliberalism is likely to come to an end by 2030, having lasted for about half a century. Para poder crear situaciones de posibles futuros financieros y llegar a comprender las responsabilidades legales y las tendencias de los mercados financieros globales, no necesitamos tan solo analogas historicas de crisis y desplomes anteriores, sino tambien un modelo teorico conceptual que explica los mecanismos caractersticos de los mercados financieros. Primero, resumo el entendimiento neoclasico de los mercados financieros y sus efectos caractersticos. Este entendimiento dio una legitimacion retrospectiva al resurgimiento de las finanzas globales a principio de la decada de 1970, y subsecuentemente ha justificado e impulsado su subida al predominio en la economa mundial. Proporciono razones para sospechar que el informe ortodoxo es desorientador no solo porque no ha sido capaz de anticipar la crisis de 2008?2009 (o ninguna otra crisis mayor), pero fundamentalmente porque carece de conocimiento profundo de las operaciones basicas de los mercados financieros. Segundo, esbozo un modelo explicativo de la crisis financiera, en base a Keynes y Minsky como en los conceptos derivados de Shumpeter, la teora del caos y de la accion colectiva y de la racionalidad. Esta explicacion provee la base para dos situaciones a corto tiempo de desarrollos futuros, incorporando la posibilidad de un desplome mayor a finales del 2010 o alrededor del 2020; (tambien aprendizaje patologico); y el surgimiento de las polticas Keynesianas verdes globales y de las instituciones. Concluyo con la sugerencia que la era del neoliberalismo probablemente llegara a su fin antes de 2030, habiendo durado cerca de medio siglo.},
  citeulike-article-id = {8392007},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14747731003593174},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/routledg/rglo/2010/00000007/F0020001/art00007},
  citeulike-linkout-2  = {http://www.tandfonline.com/doi/abs/10.1080/14747731003593174},
  day                  = {1},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-04-02 09:31:32},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 19:22},
}

@Article{Pinar-2007,
  author               = {Pinar, Mustafa},
  date                 = {2007-04},
  journaltitle         = {OR Spectrum},
  title                = {Robust scenario optimization based on downside-risk measure for multi-period portfolio selection},
  doi                  = {10.1007/s00291-005-0023-2},
  issn                 = {0171-6468},
  number               = {2},
  pages                = {295--309},
  volume               = {29},
  abstract             = {We develop and test multistage portfolio selection models maximizing expected end-of-horizon wealth while minimizing one-sided deviation from a target wealth level. The trade-off between two objectives is controlled by means of a non-negative parameter as in Markowitz Mean-Variance portfolio theory. We use a piecewise-linear penalty function, leading to linear programming models and ensuring optimality of subsequent stage decisions. We adopt a simulated market model to randomly generate scenarios approximating the market stochasticity. We report results of rolling horizon simulation with two variants of the proposed models depending on the inclusion of transaction costs, and under different simulated stock market conditions. We compare our results with the usual stochastic programming models maximizing expected end-of-horizon portfolio value. The results indicate that the robust investment policies are indeed quite stable in the face of market risk while ensuring expected wealth levels quite similar to the competing expected value maximizing stochastic programming model at the expense of solving larger linear programs.},
  citeulike-article-id = {1137479},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s00291-005-0023-2},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/klu/291/2007/00000029/00000002/00000023},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1007/s00291-005-0023-2},
  groups               = {PortfOptim_Robust, PortfOptim_Scenario, Scenario_Portfolio, Optimiz_Robust},
  owner                = {cristi},
  posted-at            = {2016-11-04 20:45:10},
  publisher            = {Springer-Verlag},
  timestamp            = {2020-02-29 19:22},
}

@Article{Ponomareva-et-al-2015,
  author               = {Ponomareva, K. and Roman, D. and Date, P.},
  date                 = {2015-02},
  journaltitle         = {European Journal of Operational Research},
  title                = {An algorithm for moment-matching scenario generation with application to financial portfolio optimisation},
  doi                  = {10.1016/j.ejor.2014.07.049},
  issn                 = {0377-2217},
  number               = {3},
  pages                = {678--687},
  volume               = {240},
  abstract             = {Mean, covariance, average marginal third and fourth moments are matched exactly. Scenarios and corresponding probability weights are produced without optimisation. The algorithm is used in a mean-CVaR portfolio optimisation model. Results show desirable in-sample and out-of-sample stability. Good solutions can be obtained with a relatively small number of scenarios. We present an algorithm for moment-matching scenario generation. This method produces scenarios and corresponding probability weights that match exactly the given mean, the covariance matrix, the average of the marginal skewness and the average of the marginal kurtosis of each individual component of a random vector. Optimisation is not employed in the scenario generation process and thus the method is computationally more advantageous than previous approaches. The algorithm is used for generating scenarios in a mean-CVaR portfolio optimisation model. For the chosen optimisation example, it is shown that desirable properties for a scenario generator are satisfied, including in-sample and out-of-sample stability. It is also shown that optimal solutions vary only marginally with increasing number of scenarios in this example; thus, good solutions can apparently be obtained with a relatively small number of scenarios. The proposed method can be used either on its own as a computationally inexpensive scenario generator or as a starting point for non-convex optimisation based scenario generators which aim to match all the third and the fourth order marginal moments (rather than average marginal moments).},
  citeulike-article-id = {13989059},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2014.07.049},
  groups               = {Scenario generation, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 17:40:07},
  timestamp            = {2020-02-29 19:22},
}

@Article{Rios-et-al-2015,
  author               = {Rios, Ignacio and Wets, RogerJ-B and Woodruff, DavidL},
  date                 = {2015},
  journaltitle         = {Computational Management Science},
  title                = {Multi-period forecasting and scenario generation with limited data},
  doi                  = {10.1007/s10287-015-0230-5},
  number               = {2},
  pages                = {267--295},
  volume               = {12},
  abstract             = {Data for optimization problems often comes from (deterministic) forecasts, but it is naive to consider a forecast as the only future possibility. A more sophisticated approach uses data to generate alternative future scenarios, each with an attached probability. The basic idea is to estimate the distribution of forecast errors and use that to construct the scenarios. Although sampling from the distribution of errors comes immediately to mind, we propose instead to approximate rather than sample. Benchmark studies show that the method we propose works well.},
  citeulike-article-id = {14171144},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10287-015-0230-5},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10287-015-0230-5},
  groups               = {Scenario_Best},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:13:18},
  publisher            = {Springer Berlin Heidelberg},
  timestamp            = {2020-02-29 19:22},
}

@Article{Romanko-Mausser-2016,
  author         = {Romanko, Oleksandr and Mausser, Helmut},
  date           = {2016-02},
  journaltitle   = {Annals of Operations Research},
  title          = {Robust scenario-based value-at-risk optimization},
  doi            = {10.1007/s10479-015-1822-8},
  issn           = {0254-5330},
  number         = {1-2},
  pages          = {203--218},
  volume         = {237},
  abstract       = {This paper develops and tests a heuristic algorithm for scenario-based value-at-risk (VaR) optimization. Due to the high computational complexity of VaR optimization, conditional value-at-risk-based proxies are utilized for VaR objectives. It is shown that our heuristic algorithm obtains robust results with low computational complexity.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_Risk, Optimiz_Robust},
  timestamp      = {2020-02-29 19:22},
}

@Book{Romisch-2009,
  author         = {Romisch, Werner},
  date           = {2009-10-26},
  title          = {Scenario reduction techniques in stochastic programming},
  isbn           = {3-642-04943-5, 978-3-642-04943-9},
  publisher      = {Springer-Verlag},
  url            = {https://dl.acm.org/citation.cfm?id=1814089},
  urldate        = {2019-10-12},
  abstract       = {Stochastic programming problems appear as mathematical models for optimization problems under stochastic uncertainty. Most computational approaches for solving such models are based on approximating the underlying probability distribution by a probability measure with finite support. Since the computational complexity for solving stochastic programs gets worse when increasing the number of atoms (or scenarios), it is sometimes necessary to reduce their number. Techniques for scenario reduction often require fast heuristics for solving combinatorial subproblems. Available techniques are reviewed and open problems are discussed.},
  day            = {26},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Rosen-2015,
  author               = {Rosen, Dan},
  date                 = {2015-10},
  journaltitle         = {SSRN e-Print},
  title                = {Integrating Economic Scenarios with Advanced Scenario Analytics to Manage Investment Portfolios},
  url                  = {https://www.researchgate.net/publication/286231794_Integrating_Economic_Scenarios_with_Advanced_Scenario_Analytics_to_Manage_Investment_Portfolios},
  abstract             = {Scenarios are the language of Risk. The quality of a risk management analysis depends on the ability to generate relevant forward-looking scenarios that properly represent the future. While the future cannot be predicted, the combination of economic and expert analysis, with advanced scenario and portfolio analytics, provides a strong basis for managing risk and making better, more informed investment decisions.

In this workshop, we present the latest simulation methodologies to analyze in practice the risk and investment opportunities of financial portfolios. We show how economic forecasts and expert views can be effectively combined with advanced Monte Carlo and Conditional Scenario methods to create meaningful scenarios for buy-side investment analysis, stress testing of credit portfolios, and systemic risk.

Finally, we illustrate their practical application through various real-life examples where we analyze the performance of portfolios under recent economic research reports, regulatory scenarios and expert subjective views.},
  citeulike-article-id = {13923408},
  groups               = {Scenario generation, PortfOptim_Scenario, Scenario_Risk, Scenario_SubsetFactor, StressTest_ExpertView},
  howpublished         = {Available at https://www.researchgate.net/publication/286231794IntegratingEconomicScenarioswithAdvancedScenarioAnalyticstoManageInvestmentPortfolios},
  owner                = {zkgst0c},
  posted-at            = {2016-02-01 18:09:05},
  timestamp            = {2020-02-29 19:22},
}

@Article{Rosen-2015a,
  author               = {Rosen, Dan},
  date                 = {2015},
  journaltitle         = {SSRN e-Print},
  title                = {Re-Thinking Scenarios: Stress Testing of Multi-Asset Portfolios by Integrating Economic Scenarios with Advanced Simulation Analytics},
  url                  = {http://www.fields.utoronto.ca/talks/re-thinking-scenarios-stress-testing-multi-asset-portfolios-integrating-economic-scenario-0},
  abstract             = {Scenarios are the language of Risk. While scenario analysis and stress testing have been an explicit part of risk management methodologies and systems for over two decades, the typical scenario and stress testing tools haven't evolved much and are still generally quite static and largely subjective. In this talk, we present a simple and powerful approach to create meaningful stress scenarios for risk management and investment analysis of multi-asset portfolios, which effectively combines economic forecasts and expert views with portfolio simulation methods.

Expert scenarios are typically described in terms of a small number of key economic variables or factors. However, when applied to a portfolio, they are incomplete: they generally do not describe what occurs to all relevant market risk factors that affect the portfolio. We need to understand how these market risk factors behave, conditional on the outcome of the economic factors. The key insight to our approach is that the conditional expectation, and more generally the full conditional distribution of all the factors, and of the portfolio P and L, can be estimated directly from a pre-computed simulation using Least Squares Regression.

We refer to this approach as Least Squares Stress Testing (LSST). LSST is a simulation-based conditional scenario generation method that offers many advantages over more traditional analytical methods. Simulation techniques are simple, flexible, and provide very transparent results, which are auditable and easy to explain. LSST can be applied to both market and credit risk stress testing with a large number of risk factors, which can follow completely general stochastic processes, with fat-tails, non-parametric and general codependence structures, autocorrelation, etc. LSST further produces explicit risk factor P and L contributions. From a methodology perspective, we also discuss some of the assumptions the LSST approach, statistical tests to check when these assumptions fail, and remedies that can be applied.

Finally, we illustrate the application of the methodology through the analysis of the performance of a real-life portfolio under scenarios from a recent economic research report as well as regulatory scenarios.},
  citeulike-article-id = {13923413},
  groups               = {Test_Scenario, Scenario_Risk, Scenario_SubsetFactor, StressTest_ExpertView},
  howpublished         = {Available at https://www.fields.utoronto.ca/video-archive/2015/11/12-5360},
  organization         = {Fields Institute},
  owner                = {zkgst0c},
  posted-at            = {2016-02-01 18:15:37},
  timestamp            = {2020-02-29 19:22},
}

@Article{Rosen-Saunders-2015,
  author               = {Rosen, Dan and Saunders, David},
  date                 = {2015-12},
  journaltitle         = {SSRN e-Print},
  title                = {Regress Under Stress: A Simple Least-Squares Method for Integrating Economic Scenarios with Risk Simulations},
  url                  = {https://ssrn.com/abstract=2699474},
  abstract             = {Scenarios are the language of Risk. While scenario analysis and stress testing have been an explicit part of risk management methodologies and systems for over two decades, the typical scenario and stress testing tools haven't evolved much and are still generally quite static and largely subjective. In this paper, we present a simple and powerful approach to create meaningful stress scenarios for risk management and investment analysis of multi-asset portfolios, which effectively combines economic forecasts and expert views with portfolio simulation methods.

Expert scenarios are typically described in terms of a small number of key economic variables or factors. However, when applied to a portfolio, they are incomplete they generally do not describe what occurs to all relevant market risk factors that affect the portfolio. We need to understand how these market risk factors behave, conditional on the outcome of the economic factors. The key insight to our approach is that the conditional expectation, or more generally the full conditional distribution of all the factors, and of the portfolio P and L, can be estimated directly from a pre-computed simulation using Least Squares Regressions (LSR). All the conditional scenario analytics can be derived from the regression results.

We refer to this approach as Least Squares Stress Testing (LSST). LSST is a simulation-based conditional scenario generation method that offers many advantages over more traditional analytical methods. Simulation techniques are simple, flexible, and provide very transparent results, which are auditable and easy to explain. LSST can be applied to a large number of risk factors, which can follow completely general joint stochastic processes, with fat-tails, non-parametric and general codependence structures, autocorrelation, etc. LSST further produces explicit risk factor P and L contributions.

We illustrate the application of the methodology through the analysis of the performance of a real-life portfolio under recent economic research reports.},
  citeulike-article-id = {13922109},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2699474},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2699474code2025999.pdf?abstractid=2699474 and mirid=1},
  day                  = {8},
  groups               = {Scenario_Risk, StressTest_ExpertView},
  howpublished         = {Available at SSRN: ssrn.com/abstract=2699474},
  owner                = {cristi},
  posted-at            = {2016-01-30 08:07:48},
  timestamp            = {2020-02-29 19:22},
}

@Article{Rosen-Saunders-2016,
  author               = {Rosen, Dan and Saunders, David},
  date                 = {2016-10},
  journaltitle         = {Journal of Risk Management in Financial Institutions},
  title                = {Regress under stress: A simple least-squares method for integrating economic scenarios with risk simulations},
  issn                 = {1752-8887},
  pages                = {391--412},
  url                  = {https://www.ingentaconnect.com/content/hsp/jrmfi/2016/00000009/00000004/art00009},
  abstract             = {We present a simple and powerful approach to create meaningful stress scenarios for risk management and investment analysis of multi-asset portfolios, which effectively combines economic forecasts and 'expert' views with portfolio simulation methods. Expert scenarios are typically described in terms of a small number of key economic variables or factors. However, when applied to a portfolio, they are incomplete - they generally do not describe what occurs to all relevant market risk factors that affect the portfolio. We need to understand how these market risk factors behave, conditional on the outcome of the economic factors. The key insight to our approach is that the conditional expectation, and more generally the full conditional distribution of all the factors, and of the portfolio profit and loss (P and L), can be estimated directly from a pre-computed simulation using least squares regression. We refer to this approach as least squares stress testing (LSST). LSST is a simulation-based conditional scenario generation method that offers many advantages over more traditional analytical methods. Simulation techniques are simple, flexible and provide very transparent results, which are auditable and easy to explain. LSST can be applied to both market and credit risk stress testing with a large number of risk factors, which can follow completely general stochastic processes, with fat-tails, non-parametric and general co-dependence structures, autocorrelation, etc. LSST further produces explicit risk factor P and L contributions. We demonstrate the methodology in detail with the practical example of a multi-asset investment portfolio and economic scenarios from an industry report.},
  citeulike-article-id = {14231323},
  citeulike-linkout-0  = {http://www.ingentaconnect.com/content/hsp/jrmfi/2016/00000009/00000004/art00009},
  groups               = {Scenario_Risk, Scenario_SubsetFactor, StressTest_ExpertView, Regression_Estim, Risk_Stress},
  owner                = {cristi},
  posted-at            = {2016-12-21 15:25:24},
  publisher            = {Henry Stewart Publications},
  timestamp            = {2020-02-29 19:22},
}

@Article{Rubasheuski-et-al-2014,
  author               = {Rubasheuski, Uladzimir and Oppen, Johan and Woodruff, David L.},
  date                 = {2014-07},
  journaltitle         = {Operations Research Letters},
  title                = {Multi-stage scenario generation by the combined moment matching and scenario reduction method},
  doi                  = {10.1016/j.orl.2014.06.006},
  issn                 = {0167-6377},
  number               = {5},
  pages                = {374--377},
  volume               = {42},
  abstract             = {We describe an opportunity to speed up multi-stage scenario generation and reduction using a combination of two well-known methods: the moment matching method (Hyland and Wallace, 2001) and the method for scenario reduction to approximately minimize a metric (Heitsch and Romish, 2009).

Our suggestion is to combine them rather than using them in serial by making use of a stage-wise approximation to the moment matching algorithm. Computational results show that combining the methods can bring significant benefits.},
  citeulike-article-id = {13931104},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.orl.2014.06.006},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:50:55},
  timestamp            = {2020-02-29 19:22},
}

@Article{Sankary-Ostfeld-2018,
  author         = {Sankary, Nathan and Ostfeld, Avi},
  date           = {2018-04},
  journaltitle   = {Water resources research},
  title          = {Stochastic Scenario Evaluation in Evolutionary Algorithms Used for Robust Scenario-Based Optimization},
  doi            = {10.1002/{2017WR022068}},
  issn           = {0043-1397},
  number         = {4},
  pages          = {2813--2833},
  url            = {http://doi.wiley.com/10.1002/{2017WR022068}},
  urldate        = {2019-10-12},
  volume         = {54},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Schneider-et-al-2019,
  author         = {Schneider, Paul Georg},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {A theory of scenario generation},
  doi            = {10.2139/ssrn.3358388},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3358388},
  urldate        = {2019-04-21},
  abstract       = {We show how distributions can be reduced to low-dimensional scenario trees. Applied to intertemporal distributions, the scenarios and their probabilities become time-varying factors. From S\&P 500 options, two or three time-varying scenarios suffice to forecast returns, implied variance or skewness on par, or better, than extant multivariate stochastic volatility jump-diffusion models, while reducing the computational effort to fractions of a second.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@MastersThesis{Seeve-2018,
  author         = {Seeve, Teemu},
  date           = {2018},
  institution    = {Aalto University},
  title          = {A Structured Method for Identifying and Visualizing Scenarios},
  type           = {THESIS.MASTER},
  url            = {https://aaltodoc.aalto.fi/handle/123456789/32472},
  abstract       = {To retain their competitive edge, companies and other organizations must develop methods to increase their understanding of changes in the future operational environment. These changes can be characterized with a set of scenarios, which provide plausible depictions of the future. Scenarios can be modelled as combinations of levels of uncertainty factors describing, e.g., alternative political or technological developments. However, the number of possible scenarios grows exponentially in the number of these factors. To date, there are few systematic methods to support the selection of a small set of plausible but mutually dissimilar scenarios from an exponentially large set of candidate scenarios. In this thesis, we develop a method for identifying a set of internally consistent and sufficiently dissimilar scenarios. The method filters an exponentially large set of candidate scenarios to a smaller set of most plausible scenarios, as assessed by the consistencies of the pairs of uncertainty factor levels in the candidate scenarios. By applying Multiple Correspondence Analysis to this set, the most consistent scenarios are visualized by a Scenario Map, from which a set mutually dissimilar consistent scenarios can be selected. As a part of this thesis, an interactive software tool was developed, which implements the scenario identi cation and visualization method. This thesis also presents a case study in which this tool was used to identify a set of plausible futures for a Finnish National Emergency Supply Organization to support their strategic decision making. Our method provides mathematically sound means for building scenarios efficiently based on numerous uncertainty factors. By guiding the building process through effective visualizations, the method leaves room for explorative thinking. Moreover, advancing in transparent and accessible steps, the method fosters trust in the developed scenarios. Based on the positive feedback from the case study, the method can provide valuable support in other scenario exercises as well.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Book{Sharpe-2019,
  author         = {Sharpe, William},
  date           = {2019},
  title          = {Retirement Income Analysis with scenario matrices},
  publisher      = {Stanford University},
  url            = {https://web.stanford.edu/\~wfsharpe/{RISMAT}/{RIAbook}.pdf},
  urldate        = {2019-04-24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Shumadine-2005,
  author               = {Shumadine, Anne},
  date                 = {2005-01},
  journaltitle         = {The Journal of Wealth Management},
  title                = {Anticipating the Future: Scenario Planning as an Investment Tool},
  doi                  = {10.3905/jwm.2005.598424},
  issn                 = {1520-4154},
  number               = {3},
  pages                = {77--80},
  volume               = {8},
  abstract             = {Starting with the observation that unexpected events offer opportunities for investment gain-and loss-the author suggests a scenario-planning approach that encourages investors to consider the unexpected. When incorporated as part of the investment process, scenario planning increases the likelihood that decisions made today will continue to be valid for the future. This article thus gives an overview of scenario planning and its possible application to the process of investing. The author first provides an example of possible scenarios out in the future and then, having defined scenario planning, discusses how that process would cause one to envisage a portfolio differently. Having suggested four possible guidelines to allow users to benefit from the approach, the author concludes that scenario planning is a powerful tool for thinking about the future as it enforces a long-term view and changes the focus of discussion.},
  citeulike-article-id = {14343833},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jwm.2005.598424},
  groups               = {PortfOptim_Scenario, Scenario_Portfolio},
  posted-at            = {2017-04-24 11:07:07},
  timestamp            = {2020-02-29 19:22},
}

@Article{Siew-et-al-2015a,
  author               = {Siew, Lam W. and Hj and Ismail, Hamizun B.},
  date                 = {2015-05},
  journaltitle         = {Advanced Science Letters},
  title                = {The Impact of Different Economic Scenarios Towards Portfolio Selection in Enhanced Index Tracking Problem},
  doi                  = {10.1166/asl.2015.6001},
  issn                 = {1936-6612},
  pages                = {1285--1288},
  abstract             = {Enhanced index tracking is a portfolio management which aims to track and outperform the benchmark stock market index without purchasing all stocks from the benchmark index components. Enhanced index tracking is a portfolio selection problem which can be represented by an optimization model. The objective of this paper is to study the impact of three different economic scenarios towards portfolio selection and portfolio performance in enhanced index tracking problem. The economic scenarios are divided into three sub-periods representing the growth period in the economy, financial crisis and the recovery period. In this study, the optimization model with sum weighted approach is applied in constructing the optimal portfolio. This paper also studies the consistency of the optimal portfolios to outperform the benchmark index for the three economic scenarios. The data consists of weekly price of 24 components stocks in Malaysia main market index which is Kuala Lumpur Composite Index from January 1994 until June 2008. The results of this study show different optimal portfolio selections and performances for the three economic scenarios. Besides that, the optimal portfolios are able to generate higher mean return than the benchmark index return with only selecting 10 percent from the benchmark index components for the three economic scenarios. Therefore, the optimization model with sum weighted approach is appropriate for the investors in Malaysia. The significance of this study is to identify the optimal portfolios for three different economic scenarios to track and outperform Malaysia market index consistently. In addition, this study concludes that the performance of the optimal portfolio is the highest during economic recovery in Malaysia.},
  citeulike-article-id = {14320261},
  citeulike-linkout-0  = {http://dx.doi.org/10.1166/asl.2015.6001},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/asp/asl/2015/00000021/00000005/art00050},
  groups               = {Scenario generation, Scenario_Portfolio},
  posted-at            = {2017-03-26 18:00:20},
  publisher            = {American Scientific Publishers},
  timestamp            = {2020-02-29 19:22},
}

@Book{Sironi-2015,
  author    = {Paolo Sironi},
  date      = {2015},
  title     = {Modern Portfolio Management: from Markowitz to Probabilistic Scenario Optimisation},
  publisher = {Risk Books},
  url       = {https://www.risk.net/modern-portfolio-management},
  abstract  = {Modern Portfolio Management provides a methodology for portfolio choice based upon modern risk management techniques and a clearer definition of the investment risk/return profile to feature goal-based investing and probabilistic scenario optimisation.

The financial markets have undergone a period of distress that has strained the trusted relationship between investors and financial advisors; new regulation has been forged to push for higher levels of transparency and risk-based communication as part of investment decision-making. This has ignited the quest for better portfolio optimisation techniques that can combine the added-value asymmetry of real products (as they strongly contributed to pre-crisis budgets) with the life-cycle requirements of investors, supported by intuitive graphical representation of seemingly complex mathematical relationships between real portfolios and products as required by regulation.

Upon reading Modern Portfolio Management, readers will understand the importance of simulating real securities (especially fixed income and structured products) during the making of optimal portfolios, as well as the importance of simulating financial investments over time to match in a transparent way actual goals and constraints instead of relying solely upon past performance or personal judgement.},
  groups    = {Scenario generation, PortfOptim_Scenario, PortfOptim_FullScale, Scenario_Market, Scenario_Risk, Scenario_Portfolio},
  owner     = {zkgst0c},
  timestamp = {2020-02-29 19:22},
}

@TechReport{Six-Wiedemann-2014,
  author               = {Six, Timo and Wiedemann, Arnd},
  date                 = {2014},
  institution          = {Union Investment},
  title                = {Scenario-based asset allocation},
  abstract             = {It shows how scenario techniques are used in asset allocation and how an investor's personal attitude to risk can be factored into the optimisation process. Extreme fat-tail events can also be incorporated into the analysis.},
  citeulike-article-id = {13931105},
  groups               = {Scenario generation},
  howpublished         = {Available at http://www.union-investment.com/data/docme/news/Risk-management-study-2013/E112Scenariobasedassetallocationpdf/document/E1.12Scenariobasedassetallocation.pdf},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:56:52},
  timestamp            = {2020-02-29 19:22},
}

@Article{Spaniol-Rowland-2018,
  author         = {Spaniol, Matthew J. and Rowland, Nicholas J.},
  date           = {2018},
  journaltitle   = {Futures},
  title          = {The Scenario Planning Paradox},
  doi            = {10.1016/j.futures.2017.09.006},
  issn           = {0016-3287},
  pages          = {33--43},
  url            = {http://linkinghub.elsevier.com/retrieve/pii/S0016328717302537},
  urldate        = {2019-04-21},
  volume         = {95},
  abstract       = {For more than a decade, futures studies scholars have prefaced scholarly contributions by repeating the claim that there is insufficient theory to support chaotic scenario methodology. The strategy is formulaic, and the net effect is a curious one, which the authors refer to as the scenario planning paradox. Contributing fresh theory supposedly attends to the state of theory, while contributing new typologies purportedly helps bring order to methodological chaos. Repeated over time, the contribution strategy breaks down. Effort to resolve the theoretical and methodological issue, which motivates re-statement of the claim in the first place, ultimately fails. In actuality, the field is distanced from its purported goals. The state of theory encourages scholars to adopt theory that is not necessarily tethered to a common core, which does not contribute to a shared, foundational theoretical perspective in futures studies. Perceived chaos gives way to typologies, which, as they mount, contribute to the chaos they were meant to resolve. The end result, intended by no one, is that theory remains dismal and methods remain chaotic. This direction for the field is indefensible and untenable; either the field accepts this claim as a statement of truth, for which the solution is substantially enhanced empiricism, or rejects the claim and re-interprets the bounty produced by said claim to be a kind of richness in theory and method rather than the implicit paucity, poverty, and imperfection that they oft signify to the field now.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Staino-Russo-2015,
  author               = {Staino, Alessandro and Russo, Emilio},
  date                 = {2015-10},
  journaltitle         = {European Journal of Operational Research},
  title                = {A moment-matching method to generate arbitrage-free scenarios},
  doi                  = {10.1016/j.ejor.2015.04.045},
  issn                 = {0377-2217},
  number               = {2},
  pages                = {619--630},
  volume               = {246},
  abstract             = {The monomial method is applied to build a scenario generator for financial assets. The scenario generator belongs to the class of the moment-matching methods. The marginal moments up to the fourth order and correlations are matched. The generated scenarios satisfy the no-arbitrage condition. Accuracy and efficiency analysis is carried out when solving financial problems. We propose a new moment-matching method to build scenario trees that rule out arbitrage opportunities when describing the dynamics of financial assets. The proposed scenario generator is based on the monomial method, a technique to solve systems of algebraic equations. Extensive numerical experiments show the accuracy and efficiency of the proposed moment-matching method when solving financial problems in complete and incomplete markets.},
  citeulike-article-id = {13989069},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2015.04.045},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-03-27 17:43:36},
  timestamp            = {2020-02-29 19:22},
}

@InProceedings{Steehouwer-2014,
  author               = {Steehouwer, Hens},
  booktitle            = {CFA Society Switzerland Series},
  date                 = {2014},
  title                = {Ex-ante risk management with scenarios},
  url                  = {https://www.youtube.com/watch?v=_J4vrogFAmQ},
  abstract             = {Ex-ante risk management with help of scenarios including important information such as the impact of the business cycle is an innovative methodology used by sophisticated managers gaining worldwide popularity. Advanced stochastic scenario analysis supports financial decision-making when dealing with uncertainty through accounting for structural changes impacting trend values as well as short term events influencing the market momentum. The extent to which investor objectives will be achieved without violating risk limits is influenced by the path that financial and economic risk drivers will follow. A frequency domain based approach differs from traditional simulation approaches through the creation of plausible and relevant scenarios.

The Dynamic Scenario Generator (DSG) employs a unique combination of statistical and econometric techniques. The methodology is a mixture of frequency domain techniques, dynamic factor models and special approaches for dealing with non-normal distribution characteristics such as skewness and fat tails. The main purpose of the central frequency domain methodology is to describe all empirical features ( stylized facts ) of the time series behavior of financial and economic variables simultaneously, rather than focusing on only one or a few aspects in isolation.},
  citeulike-article-id = {13924423},
  howpublished         = {Available at https://www.cfasociety.org/switzerland/Lists/Events0Calendar/Attachments/484/CFAScenariosJune2014FINAL.pdf},
  journaltitle         = {SSRN e-Print},
  owner                = {zkgst0c},
  posted-at            = {2016-02-03 04:24:02},
  timestamp            = {2020-02-29 19:22},
}

@Article{Steehouwer-Slater-2010,
  author               = {Steehouwer, Hens and Slater, Andrew},
  date                 = {2010},
  journaltitle         = {SSRN e-Print},
  title                = {Macroeconomic Scenarios: A Frequency Domain Approach},
  citeulike-article-id = {13931103},
  groups               = {Scenario generation},
  howpublished         = {Available at www.actuaries.org.uk/sites/default/files/documents/pdf/slater0.pdf},
  owner                = {cristi},
  posted-at            = {2016-02-11 07:50:02},
  timestamp            = {2020-02-29 19:22},
}

@Book{Sullivan-Lazenby-2005,
  author               = {Sullivan, Patrick J. and Lazenby, David L.},
  date                 = {2005-07},
  title                = {Scenario Selling: Technology and the Future of Professional Selling},
  publisher            = {Trafford Publishing},
  url                  = {https://www.amazon.com/Scenario-Selling-Technology-Future-Professional/dp/1412036208},
  abstract             = {The development and widespread use of Digital-Age technologies has resulted in and continues to introduce significant changes in the way people live, work, learn, buy, and sell. Will technology eliminate the need for salespeople?

Within these pages you'll discover why today's Digital-Age technologies may well replace many salespeople; and be introduced to perhaps the only sales method and toolset that will make salespeople irreplaceable: ScenarioSellingSM.

ScenarioSelling isn't a new sales technique...it's a new sales process. It represents the first major change in sales process since consultative selling was introduced over 40 years ago. ScenarioSelling provides the logic and framework for a whole new way of selling - a model that will surpass the current paradigm of consultative selling in productivity, personal touch, and professionalism.

ScenarioSelling explains the knowledge, skills, and tools required for just-in-time (fast) professional selling. It results in a significant reduction in the time required for complex decisions and sales, which can be reduced to hours rather than weeks or months while dramatically improving the customer's sales or service experience.},
  citeulike-article-id = {13937254},
  citeulike-linkout-0  = {http://www.amazon.ca/exec/obidos/redirect?tag=citeulike09-20 and amp;path=ASIN/B000PY4LDI},
  citeulike-linkout-1  = {http://www.amazon.de/exec/obidos/redirect?tag=citeulike01-21 and amp;path=ASIN/B000PY4LDI},
  citeulike-linkout-2  = {http://www.amazon.fr/exec/obidos/redirect?tag=citeulike06-21 and amp;path=ASIN/B000PY4LDI},
  citeulike-linkout-3  = {http://www.amazon.jp/exec/obidos/ASIN/B000PY4LDI},
  citeulike-linkout-4  = {http://www.amazon.co.uk/exec/obidos/ASIN/B000PY4LDI/citeulike00-21},
  citeulike-linkout-5  = {http://www.amazon.com/exec/obidos/redirect?tag=citeulike07-20 and path=ASIN/B000PY4LDI},
  citeulike-linkout-6  = {http://www.worldcat.org/isbn/9781412227407},
  citeulike-linkout-7  = {http://books.google.com/books?vid=ISBN9781412227407},
  citeulike-linkout-8  = {http://www.amazon.com/gp/search?keywords=9781412227407 and index=books and linkCode=qs},
  citeulike-linkout-9  = {http://www.librarything.com/isbn/9781412227407},
  day                  = {15},
  howpublished         = {Kindle Edition},
  owner                = {zkgst0c},
  posted-at            = {2016-02-20 19:43:06},
  timestamp            = {2020-02-29 19:22},
}

@Article{Sun-2019a,
  author         = {Sun, Fei},
  date           = {2019-04-16},
  journaltitle   = {arXiv e-Print},
  title          = {Loss-based risk statistics with scenario analysis},
  url            = {https://arxiv.org/abs/1904.11032},
  urldate        = {2019-04-27},
  abstract       = {Since the investors and regulators pay more attention to losses rather than gains, we will study a new class of risk statistics, named loss-based risk statistics in this paper. This new class of risk statistics can be considered as a kind of risk extension of risk statistics introduced by Kou, Peng and Heyde (2013), and also data-based versions of loss-based risk measures introduced by Cont et al. (2013) and Sun et al. (2018).},
  day            = {16},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Tanaka-2017,
  author               = {Tanaka, Katsuhiro},
  date                 = {2017},
  journaltitle         = {The Journal of Risk Model Validation},
  title                = {Forecasting scenarios from the perspective of a reverse stress test using second-order cone programming},
  doi                  = {10.21314/jrmv.2017.166},
  issn                 = {1753-9579},
  number               = {2},
  pages                = {1-21},
  volume               = {11},
  abstract             = {This paper proposes a model for forecasting scenarios from the perspective of a reverse stress test using interest rate (JGB10Y yield), equity (Nikkei 225) and foreign exchange (USD/U) data. The model consists of a constraint with error terms of dynamic conditional correlation-generalized autoregressive conditional heteroscedasticity (DCC-GARCH) for expressing risk factors (RFs) located in an acceptable range, where the acceptable range is determined by the Mahalanobis distance, which consists of error terms of DCC-GARCH and the correlation between one RF and another RF; maximization of the objection function, which is the loss of market portfolio (ie, minimization of the difference of a market portfolio).

I also show that forecasting scenarios identified by this model are valid in terms of expressing very stressful data, which documents that some financial institutions may be in default, and that there is a mostly distributed multivariate normal distribution; and this model can be solved by formulating second-order cone programming, which is standard in the field of mathematical optimization programming.},
  citeulike-article-id = {14386371},
  citeulike-linkout-0  = {http://dx.doi.org/10.21314/jrmv.2017.166},
  groups               = {Scenario generation, Test_Scenario, Scenario_Risk, Scenario_Portfolio},
  posted-at            = {2017-07-02 23:39:22},
  timestamp            = {2020-02-29 19:22},
}

@Article{Tanaka-2019,
  author         = {Tanaka, Katsuhiro},
  date           = {2019-09},
  journaltitle   = {Journal of Financial Engineering},
  title          = {Forecasting plausible scenarios and losses in interest rate targeting using mathematical optimization},
  doi            = {10.1142/S2424786319500257},
  issn           = {2424-7863},
  number         = {03},
  pages          = {1950025},
  urldate        = {2020-01-13},
  volume         = {06},
  abstract       = {This study proposes a mathematical optimization model for simultaneously forecasting plausible market scenarios and portfolio losses. Interest rates, volatilities and correlation coefficients can be modeled by the DCC-GARCH. A constraint condition is set by the Mahalanobis distance for deciding an acceptable range of change in interest rates. An objective function is set as a hypothetical market portfolio loss from delta, gamma and vega. The mathematical optimization model becomes a nonlinear programming problem for which it is difficult to find appropriate solutions. Therefore, the study introduces an original heuristic approach for preventing the signs of solutions from unintentionally becoming inverse. The study finds that, compared to a stressful scenario in Japan, the forecasting scenarios and losses are plausible.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Tavin-2018,
  author         = {Tavin, Bertrand},
  date           = {2018-11},
  journaltitle   = {European Journal of Operational Research},
  title          = {Measuring exposure to dependence risk with random Bernstein copula scenarios},
  doi            = {10.1016/j.ejor.2017.10.044},
  issn           = {0377-2217},
  number         = {3},
  pages          = {873--888},
  volume         = {270},
  abstract       = {This paper considers the problem of measuring the exposure to dependence risk carried by a portfolio with an arbitrary number of two-asset derivative contracts. We develop a worst-case risk measure computed over a set of dependence scenarios within a divergence restricted region. The set of dependence scenarios corresponds to Bernstein copulas obtained by simulating random doubly stochastic matrices. We then devise a method to compute hedging positions when a limited number of hedging instruments are available for trading. In an empirical study, we show how the proposed method can be used to reveal an exposure to dependence risk where usual sensitivity methods fail to reveal it. We also illustrate the ability of the proposed method to generate parsimonious hedging strategies in order to reduce the exposure to dependence risk of a given portfolio.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_Risk, Scenario_Portfolio},
  timestamp      = {2020-02-29 19:22},
}

@Article{Theiakos-et-al-2015,
  author         = {Theiakos, Alexios and Tas, Jurgen and van der Lem, Han and Kandhai, Drona},
  date           = {2015-02},
  journaltitle   = {The Journal of Risk},
  title          = {Ultra-fast scenario analysis of mortgage prepayment risk},
  doi            = {10.21314/{JOR}.2015.323},
  issn           = {1465-1211},
  number         = {3},
  pages          = {19--33},
  url            = {http://www.risk.net/journal-of-risk/technical-paper/2394182/online-first-ultra-fast-scenario-analysis-of-mortgage-prepayment-risk},
  urldate        = {2019-05-30},
  volume         = {17},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Tourki-et-al-2013,
  author               = {Tourki, Yousra and Keisler, Jeffrey and Linkov, Igor},
  date                 = {2013},
  journaltitle         = {Environment Systems and Decisions},
  title                = {Scenario analysis: a review of methods and applications for engineering and environmental systems},
  doi                  = {10.1007/s10669-013-9437-6},
  number               = {1},
  pages                = {3--20},
  volume               = {33},
  abstract             = {Changing environment, uncertain economic conditions, and socio-political unrest have renewed interest in scenario analysis, both from theoretical and applied points of view. Nevertheless, neither the processes for scenario analysis (SA) nor evaluation criteria and metrics have been regularized. In this paper, SA-reported applications and implementation methodology are discussed in the context of an extensive literature review covering papers published between 2000 and 2010. Over 340 papers were identified through a series of queries in the web of science database. The papers were classified based on the North American Industrial Classification System and SA application goals (environmental, business, and social). SA methodology used in each paper was assessed based on four main criteria: coverage, consistency, uncertainty assessment, and efficiency. We find a significant increase in SA applications, especially in the environmental field. Theoretical developments in the field represent a small fraction of published studies and do not increase in time. The methods used to develop different scenarios vary widely across the academic literature and applications reviewed. Similarly, the methods and data used to characterize the scenarios and develop response strategies are extremely diverse and are limited by factors such as computational tractability and available time and resources. Based on this review, we recommend a regular process for scenario analysis that includes the steps of analysis, scenario definition, and evaluation.},
  citeulike-article-id = {14087942},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10669-013-9437-6},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10669-013-9437-6},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:25:30},
  publisher            = {Springer US},
  timestamp            = {2020-02-29 19:22},
}

@Article{Tugwell-2011,
  author               = {Tugwell, Jem},
  date                 = {2011-05},
  journaltitle         = {Journal of Asset Management},
  title                = {Skill or luck? The role of strategies and scenario analysis as a competitive differentiator for fund management firms},
  doi                  = {10.1057/jam.2011.10},
  issn                 = {1470-8272},
  number               = {4},
  pages                = {281--291},
  volume               = {12},
  abstract             = {This article argues that plausible and provable market differentiation for a fund management firm can only be delivered via a strategy and scenario-centric investment approach. By facilitating correct and repeatable decisions across multiple funds, firms can demonstrate a transparent and efficient investment process, implemented via skill centres that add positive returns. The emphasis is on differentiating good luck from skill, proving that the underlying decisions contribute to the ultimate performance.

The article challenges the conventional view of traditional performance attribution approaches, and illustrates the options for implementing a fundamentally more logical, measurable process.},
  citeulike-article-id = {9741217},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2011.10},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2011/00000012/00000004/art00005},
  day                  = {19},
  groups               = {Manager selection, Measuring Skill, Scenario_Market},
  owner                = {cristi},
  posted-at            = {2016-03-06 05:25:48},
  publisher            = {Palgrave Macmillan},
  timestamp            = {2020-02-29 19:22},
}

@Article{Veeramachaneni-2012,
  author               = {Veeramachaneni, Sriharsha},
  date                 = {2012-11},
  journaltitle         = {arXiv e-Print},
  title                = {Time-series Scenario Forecasting},
  eprint               = {1211.3010},
  eprinttype           = {arXiv},
  url                  = {https://arxiv.org/abs/1211.3010},
  abstract             = {Many applications require the ability to judge uncertainty of time-series forecasts. Uncertainty is often specified as point-wise error bars around a mean or median forecast. Due to temporal dependencies, such a method obscures some information. We would ideally have a way to query the posterior probability of the entire time-series given the predictive variables, or at a minimum, be able to draw samples from this distribution. We use a Bayesian dictionary learning algorithm to statistically generate an ensemble of forecasts. We show that the algorithm performs as well as a physics-based ensemble method for temperature forecasts for Houston. We conclude that the method shows promise for scenario forecasting where physics-based methods are absent.},
  citeulike-article-id = {13987664},
  citeulike-linkout-0  = {http://arxiv.org/abs/1211.3010},
  citeulike-linkout-1  = {http://arxiv.org/pdf/1211.3010},
  day                  = {13},
  groups               = {Scenario generation, FrcstQWIM_TimeSrs},
  owner                = {cristi},
  posted-at            = {2016-03-24 20:36:09},
  timestamp            = {2020-02-29 19:22},
}

@Article{Vermeulen-et-al-2019,
  author         = {Vermeulen, Robert and Schets, Edo and Lohuis, Melanie and Kolbl, Barbara and Jansen, David-Jan and Heeringa, Willem},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {The heat is on: A framework for measuring financial stress under disruptive energy transition scenarios},
  doi            = {10.2139/ssrn.3346466},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3346466},
  urldate        = {2019-03-07},
  abstract       = {This paper presents a comprehensive framework for analyzing financial stress under scenarios with a disruptive transition to a low-carbon economy. This stress testing framework is designed to be readily applied by macroprudential supervisors or financial institutions. First, we construct stress scenarios using two dimensions: climate policy and energy technology. Then, we rely on various modeling approaches to derive macroeconomic and industry-specific implications. These approaches include a novel methodology for capturing industry-specific transition risks. Third, we disaggregate EUR 2.3 trillion in assets of more than 80 Dutch financial institutions by industry. Finally, our calculations show that financial losses can be sizeable, as portfolio values can decline by up to 11\%. These outcomes suggest that climate-transition risks warrant close and timely attention from a financial stability perspective.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Vilkkumaa-et-al-2018,
  author               = {Vilkkumaa, Eeva and Liesio, Juuso and Salo, Ahti and Ilmola-Sheppard, Leena},
  date                 = {2018-09},
  journaltitle         = {European Journal of Operational Research},
  title                = {Scenario-based portfolio model for building robust and proactive strategies},
  doi                  = {10.1016/j.ejor.2017.09.012},
  issn                 = {0377-2217},
  number               = {1},
  pages                = {205-220},
  volume               = {266},
  abstract             = {In order to address major changes in the operational environment, companies can (i) define scenarios that characterize different alternatives for this environment, (ii) assign probabilities to these scenarios, (iii) evaluate the performance of strategic actions across the scenarios, and (iv) choose those actions that are expected to perform best. In this paper, we develop a portfolio model to support the selection of such strategic actions when the information about scenario probabilities is possibly incomplete and may depend on the selected actions. This model helps build a strategy that is robust in that it performs relatively well in view of all available probability information, and proactive in that it can help steer the future as reflected by the scenarios toward the desired direction. We also report a case study in which the model helped a group of Nordic, globally operating steel and engineering companies build a platform ecosystem strategy that accounts for uncertainties related to markets, politics, and technological development.},
  citeulike-article-id = {14500732},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2017.09.012},
  groups               = {PortfOptim_Scenario, Test_Scenario, Scenario_Market, Scenario_Portfolio, Scenario_Best},
  posted-at            = {2017-12-11 08:34:46},
  timestamp            = {2020-02-29 19:22},
}

@Article{Vinod-2012,
  author               = {Vinod, Hrishikesh D.},
  date                 = {2012},
  journaltitle         = {SSRN e-Print},
  title                = {Constructing Scenarios of Time Heterogeneous Series for Stress Testing},
  doi                  = {10.2139/ssrn.1987879},
  issn                 = {1556-5068},
  abstract             = {Heterogeneous global trends in asset prices and savings affect the macro economy. Our challenge is to use limited data to make inference regarding underlying causes. In general, government and business decision makers, FDIC type regulators and risk professionals need quantitative tools to help generate plausible scenarios of state-dependent and time heterogeneous nonstationary time series. We suggest using maximum entropy type bootstraps, recently implemented in an R software package called "meboot."A new modification of meboot divides the data series into blocks and can randomly modify the (down, at or up) direction of series within each block. Our large number of resamples are then available for construction of scenarios for probabilistic stress testing. A simulation study evaluates the performance of our proposal in the context of many types of time-heterogeneity showing that it behaves better than moving block bootstraps. We apply meboot tools to stress test inference regarding Granger-causality between asset prices and world savings rates, and also to the 'Value at Risk' used in Finance.},
  citeulike-article-id = {14219162},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.1987879},
  groups               = {Scenario generation, Test_Scenario, Scenario_Risk},
  owner                = {cristi},
  posted-at            = {2016-12-02 22:31:02},
  timestamp            = {2020-02-29 19:22},
}

@PhdThesis{Wang-2016a,
  author               = {Wang, Xinghao},
  date                 = {2016},
  institution          = {University of Waterloo},
  title                = {Conditional Scenario Generation with a GVAR Model},
  abstract             = {The stress-testing method formed an integral part of the practice of risk management. However, the underlying models for scenarios generation have not been much studied so far. In past practice, the users typically did not model risk factors for portfolios of moderate size endogenously due to the presence of "curse of dimensionality" problem. Moreover, it is almost impossible to impose the expert views for a future outcome of macroeconomy on the scenario generator without making ad-hoc adjustments.

In this thesis we propose a GVAR-based framework which allows an efficient simulation of risk factors for a complex multi-currency portfolio of various classes of assets conditioning on economic scenarios. Given reasonable sets of economic forecasts, the GVAR model anticipates the trend and codependency of the future path of portfolio risk factors and supports the production of meaningful results from risk analytics.},
  citeulike-article-id = {14387580},
  groups               = {Scenario generation, StressTest_ExpertView},
  posted-at            = {2017-07-03 21:23:11},
  timestamp            = {2020-02-29 19:22},
}

@Article{Wang-et-al-2019b,
  author         = {Wang, Yanli and Li, Huajiao and Guan, Jianhe and Liu, Nairong},
  date           = {2019-02},
  journaltitle   = {Physica A: Statistical Mechanics and its Applications},
  title          = {Similarities between stock price correlation networks and co-main product networks: Threshold scenarios},
  doi            = {10.1016/j.physa.2018.09.154},
  issn           = {0378-4371},
  pages          = {66--77},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0378437118312743},
  urldate        = {2019-12-03},
  volume         = {516},
  abstract       = {Abstract Because of the high yields and high risks associated with the stock market, investors can hold diversified portfolios with low relativity of stocks to reduce unsystematic risk. The current literature analyzes single factors affecting the relativity of stocks, but in this paper, we analyze the correlations between different factors to provide multiple perspectives of and about investment portfolios. This study analyzes the relationships between the similarities of the main products of listed companies and the varying degrees of correlations of stock price by examining different threshold scenarios of the energy industry between 2012 and 2016 and then constructing stock price correlation threshold networks and co-main product networks to analyze the similarities in their structures. The results indicate that two factors are significantly correlated in 97.5\% of the scenarios and that these factors are the most strongly correlated when the threshold is between 0.5 and 0.7. The two networks exhibit a high degree of similarity in degree, weighted degree and community division. Main product similarity, used as supplementary information for stock relativity research, plays a role similar to stock price correlations in certain scenarios. Furthermore, compared with stock price correlations, the similarity of main products is simpler and more intuitive. This paper proposes a new method to study stock relativity based on different threshold scenarios; thus, it could serve as a reference for investors when developing portfolio strategies from multiple perspectives.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Wong-2013,
  author               = {Wong, Man H.},
  date                 = {2013-06},
  journaltitle         = {European Journal of Operational Research},
  title                = {Investment models based on clustered scenario trees},
  doi                  = {10.1016/j.ejor.2012.11.051},
  issn                 = {0377-2217},
  number               = {2},
  pages                = {314--324},
  volume               = {227},
  abstract             = {Stochastic programming is widely applied in financial decision problems. In particular, when we need to carry out the actual calculations for portfolio selection problems, we have to assign a value for each expected return and the associated conditional probability in advance. These estimated random parameters often rely on a scenario tree representing the distribution of the underlying asset returns. One of the drawbacks is that the estimated parameters may be deviated from the actual ones. Therefore, robustness is considered so as to cope with the issue of parameter inaccuracy. In view of this, we propose a clustered scenario-tree approach, which accommodates the parameter inaccuracy problem in the context of a scenario tree. Proposed a new kind of scenario tree, called ? cluster tree ?. It accommodates the parameter inaccuracy in the context of a scenario tree. The idea is illustrated with portfolio selection problems. Three risk measures are considered: probability, downside risk and CVaR. OR techniques include fractional programming, interior point methods and SOCP.},
  citeulike-article-id = {13989083},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.ejor.2012.11.051},
  groups               = {Network_Invest, Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 18:42:57},
  timestamp            = {2020-02-29 19:22},
}

@Article{Xidonas-et-al-2017,
  author               = {Xidonas, Panos and Hassapis, Christis and Soulis, John and Samitas, Aristeidis},
  date                 = {2017-08},
  journaltitle         = {Economic Modelling},
  title                = {Robust minimum variance portfolio optimization modelling under scenario uncertainty},
  doi                  = {10.1016/j.econmod.2017.03.020},
  issn                 = {0264-9993},
  pages                = {60--71},
  volume               = {64},
  abstract             = {We develop a robust mixed-integer minimum variance portfolio optimization framework. The methodology's innovation relates to the effective parametric handling of the VCV matrix scenario uncertainty. Broad out-of-sample empirical testing with abundant historical market data is carried-out. We report consistent generation of stable out-of-sample returns, which are superior to those of the worst-case scenario. We provide strong evidence that the model assists in selective asset picking and systematic avoidance of excessive losses. Our purpose in this article is to develop a robust optimization model which minimizes portfolio variance for a finite set of covariance matrices scenarios. The proposed approach aims at the proper selection of portfolios, in a way that for every covariance matrix estimate included in the analysis, the calculated portfolio variance remains as close to the corresponding individual minimum value, as possible. To accomplish this, we formulate a mixed-integer non-linear program with quadratic constraints. With respect to practical underlying concerns, investment policy constraints regarding the portfolio structure are also taken into consideration. The validity of the proposed approach is verified through extensive out-of-sample empirical testing in the EuroStoxx 50, the S and P 100, the S\&P 500, as well as a well-diversified investment universe of ETFs. We report consistent generation of stable out-of-sample returns, which are in most cases superior to those of the worst-case scenario. Moreover, we provide strong evidence that the proposed robust model assists in selective asset picking and systematic avoidance of excessive losses.},
  citeulike-article-id = {14336730},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.econmod.2017.03.020},
  groups               = {PortfOptim_Scenario, Scenario_Portfolio},
  posted-at            = {2017-04-13 11:04:30},
  timestamp            = {2020-02-29 19:22},
}

@Article{Xu-et-al-2012,
  author               = {Xu, Daobao and Chen, Zhiping and Yang, Li},
  date                 = {2012-11},
  journaltitle         = {Journal of Computational and Applied Mathematics},
  title                = {Scenario tree generation approaches using K-means and LP moment matching methods},
  doi                  = {10.1016/j.cam.2012.05.020},
  issn                 = {0377-0427},
  number               = {17},
  pages                = {4561--4579},
  volume               = {236},
  abstract             = {We consider in this paper the efficient ways to generate multi-stage scenario trees. A general modified K-means clustering method is first presented to generate the scenario tree with a general structure. This method takes the time dependency of the simulated path into account. Based on the traditional and modified K-means analyses, the moment matching of multi-stage scenario trees is described as a linear programming (LP) problem. By simultaneously utilizing simulation, clustering, non-linear time series and moment matching skills, a sequential generation method and another new hybrid approach which can generate the whole multi-stage tree right off are proposed. The advantages of these new methods are: the vector autoregressive and multivariate generalized autoregressive conditional heteroscedasticity (VAR-MGARCH) model is adopted to properly reflect the inter-stage dependency and the time-varying volatilities of the data process, the LP-based moment matching technique ensures that the scenario tree generation problem can be solved more efficiently and the tree scale can be further controlled, and in the meanwhile, the statistical properties of the random data process are maintained properly. What is more important, our new LP methods can guarantee at least two branches are derived from each non-leaf node and thus overcome the drawback in relevant papers. We carry out a series of numerical experiments and apply the scenario tree generation methods to a portfolio management problem, which demonstrate the practicality, efficiency and advantages of our new approaches over other models or methods.},
  citeulike-article-id = {10835884},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.cam.2012.05.020},
  groups               = {Scenario generation},
  owner                = {cristi},
  posted-at            = {2016-07-01 11:58:52},
  timestamp            = {2020-02-29 19:22},
}

@Article{Xu-et-al-2017,
  author         = {Xu, Yuan and Shi, Libao and Ni, Yixin},
  date           = {2017-01-01},
  journaltitle   = {The Journal of Engineering},
  title          = {Deep-learning-based scenario generation strategy considering correlation between multiple wind farms},
  doi            = {10.1049/joe.2017.0722},
  issn           = {2051-3305},
  number         = {13},
  pages          = {2207--2210},
  urldate        = {2019-10-13},
  volume         = {2017},
  abstract       = {It is important to model the future scenarios of wind farm power output in enhancing capability of wind power accommodation and decreasing wind power curtailment. A scenario generation strategy considering the correlation between multiple wind farms is proposed. A convolutional neural network combined with quantile regression technique is introduced to achieve detailed quantiles of corresponding predicted wind power output, which can be regarded as the cumulative distribution function (CDF) by approximation. Marginal conditional probability density function (PDF) for each wind farm can be constructed from the CDF. A copula function-based method is used to form the joint PDF of multiple wind farm power outputs from the marginal PDF constructed before. By inversing the joint PDF, the required scenario set can be formed. In case studies, the proposed strategy is tested with two wind farms data, and the simulation results verify the effectiveness of the proposed strategy.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Yuen-Yang-2012,
  author               = {Yuen, FeiLung and Yang, Hailiang},
  date                 = {2012-12},
  journaltitle         = {Journal of Optimization Theory and Applications},
  title                = {Optimal Asset Allocation: A Worst Scenario Expectation Approach},
  doi                  = {10.1007/s10957-011-9972-6},
  issn                 = {0022-3239},
  number               = {3},
  pages                = {794--811},
  volume               = {153},
  abstract             = {Mean-variance criterion has long been the main stream approach in the optimal portfolio theory. The investors try to balance the risk and the return on their portfolio. In this paper, the deviation of the asset return from the investor's expectation in the worst scenario is used as the measure of risk for portfolio selection. One important advantage of this approach is that the investors can base on their own knowledge, information, and preference on various risks, in addition to the asset's volatility, to adjust their exposure to various risks. It also pinpoints one main concern of the investors when they invest, the amount they lose in the worst situation.},
  citeulike-article-id = {10130047},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10957-011-9972-6},
  citeulike-linkout-1  = {http://www.springerlink.com/content/l7t9081710662070},
  citeulike-linkout-2  = {http://link.springer.com/article/10.1007/s10957-011-9972-6},
  day                  = {7},
  groups               = {Scenario_Portfolio},
  owner                = {cristi},
  posted-at            = {2016-03-27 18:48:49},
  publisher            = {Springer US},
  timestamp            = {2020-02-29 19:22},
}

@Article{Yu-et-al-2014a,
  author               = {Yu, Philip L. H. and Li, W. K. and Ng, F. C.},
  date                 = {2014-01},
  journaltitle         = {The North American Journal of Economics and Finance},
  title                = {Formulating hypothetical scenarios in correlation stress testing via a Bayesian framework},
  doi                  = {10.1016/j.najef.2013.10.002},
  issn                 = {1062-9408},
  pages                = {17--33},
  volume               = {27},
  abstract             = {We model the correlation matrix under hypothetical scenarios in stress testing. Predict the values of other correlations given adjustments on some correlations. Our empirical example shows that empirical correlations are correlated. Including correlations among empirical correlations yields a more realistic result. Extend the posterior simulation algorithm to update two correlations simultaneously. Correlation stress testing refers to the correlation matrix adjustment to evaluate potential impact of the changes in correlations under financial crises. There are two categories, sensitivity tests and scenario tests. For a scenario test, the correlation matrix is adjusted to mimic the situation under an underlying stress event. It is only natural that when some correlations are altered, the other correlations (peripheral correlations) should vary as well. However, most existing methods ignore this potential change in peripheral correlations. In this paper, we propose a Bayesian correlation adjustment method to give a new correlation matrix for a scenario test based on the original correlation matrix and views on correlations such that peripheral correlations are altered according to the dependence structure of empirical correlations. The algorithm of posterior simulation is also extended so that two correlations can be updated in one Gibbs sampler step. This greatly enhances the rate of convergence. The proposed method is applied to an international stock portfolio dataset.},
  citeulike-article-id = {14335656},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.najef.2013.10.002},
  groups               = {Scenario generation, PortfOptim_Bayes, Test_Scenario, Scenario_Portfolio},
  posted-at            = {2017-04-11 15:25:28},
  timestamp            = {2020-02-29 19:22},
}

@Article{Zegklitz-Posik-2019,
  author         = {Zegklitz, Jan and Posik, Petr},
  date           = {2019-10},
  journaltitle   = {Applied soft computing},
  title          = {Symbolic regression in dynamic scenarios with gradually changing targets},
  doi            = {10.1016/j.asoc.2019.105621},
  issn           = {1568-4946},
  pages          = {105621},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S1568494619304016},
  urldate        = {2019-10-24},
  volume         = {83},
  abstract       = {Symbolic regression is a machine learning task: given a training dataset with features and targets, find a symbolic function that best predicts the target given the features. This paper concentrates on dynamic regression tasks, i.e. tasks where the goal changes during the model fitting process. Our study is motivated by dynamic regression tasks originating in the domain of reinforcement learning: we study four dynamic symbolic regression problems related to well-known reinforcement learning benchmarks, with data generated from the standard Value Iteration algorithm. We first show that in these problems the target function changes gradually, with no abrupt changes. Even these gradual changes, however, are a challenge to traditional Genetic Programming-based Symbolic Regression algorithms because they rely only on expression manipulation and selection. To address this challenge, we present an enhancement to such algorithms suitable for dynamic scenarios with gradual changes, namely the recently introduced type of leaf nodes called Linear Combination of Features. This type of leaf node, aided by the error backpropagation technique known from artificial neural networks, enables the algorithm to better fit the data by utilizing the error gradient to its advantage rather than searching blindly using only the fitness values. This setup is compared with a baseline of the core algorithm without any of our improvements and also with a classic evolutionary dynamic optimization technique: hypermutation. The results show that the proposed modifications greatly improve the algorithm ability to track a gradually changing target.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:22},
}

@Article{Zhu-et-al-2015a,
  author               = {Zhu, Shushang and Ji, Xiaodong and Li, Duan},
  date                 = {2015},
  journaltitle         = {Journal of Computational Finance},
  title                = {A robust set-valued scenario approach for handling modeling risk in portfolio optimization},
  abstract             = {For portfolio optimization under downside risk measures, such as conditional value-at-risk or lower partial moments, we often invoke a scenario approach to approximate the high-dimensional integral involved when calculating risk. Consequently, two types of modeling risk may arise from this procedure: uncertainty in determining the distribution of asset returns and the error caused by approximating a given distribution with scenarios. To handle these two types of modeling risk within a unified framework, we propose a mathematically tractable set-valued scenario approach. More specifically, when short selling is not permitted, the robust portfolio selection problems modeled within a minimum-maximum decision framework using several types of set-valued scenarios can be translated into linear programs or second-order cone programs. These can be efficiently solved by the interior point method. The proposed set-valued scenario approach can be used not only as a methodology to alleviate the modeling risk but also as a useful tool for evaluating the impact of modeling risk. Our simulation analysis and empirical study show that robustness does not necessarily imply conservativeness, portfolio performance is affected by the investment style characterized by the return-risk tradeoff to a large degree and modeling risk only becomes significant when an aggressive strategy is adopted.},
  citeulike-article-id = {13989088},
  groups               = {Scenario generation, PortfOptim_Scenario},
  owner                = {cristi},
  posted-at            = {2016-03-27 18:52:23},
  timestamp            = {2020-02-29 19:22},
}

@Article{Ziemba-2016,
  author               = {Ziemba, William T.},
  date                 = {2016-07},
  journaltitle         = {The Journal of Retirement},
  title                = {An Approach to Financial Planning of Retirement Pensions with Scenario-Dependent Correlation Matrixes and Convex Risk Measures},
  doi                  = {10.3905/jor.2016.4.1.099},
  issn                 = {2326-6899},
  number               = {1},
  pages                = {99--111},
  volume               = {4},
  abstract             = {The article describes an approach to asset-liability modeling using discrete time stochastic linear programming. The model uses future scenarios and optimizes the asset-liability mix subject to various constraints and is applicable to insurance companies, bank trading departments, overall bank asset-liability management, and other financial institutions. An application to the Siemens Austria pension fund, where the model has been in use since 2000, is described. The model has had considerable success and has been used by regulators to determine the effect of various possible pension fund policy changes. It has also been used by pension fund advisors who deal with uncertain assets and liabilities subject to various legal and policy constraints.},
  citeulike-article-id = {14119441},
  citeulike-linkout-0  = {http://dx.doi.org/10.3905/jor.2016.4.1.099},
  journal              = {The Journal of Retirement},
  owner                = {zkgst0c},
  posted-at            = {2016-08-21 12:21:36},
  timestamp            = {2020-02-29 19:22},
  year                 = {2016},
}

@Article{DaSilva-Shi-2019,
  author         = {Da Silva, Brandon and Shi, Sylvie Shang},
  date           = {2019-05-24},
  journaltitle   = {arXiv e-Print},
  title          = {Towards Improved Generalization in Financial Markets with Synthetic Data Generation},
  url            = {https://arxiv.org/abs/1906.03232},
  urldate        = {2019-10-05},
  abstract       = {Training deep learning models that generalize well to live deployment is a challenging problem in the financial markets. The challenge arises because of high dimensionality, limited observations, changing data distributions, and a low signal-to-noise ratio. High dimensionality can be dealt with using robust feature selection or dimensionality reduction, but limited observations often result in a model that overfits due to the large parameter space of most deep neural networks. We propose a generative model for financial time series, which allows us to train deep learning models on millions of simulated paths. We show that our generative model is able to create realistic paths that embed the underlying structure of the markets in a way stochastic processes cannot.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {DeepLearning_QWIM},
  timestamp      = {2020-02-29 19:33},
}

@Article{DaSilva-Shi-2019a,
  author         = {Da Silva, Brandon and Shi, Sylvie Shang},
  date           = {2019-05-24},
  journaltitle   = {arXiv e-Print},
  title          = {Style Transfer with Time Series: Generating Synthetic Financial Data},
  url            = {https://arxiv.org/abs/1906.03232v2},
  urldate        = {2019-12-26},
  abstract       = {Training deep learning models that generalize well to live deployment is a challenging problem in the financial markets. The challenge arises because of high dimensionality, limited observations, changing data distributions, and a low signal-to-noise ratio. High dimensionality can be dealt with using robust feature selection or dimensionality reduction, but limited observations often result in a model that overfits due to the large parameter space of most deep neural networks. We propose a generative model for financial time series, which allows us to train deep learning models on millions of simulated paths. We show that our generative model is able to create realistic paths that embed the underlying structure of the markets in a way stochastic processes cannot.},
  day            = {24},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:33},
}

@Article{Wiese-et-al-2019,
  author         = {Wiese, Magnus and Bai, Lianjun and Wood, Ben and Buehler, Hans},
  date           = {2019-11-05},
  journaltitle   = {arXiv e-Print},
  title          = {Deep Hedging: Learning to Simulate Equity Option Markets},
  url            = {https://arxiv.org/abs/1911.01700},
  urldate        = {2019-11-07},
  abstract       = {We construct realistic equity option market simulators based on generative adversarial networks (GANs). We consider recurrent and temporal convolutional architectures, and assess the impact of state compression. Option market simulators are highly relevant because they allow us to extend the limited real-world data sets available for the training and evaluation of option trading strategies. We show that network-based generators outperform classical methods on a range of benchmark metrics, and adversarial training achieves the best performance. Our work demonstrates for the first time that GANs can be successfully applied to the task of generating multivariate financial time series.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:34},
}

@Article{Wiese-et-al-2019a,
  author         = {Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter},
  date           = {2019-07-15},
  journaltitle   = {arXiv e-Print},
  title          = {Quant GANs: Deep Generation of Financial Time Series},
  url            = {https://arxiv.org/abs/1907.06673},
  urldate        = {2019-11-10},
  abstract       = {Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. In this paper, we break through this barrier and present Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function which utilize temporal convolutional networks (TCNs) and thereby achieve to capture longer-ranging dependencies such as the presence of volatility clusters. Furthermore, the generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:34},
}

@Article{Kondratyev-Schwarz-2019,
  author         = {Kondratyev, Alexei and Schwarz, Christian},
  date           = {2019},
  journaltitle   = {SSRN e-Print},
  title          = {The Market Generator},
  doi            = {10.2139/ssrn.3384948},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3384948},
  urldate        = {2019-11-07},
  abstract       = {We propose to use a special type of generative neural network - a Restricted Boltzmann Machine (RBM) - to build a powerful generator of synthetic market data that can replicate the probability distribution of the original market data. An RBM constructed with stochastic binary activation units in both the hidden and the visible layers (Bernoulli RBM) can learn complex dependence structures while avoiding overfitting. In this paper we consider an efficient data transformation and sampling approach that allows us to operate Bernoulli RBM on real-valued data sets and control the degree of autocorrelation and non-stationarity in the generated time series.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:35},
}

@Article{Takahashi-et-al-2019,
  author         = {Takahashi, Shuntaro and Chen, Yu and Tanaka-Ishii, Kumiko},
  date           = {2019-08},
  journaltitle   = {Physica A: Statistical Mechanics and its Applications},
  title          = {Modeling financial time-series with generative adversarial networks},
  doi            = {10.1016/j.physa.2019.121261},
  issn           = {0378-4371},
  pages          = {121261},
  url            = {https://linkinghub.elsevier.com/retrieve/pii/S0378437119307277},
  urldate        = {2019-11-10},
  volume         = {527},
  abstract       = {Abstract Financial time-series modeling is a challenging problem as it retains various complex statistical properties and the mechanism behind the process is unrevealed to a large extent. In this paper, a deep neural networks based approach, generative adversarial networks (GANs) for financial time-series modeling is presented. GANs learn the properties of data and generate realistic data in a data-driven manner. The GAN model produces a time-series that recovers the statistical properties of financial time-series such as the linear unpredictability, the heavy-tailed price return distribution, volatility clustering, leverage effects, the coarse-fine volatility correlation, and the gain/loss asymmetry.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:36},
}

@MastersThesis{DeMeerPardo-2019,
  author      = {{De Meer Pardo}, Fernando},
  date        = {2019},
  institution = {Delft University of Technology},
  title       = {Enriching Financial Datasets with Generative Adversarial Networks},
  url         = {https://repository.tudelft.nl/islandora/object/uuid:51d69925-fb7b-4e82-9ba6-f8295f96705c?collection=education},
  abstract    = {The scarcity of historical financial data has been a huge hindrance for the development algorithmic trading models ever since the first models were devised. Most financial models assume as hypothesis a series of characteristics regarding the nature of financial time series and seek extracting information about the state of the market through calibration. Through backtesting, a large number of these models are seen not to perform and are thus discarded. The remaining well-performing models however, are highly vulnerable to overfitting. Financial time series are complex by nature and their behaviour changes over time, so this concern is well founded. In addition to the problem of overfitting, available data is far too scarce for most machine learning applications and impossibly scarce for advanced approaches such as reinforcement learning, which has heavily impaired the application of these novel techniques in financial settings. This is where data generation comes into play. Generative Adversarial Networks, GANs, are a type of neural network architecture that focuses on sample generation. Through adversarial training, the GAN can learn the underlying structure of the input data and become able to generate samples very similar to those of the data distribution. This is specially useful in the case of high-dimensional objects, in which the dimensions are heavily inter-dependent, such as images, music and in our case financial time series. In this work we want to explore the generating capabilities of GANs applied to financial time series and investigate whether or not we can generate realistic financial scenarios.},
  timestamp   = {2020-02-29 19:36},
}

@Article{Koshiyama-et-al-2019,
  author         = {Koshiyama, Adriano and Firoozye, Nick and Treleaven, Philip},
  date           = {2019-01-07},
  journaltitle   = {arXiv e-Print},
  title          = {Generative Adversarial Networks for Financial Trading Strategies Fine-Tuning and Combination},
  url            = {https://arxiv.org/abs/1901.01751},
  abstract       = {Systematic trading strategies are algorithmic procedures that allocate assets aiming to optimize a certain performance criterion. To obtain an edge in a highly competitive environment, the analyst needs to proper fine-tune its strategy, or discover how to combine weak signals in novel alpha creating manners. Both aspects, namely fine-tuning and combination, have been extensively researched using several methods, but emerging techniques such as Generative Adversarial Networks can have an impact into such aspects. Therefore, our work proposes the use of Conditional Generative Adversarial Networks (cGANs) for trading strategies calibration and aggregation. To this purpose, we provide a full methodology on: (i) the training and selection of a cGAN for time series data; (ii) how each sample is used for strategies calibration; and (iii) how all generated samples can be used for ensemble modelling. To provide evidence that our approach is well grounded, we have designed an experiment with multiple trading strategies, encompassing 579 assets. We compared cGAN with an ensemble scheme and model validation methods, both suited for time series. Our results suggest that cGANs are a suitable alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha.},
  day            = {7},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:37},
}

@Article{Mariani-et-al-2019,
  author         = {Mariani, Giovanni and Zhu, Yada and Li, Jianbo and Scheidegger, Florian and Istrate, Roxana and Bekas, Costas and Malossi, A. Cristiano I.},
  date           = {2019-09-19},
  journaltitle   = {arXiv e-Print},
  title          = {PAGAN: Portfolio Analysis with Generative Adversarial Networks},
  url            = {https://arxiv.org/abs/1909.10578},
  urldate        = {2019-10-02},
  abstract       = {Since decades, the data science community tries to propose prediction models of financial time series. Yet, driven by the rapid development of information technology and machine intelligence, the velocity of today's information leads to high market efficiency. Sound financial theories demonstrate that in an efficient marketplace all information available today, including expectations on future events, are represented in today prices whereas future price trend is driven by the uncertainty. This jeopardizes the efforts put in designing prediction models. To deal with the unpredictability of financial systems, today's portfolio management is largely based on the Markowitz framework which puts more emphasis in the analysis of the market uncertainty and less in the price prediction. The limitation of the Markowitz framework stands in taking very strong ideal assumptions about future returns probability distribution. To address this situation we propose PAGAN, a pioneering methodology based on deep generative models. The goal is modeling the market uncertainty that ultimately is the main factor driving future trends. The generative model learns the joint probability distribution of price trends for a set of financial assets to match the probability distribution of the real market. Once the model is trained, a portfolio is optimized by deciding the best diversification to minimize the risk and maximize the expected returns observed over the execution of several simulations. Applying the model for analyzing possible futures, is as simple as executing a Monte Carlo simulation, a technique very familiar to finance experts. The experimental results on different portfolios representing different geopolitical areas and industrial segments constructed using real-world public data sets demonstrate promising results.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:39},
}

@Article{Marti-2019,
  author         = {Marti, Gautier},
  date           = {2019-10-21},
  journaltitle   = {arXiv e-Print},
  title          = {CorrGAN: Sampling Realistic Financial Correlation Matrices Using Generative Adversarial Networks},
  url            = {https://arxiv.org/abs/1910.09504},
  urldate        = {2019-10-24},
  abstract       = {We propose a novel approach for sampling realistic financial correlation matrices. This approach is based on generative adversarial networks. Experiments demonstrate that generative adversarial networks are able to recover most of the known stylized facts about empirical correlation matrices estimated on asset returns. This is the first time such results are documented in the literature. Practical financial applications range from trading strategies enhancement to risk and portfolio stress testing. Such generative models can also help ground empirical finance deeper into science by allowing for falsifiability of statements and more objective comparison of empirical methods.},
  day            = {21},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:40},
}

@Article{Li-Floudas-2016,
  author               = {Li, Zukui and Floudas, Christodoulos A.},
  date                 = {2016-01},
  journaltitle         = {Computers and Chemical Engineering},
  title                = {Optimal scenario reduction framework based on distance of uncertainty distribution and output performance: II. Sequential reduction},
  doi                  = {10.1016/j.compchemeng.2015.05.010},
  issn                 = {0098-1354},
  pages                = {599--610},
  volume               = {84},
  abstract             = {Novel method for reduction of a huge number of scenarios generated from the factorial combination. Sequential reduction framework greatly reduces the computational complexity. Scenario and robust optimization based criteria for quantifying the quality of reduction. In this paper, a novel sequential scenario reduction framework for general optimization problem is proposed. The proposed method extends the previous work (Li and Floudas, 2014) and aims to tackle optimization problems with a large number of uncertain parameters and a huge number of scenarios generated from the factorial combination. The proposed method first ranks the uncertain parameters based on their effects on the optimal objective using global sensitivity analysis. Then, the parameters are sequentially considered in generating uncertainty scenarios. This method can essentially reduce the computational efforts needed for evaluating the objective values of all scenarios, which is often impractical for a huge number of scenarios. Criteria for quantifying the quality of scenario reduction are also proposed based on robust optimization and scenario optimization. Case studies are presented to illustrate the sequential scenario reduction framework and the results verify the efficiency of the proposed approach.},
  citeulike-article-id = {14171143},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.compchemeng.2015.05.010},
  owner                = {cristi},
  posted-at            = {2016-10-24 20:12:12},
  timestamp            = {2020-02-29 19:42},
}

@Article{Andersen-et-al-2015a,
  author               = {Andersen, Torben G. and Fusari, Nicola and Todorov, Viktor},
  date                 = {2015-05},
  journaltitle         = {Econometrica},
  title                = {Parametric Inference and Dynamic State Recovery From Option Panels},
  doi                  = {10.3982/ecta10719},
  number               = {3},
  pages                = {1081--1145},
  volume               = {83},
  abstract             = {We develop a new parametric estimation procedure for option panels observed with error. We exploit asymptotic approximations assuming an ever increasing set of option prices in the moneyness (cross-sectional) dimension, but with a fixed time span. We develop consistent estimators for the parameters and the dynamic realization of the state vector governing the option price dynamics. The estimators converge stably to a mixed-Gaussian law and we develop feasible estimators for the limiting variance. We also provide semiparametric tests for the option price dynamics based on the distance between the spot volatility extracted from the options and one constructed nonparametrically from high-frequency data on the underlying asset. Furthermore, we develop new tests for the day-by-day model fit over specific regions of the volatility surface and for the stability of the risk-neutral dynamics over time. A comprehensive Monte Carlo study indicates that the inference procedures work well in empirically realistic settings. In an empirical application to S\&P 500 index options, guided by the new diagnostic tests, we extend existing asset pricing models by allowing for a flexible dynamic relation between volatility and priced jump tail risk. Importantly, we document that the priced jump tail risk typically responds in a more pronounced and persistent manner than volatility to large negative market shocks.},
  citeulike-article-id = {14025504},
  citeulike-linkout-0  = {http://dx.doi.org/10.3982/ecta10719},
  day                  = {1},
  groups               = {Implied_Fwd_Distrib, Market_States},
  owner                = {cristi},
  posted-at            = {2016-05-01 15:22:36},
  publisher            = {Blackwell Publishing Ltd},
  timestamp            = {2020-02-29 19:46},
}

@Article{Audrino-et-al-2015,
  author               = {Audrino, Francesco and Huitema, Robert and Ludwig, Markus},
  date                 = {2014-05},
  journaltitle         = {SSRN e-Print},
  title                = {An Empirical Analysis of the Ross Recovery Theorem},
  url                  = {https://ssrn.com/abstract=2433170},
  abstract             = {Building on the method of Ludwig (2015) to construct robust state price density surfaces from snapshots of option prices, we develop a nonparametric estimation strategy for the recovery theorem of Ross (2013). Using options on the SandP 500, we then investigate whether or not recovery yields predictive information beyond what can be gleaned from risk-neutral densities. Over the 13 year period from 2000 to 2012, we find that market timing strategies based on recovered moments significantly outperform those based on their risk-neutral counterparts.},
  citeulike-article-id = {14024361},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2433170},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2571704code1889202.pdf?abstractid=2433170 and mirid=1},
  day                  = {6},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-04-29 21:11:45},
  timestamp            = {2020-02-29 19:46},
}

@Article{Aydin-Yildirim-2015,
  author               = {Aydin, Halil I. and Yildirim, Yildiray},
  date                 = {2015},
  journaltitle         = {SSRN e-Print},
  title                = {Extracting Expectations in Affine Term Structure Models},
  abstract             = {In this paper, we study the problem of implementation of Ross (2015) Recovery theorem to disentangle the pricing kernel and physical probabilities from observed bond yields within discrete time affine term structure models. As a remedy to the problem of obtaining Arrow-Debreu prices of state transitions, we propose Markov chain approximation to autoregressive processes. Our work suggests that affine setting offers rich structure that enables us to obtain necessary inputs in empirical applications. In the second part, we estimate a canonical discrete time Gaussian three factor term structure model with the U.S. Treasury bond yields. We decompose bond yields into expectation and risk components without specifying risk adjustment inside the model. The results indicate that power of term spread in predicting economic activity stems from level of expectations component and change in risk premium component.},
  citeulike-article-id = {14025432},
  citeulike-linkout-0  = {http://faculty.baruch.cuny.edu/lwu/890/AydinYildirim2015.pdf},
  groups               = {Implied_Fwd_Distrib},
  howpublished         = {Available at http://faculty.baruch.cuny.edu/lwu/890/AydinYildirim2015.pdf},
  owner                = {cristi},
  posted-at            = {2016-05-01 13:28:18},
  timestamp            = {2020-02-29 19:46},
}

@Article{Backwell-2015,
  author               = {Backwell, Alex},
  date                 = {2015-01},
  journaltitle         = {Journal of Risk and Financial Management},
  title                = {State Prices and Implementation of the Recovery Theorem},
  doi                  = {10.3390/jrfm8010002},
  number               = {1},
  pages                = {2--16},
  volume               = {8},
  abstract             = {It is generally held that derivative prices do not contain useful predictive information, that is, information relating to the distribution of future financial variables under the real-world measure. This is because the market's implicit forecast of the future becomes entangled with market risk preferences during derivative price formation. A result derived by Ross [1], however, recovers the real-world distribution of an equity index, requiring only current prices and mild restrictions on risk preferences. In addition to being of great interest to the theorist, the potential practical value of the result is considerable. This paper addresses implementation of the Ross Recovery Theorem. The theorem is formalised, extended, proved and discussed. Obstacles to application are identified and a workable implementation methodology is developed.},
  citeulike-article-id = {14025089},
  citeulike-linkout-0  = {http://dx.doi.org/10.3390/jrfm8010002},
  citeulike-linkout-1  = {http://www.mdpi.com/1911-8074/8/1/2},
  citeulike-linkout-2  = {http://www.mdpi.com/1911-8074/8/1/2/pdf},
  day                  = {19},
  groups               = {Implied_Fwd_Distrib, Market_States},
  owner                = {cristi},
  posted-at            = {2016-04-30 13:34:12},
  timestamp            = {2020-02-29 19:46},
}

@Article{Bailey-dePrado-2013,
  author       = {Bailey, D.H. and de Prado, M. Lopez},
  date         = {2013},
  journaltitle = {SSRN e-Print},
  title        = {Drawdown-Based Stop-Outs and the 'Triple Penance' Rule},
  abstract     = {At what loss should a portfolio manager be stopped-out? What is an acceptable time under water? We demonstrate that, under standard portfolio theory assumptions, the answer to the latter question is strikingly unequivocal: On average, the recovery spans three times the period involved in accumulating the maximum quantile loss for a given confidence level. We denote this principle the triple penance rule .

We provide a theoretical justification to why investment firms typically set less strict stop-out rules to portfolio managers with higher Sharpe ratios, despite the fact that they should be expected to deliver superior performance. We generalize this framework to the case of first-order auto-correlated investment outcomes, and conclude that ignoring the effect of serial correlation leads to a gross underestimation of the downside potential of hedge fund strategies, by as much as 70 percent. We also estimate that some hedge funds may be firing more than three times the number of skillful portfolio managers, compared to the number that they were willing to accept, as a result of evaluating their performance through traditional metrics, such as the Sharpe ratio.

We believe that our closed-form compact expression for the estimation of downside potential, without having to assume IID cashflows, will open new practical applications in risk management, portfolio optimization and capital allocation. The Python code included confirms the accuracy of our analytical solution.

The appendices for this paper are available at SSRN},
  groups       = {Implied_Fwd_Distrib},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=2201302},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:46},
}

@Article{Bailey-dePrado-2015,
  author       = {David H. Bailey and de Prado, Marcos Lopez},
  date         = {2015},
  journaltitle = {Journal of Risk},
  title        = {Stop-outs under serial correlation and the triple penance rule},
  abstract     = {At what loss should a portfolio manager (PM) be stopped out? What is an acceptable time under water? We demonstrate that, under standard portfolio theory assumptions, the answer to the latter question is strikingly unequivocal: on average, the recovery spans three times the period involved in accumulating the maximum quantile loss for a given confidence level. We denote this principle the triple penance rule .

We provide a theoretical justification as to why investment firms typically set less strict stop-out rules for PMs with higher Sharpe ratios, despite the fact that they should be expected to deliver a superior performance. We generalize this framework to the case of first-order autocorrelated investment outcomes, and we conclude that ignoring the effect of serial correlation leads to a gross underestimation of the downside potential of hedge fund strategies, by as much as 70 percent.

We also estimate that some hedge funds may be firing more than three times as many skillful PMs as they are willing to accept, as a result of evaluating their performance through traditional metrics, such as the Sharpe ratio. We believe that our closed-form compact expression for the estimation of downside potential, without having to assume independent and identically distributed cashflows, will open new practical applications in risk management, portfolio optimization and capital allocation. The Python code included in the online appendix confirms the accuracy of our analytical solution.},
  groups       = {Implied_Fwd_Distrib},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:46},
}

@Article{Bakshi-et-al-2015,
  author               = {Bakshi, Gurdip and Chabi-Yob, Fousseni and Gao, Xiaohui},
  date                 = {2015},
  journaltitle         = {SSRN e-Print},
  title                = {A Recovery That We Can Trust? Deducing and Testing the Restrictions of the Recovery Theorem},
  abstract             = {How reliable is the recovery theorem of Ross (2015)? We explore this question in the context of options on the 30-year Treasury bond futures, allowing us to deduce restrictions that link the risk-neutral and physical distributions. The backbone of these restrictions is that the martingale component of the stochastic discount factor is unity. Our approach and empirical results provide an agnostic view of the claims of the recovery theorem in the long-term bond futures market. Moreover, our theoretical formulation and implementation reveal the presence of a martingale component that exhibits substantial dispersion and is positively correlated with the transitory component.},
  citeulike-article-id = {14025438},
  groups               = {Implied_Fwd_Distrib},
  howpublished         = {Available at http://scholar.rhsmith.umd.edu/sites/default/files/gbakshi/files/rossrecovery12-2015.pdf},
  owner                = {cristi},
  posted-at            = {2016-05-01 13:38:19},
  timestamp            = {2020-02-29 19:46},
}

@Article{Bakshi-et-al-2017a,
  author               = {Bakshi, Gurdip and Chabi-Yo, Fousseni and Gao, Xiaohui},
  date                 = {2017-09-25},
  journaltitle         = {The Review of Financial Studies},
  title                = {A Recovery that We Can Trust? Deducing and Testing the Restrictions of the Recovery Theorem},
  doi                  = {10.1093/rfs/hhx108},
  issn                 = {0893-9454},
  abstract             = {How reliable is the recovery theorem of Ross (2015)? We explore this question in the context of options on the 30-year Treasury bond futures, allowing us to deduce restrictions that link the physical and risk-neutral return distributions. Our empirical results undermine the implications of the recovery theorem. First, we reject an implicit assumption of the recovery theorem that the martingale component of the stochastic discount factor is identical to unity. Second, we consider the restrictions between the physical and risk-neutral return moments when the recovery theorem holds, and reject them in both forecasting regressions and generalized method of moments estimations.},
  citeulike-article-id = {14486026},
  citeulike-linkout-0  = {http://dx.doi.org/10.1093/rfs/hhx108},
  day                  = {25},
  groups               = {Implied_Fwd_Distrib},
  posted-at            = {2017-11-29 23:43:55},
  timestamp            = {2020-02-29 19:46},
}

@Article{Bjornland-et-al-2015,
  author               = {Bjornland, Hilde C. and Ravazzolo, Francesco and Thorsrud, Leif A.},
  date                 = {2015-05},
  journaltitle         = {SSRN e-Print},
  title                = {Forecasting GDP with Global Components. This Time is Different},
  url                  = {https://ssrn.com/abstract=2602492},
  abstract             = {A long strand of literature has shown that the world has become more global. Yet, the recent Great Global Recession turned out to be hard to predict, with forecasters across the world committing large forecast errors. We examine whether knowledge of in-sample co-movement across countries could have been used in a more systematic way to improve forecast accuracy at the national level. In particular, we ask if a model with common international business cycle factors forecasts better than the purely domestic alternative? To answer this question we employ a Dynamic Factor Model (DFM) and run an out-of-sample forecasting experiment. Our results show that exploiting the informational content in a common global business cycle factor improves forecasting accuracy in terms of both point and density forecast evaluation across a large panel of countries. In line with much reported in-sample evidence, we also document that the Great Recession has a huge impact on this result. The event causes a clear preference shift towards the model including a common global factor. Similar shifts are not observed earlier in the evaluation sample. However, this time is different also in other respects. On longer forecasting horizons the performance of the DFM deteriorates substantially in the aftermath of the Great Recession. This indicates that the recession shock itself was felt globally, but that the recovery phase has been very different across countries.},
  citeulike-article-id = {14070605},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2602492},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2602492code1689451.pdf?abstractid=2602492 and mirid=1},
  day                  = {4},
  groups               = {Implied_Fwd_Distrib},
  owner                = {zkgst0c},
  posted-at            = {2016-06-16 23:34:34},
  timestamp            = {2020-02-29 19:46},
}

@Online{Davis-2017,
  author    = {Tom Davis},
  date      = {2017},
  title     = {Mind your Ps and Qs: real world vs. risk neutral probabilities},
  url       = {https://insight.factset.com/mind-your-ps-and-qs-real-world-vs.risk-neutral-probabilities},
  groups    = {Implied_Fwd_Distrib, Q_vs_P_measures},
  timestamp = {2020-02-29 19:47},
}

@Article{Dillschneider-Maurer-2017,
  author               = {Dillschneider, Yannick and Maurer, Raimond},
  date                 = {2017-04},
  journaltitle         = {SSRN e-Print},
  title                = {Functional Ross Recovery: An Operator Approach},
  url                  = {https://ssrn.com/abstract=2991984},
  abstract             = {Recently, Ross (2015) showed that the real-world probability distribution of a discrete Markovian state variable can be recovered from observed option prices. The so-called recovery theorem follows from Perron-Frobenius matrix theory when the pricing kernel is transition independent. In this paper, we generalize the recovery theorem to unbounded continuous state spaces using Perron-Frobenius operator theory. Building on our theoretical results, we devise a nonparametric estimation approach to empirically recover the pricing kernel and real-world probability density in closed form. Using SandP 500 index options, we find that recovered pricing kernels are persistently U-shaped, documenting the well-known pricing kernel puzzle.},
  citeulike-article-id = {14403678},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2991984},
  groups               = {Implied_Fwd_Distrib},
  posted-at            = {2017-07-31 15:29:39},
  timestamp            = {2020-02-29 19:47},
}

@Article{Flint-Mare-2016,
  author               = {Flint, Emlyn J. and Mare, Eben},
  date                 = {2016-08},
  journaltitle         = {SSRN e-Print},
  title                = {Estimating Option-Implied Distributions in Illiquid Markets and Implementing the Ross Recovery Theorem},
  url                  = {https://ssrn.com/abstract=2817080},
  abstract             = {We describe how forward-looking information on the statistical properties of an asset can be extracted directly from options market data and how this can be used practically in portfolio management. Although the extraction of a forward-looking risk-neutral distribution is well-established in the literature, the issue of estimation in an illiquid market is not. We use the deterministic SVI volatility model to estimate weekly risk-neutral distribution surfaces. The issue of calibration with sparse and noisy data is considered at length and a simple but robust fitting algorithm is proposed. Furthermore, we attempt to extract real-world implied information by implementing the recovery theorem introduced by Ross (2015). Recovery is an ill-posed problem that requires careful consideration. We describe a regularization methodology for extracting real-world implied distributions and implement this method on a history of SVI volatility surfaces. We analyse the first four moments from the implied risk-neutral and real-world implied distributions and use them as signals within a simple tactical asset allocation framework, finding promising results.},
  citeulike-article-id = {14108494},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2817080},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2817080code1247922.pdf?abstractid=2817080 and mirid=1},
  day                  = {4},
  groups               = {Implied_Fwd_Distrib},
  owner                = {zkgst0c},
  posted-at            = {2016-08-05 14:30:16},
  timestamp            = {2020-02-29 19:47},
}

@Article{Gagnon-et-al-2018,
  author         = {Gagnon, Marie-Helene and Power, Gabriel and Toupin, Dominique},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Forecasting Market Index Volatility Using Ross-Recovered Distributions},
  doi            = {10.2139/ssrn.3259654},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3259654},
  abstract       = {Ross (2015) shows that options data can reveal the market true expectations. Adapting this approach to index options (SandP, FTSE, CAC, SMI and DAX), we separate option-implied volatility into Ross-recovered true expected volatility and a risk preference factor. We investigate whether these factors perform better to forecast realized volatility if constructed locally or globally, yielding new insights to understand international dynamics in risk expectations and preferences. We find evidence of significantly improved realized volatility forecasts. Models using Ross-recovered value-weighted global measures of risk preferences have the best forecasting performances across indices. Risk preferences are best measured globally. Overall, the findings suggest that the Recovery Theorem is useful empirically and accurately recovers the true expected returns distribution and its associated pricing kernel.},
  f1000-projects = {QuantInvest},
  groups         = {Implied_Fwd_Distrib},
  timestamp      = {2020-02-29 19:47},
}

@Article{Guidolin-Timmermann-2006,
  author       = {Guidolin, Massimo and Timmermann, Allan G},
  date         = {2007},
  journaltitle = {Journal of Economic Dynamics and Control},
  title        = {Asset Allocation under Multivariate Regime Switching},
  doi          = {10.1016/j.jedc.2006.12.004},
  url          = {https://www.sciencedirect.com/science/article/pii/S0165188906002272},
  abstract     = {This paper studies asset allocation decisions in the presence of regime switching in asset returns. We find evidence that four separate regimes (characterized as crash, slow growth, bull and recovery states) are required to capture the joint distribution of stock and bond returns.

Optimal asset allocations vary considerably across these states and change over time as investors revise their estimates of the state probabilities. In the crash state, buy-and-hold investors allocate more of their portfolio to stocks the longer their investment horizon, while the optimal allocation to stocks declines as a function of the investment horizon in bull markets.

The joint effects of learning about state probabilities and predictability of asset returns from the dividend yield give rise to a non-monotonic relationship between the investment horizon and the demand for stocks.

Out-of-sample forecasting experiments confirm the economic importance of accounting for the presence of regimes in asset returns.},
  groups       = {Regime_Invest, Implied_Fwd_Distrib},
  howpublished = {Available at SSRN: http://ssrn.com/abstract=940652},
  owner        = {zkgst0c},
  timestamp    = {2020-02-29 19:47},
}

@Article{Jackwerth-Menner-2017,
  author               = {Jackwerth, Jens C. and Menner, Marco},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Does the Ross Recovery Theorem Work Empirically?},
  doi                  = {10.2139/ssrn.2960733},
  issn                 = {1556-5068},
  abstract             = {Starting with the fundamental relationship that state prices are the product of physical probabilities and the pricing kernel, Ross (2015) shows that, given strong assumptions, knowing state prices suffices for backing out physical probabilities and the pricing kernel at the same time. We find that such recovered physical distributions based on the SandP 500 index are incompatible with future realized returns. This negative result remains, even when we add economically reasonable constraints. Reasons for the rejection seem to be numerical instabilities of the recovery algorithm and the inability of the constrained versions to generate pricing kernels sufficiently away from risk-neutrality.},
  citeulike-article-id = {14412400},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2960733},
  groups               = {Implied_Fwd_Distrib},
  posted-at            = {2017-08-11 07:55:08},
  timestamp            = {2020-02-29 19:48},
}

@Article{Jensen-et-al-2016,
  author               = {Jensen, Christian S. and Lando, David and Pedersen, Lasse H.},
  date                 = {2015-12},
  journaltitle         = {SSRN e-Print},
  title                = {Generalized Recovery},
  url                  = {https://ssrn.com/abstract=2674541},
  abstract             = {We characterize when physical probabilities, marginal utilities, and the discount rate can be recovered from observed state prices for several future time periods. Our characterization makes no assumptions of the probability distribution, thus generalizing the time-homogeneous stationary model of Ross (2015). Our characterization is simple and intuitive, linking recovery to the relation between the number of time periods and the number of states. When recovery is feasible, our model is easy to implement, allowing a closed-form linearized solution. We implement our model empirically, testing the predictive power of the recovered expected return and other recovered statistics.},
  citeulike-article-id = {14024364},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2674541},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2763573code2462509.pdf?abstractid=2674541 and mirid=1},
  day                  = {8},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-04-29 21:15:35},
  timestamp            = {2020-02-29 19:48},
}

@Article{Kremer-et-al-2017,
  author               = {Kremer, PhilippJ and Talmaciu, Andreea and Paterlini, Sandra},
  date                 = {2017},
  journaltitle         = {Annals of Operations Research},
  title                = {Risk minimization in multi-factor portfolios: What is the best strategy?},
  doi                  = {10.1007/s10479-017-2467-6},
  pages                = {1--37},
  abstract             = {Exposures to risk factors, as opposed to individual securities or bonds, can lead to an ex-ante improved risk management and a more transparent and cheaper way of developing active asset allocation strategies. This paper provides an extensive analysis of eight state-of-the-art risk-minimization schemes and compares risk factor performance in a conditional performance analysis, contrasting good and bad states of the economy. The investment universe spans a total of 25 risk factors, including size, momentum, value, high profitability and low investments, from five non-overlapping regions (i.e., USA, UK, Japan, Developed Europe ex. UK and, Asia ex. Japan). Considering as investment period the interval from May 2004 to June 2015, our results show that each single factor yields positive premia in exchange for risk, which can lead to considerable underperformance and extensive recovery periods during times of crisis. The best factor investments can be found in Asia ex. Japan and the US. However, risk factor based portfolio construction across the various regions enables the investor to exploit low correlation structures, reducing the overall volatility, as well as tail- and extreme risk measures. Finally, the empirical results point towards the long-only global minimum variance portfolio, as the best risk minimization strategy.},
  citeulike-article-id = {14367303},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s10479-017-2467-6},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s10479-017-2467-6},
  groups               = {Invest_Risk, Implied_Fwd_Distrib, Multifactor_Invest},
  posted-at            = {2017-06-02 20:57:54},
  publisher            = {Springer US},
  timestamp            = {2020-02-29 19:48},
}

@Article{Kremer-et-al-2017a,
  author               = {Kremer, Philipp J. and Talmaciu, Andreea and Paterlini, Sandra},
  date                 = {2017},
  journaltitle         = {SSRN e-Print},
  title                = {Risk Minimization in Multi-Factor Portfolios: What is the Best Strategy?},
  url                  = {https://ssrn.com/abstract=2943464},
  abstract             = {Exposures to risk factors, as opposed to individual securities or bonds, can lead to an ex-ante improved risk management and a more transparent and cheaper way of developing active asset allocation strategies. This paper provides an extensive analysis of eight state-of-the-art risk-minimization schemes and compares risk factor performance in a conditional performance analysis, contrasting good and bad states of the economy. The investment universe spans a total of 25 risk factors, including size, momentum, value, high profitability and low investments, from five nonoverlapping regions (i.e., USA, UK, Japan, Developed Europe ex. UK, and Asia ex. Japan). Considering as investment period the interval from May 2004 to June 2015, our results show that each single factor yields positive premia in exchange for risk, which can lead to considerable underperformance and extensive recovery periods during times of crisis. The best factor investments can be found in Asia ex. Japan and the US. However, risk factor based portfolio construction across the various regions enables the investor to exploit low correlation structures, reducing the overall volatility, as well as tail- and extreme risk measures. Finally, the empirical results point towards the long-only global minimum variance portfolio, as the best risk minimization strategy.},
  citeulike-article-id = {14328046},
  citeulike-linkout-0  = {https://papers.ssrn.com/sol3/papers.cfm?abstractid=2943464},
  groups               = {Implied_Fwd_Distrib, Multifactor_Invest},
  posted-at            = {2017-04-03 21:15:07},
  timestamp            = {2020-02-29 19:48},
}

@Article{Liu-2014a,
  author               = {Liu, Fang},
  date                 = {2014},
  journaltitle         = {SSRN e-Print},
  title                = {Recovering Conditional Return Distributions by Regression: Estimation and Applications},
  doi                  = {10.2139/ssrn.2530183},
  issn                 = {1556-5068},
  abstract             = {I propose a regression approach to recovering the return distribution of an individual asset conditional on the return of an aggregate index based on their marginal distributions. This approach relies on the identifying assumption that the conditional return distribution of the asset given the index return does not vary over time. I show how to empirically implement this approach using option price data. I then apply this approach to examine the cross-sectional equity risk premium associated with systematic disaster risk, to estimate the exposure of banks to systemic shocks, and to extend the Ross (Journal of Finance, 2014) recovery theorem to individual assets.},
  citeulike-article-id = {14025441},
  citeulike-linkout-0  = {http://dx.doi.org/10.2139/ssrn.2530183},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-05-01 13:42:09},
  timestamp            = {2020-02-29 19:48},
}

@Article{Marin-Sucarrat-2015,
  author               = {Marin, J. Miguel and Sucarrat, Genaro},
  date                 = {2015-11},
  journaltitle         = {The European Journal of Finance},
  title                = {Financial density selection},
  doi                  = {10.1080/1351847x.2012.706906},
  number               = {13-14},
  pages                = {1195--1213},
  volume               = {21},
  abstract             = {We propose and study simple but flexible methods for density selection of skewed versions of the two most popular density classes in finance, the exponential power distribution and the t distribution.

For the first type of method, which simply consists of selecting a density by means of an information criterion, the Schwarz criterion stands out since it performs well across density categories, and in particular when the DGP is normal.

For the second type of method, general-to-specific density selection, the simulations suggest that it can improve the recovery rate in predictable ways by changing the significance level. This is useful because it enables us to increase (reduce) the recovery rate of non-normal densities by increasing (reducing) the significance level, if one wishes to do so.

The third type of method is a generalisation of the second type, such that it can be applied across an arbitrary number of density classes, nested or non-nested.

Finally, the methods are illustrated in an empirical application.},
  citeulike-article-id = {13935048},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/1351847x.2012.706906},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/1351847X.2012.706906},
  day                  = {14},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-02-18 07:43:49},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 19:48},
}

@Article{Massacci-et-al-2016,
  author               = {Massacci, Fabio and Williams, Julian M. and Zhang, Yang},
  date                 = {2016-05},
  journaltitle         = {SSRN e-Print},
  title                = {Empirical Recovery: Hansen-Scheinkman Factorization and Ross Recovery from High Frequency Option Prices},
  url                  = {https://ssrn.com/abstract=2784153},
  abstract             = {Recent research has shown that the Perron-Frobenius eigenfunction of a Markov risk neutral state price transition matrix has an interesting economic interpretation. Yet, the application to actual market prices presents significant challenge. For instance, even at the intraday frequency market data, has lots of gaps and can contain unpredictable levels of noise. As a consequence, the identification of the risk neutral state transition matrix often results in a matrix that violates the basic properties of the Markov chain presumed to be driving the evolution of asset prices. We provide a fast non-linear programming approach to the Recovery Theorem such that the attained minimum formally satisfies the desired mathematical and economical constraints (e.g. the de facto discount factor being smaller than unity and unimodality of the transition matrix). We demonstrate the empirical effectiveness of the methodology on SandP 500 index options and appeal to recent theoretical results to extend this approach to individual stocks.},
  citeulike-article-id = {14063981},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2784153},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2785946code2186409.pdf?abstractid=2784153 and mirid=1},
  day                  = {25},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-06-09 18:11:48},
  timestamp            = {2020-02-29 19:48},
}

@Article{Park-2016,
  author               = {Park, Hyungbin},
  date                 = {2016-05},
  journaltitle         = {Quantitative Finance},
  title                = {Ross recovery with recurrent and transient processes},
  doi                  = {10.1080/14697688.2015.1092572},
  number               = {5},
  pages                = {667--676},
  volume               = {16},
  citeulike-article-id = {14071816},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1092572},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1092572},
  day                  = {3},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-06-18 18:38:14},
  publisher            = {Routledge},
  timestamp            = {2020-02-29 19:49},
}

@Article{Qin-et-al-2018,
  author         = {Qin, Likuan and Linetsky, Vadim and Nie, Yutian},
  date           = {2018-04-06},
  journaltitle   = {The Review of Financial Studies},
  title          = {Long forward probabilities, recovery, and the term structure of bond risk premiums},
  doi            = {10.1093/rfs/hhy042},
  issn           = {0893-9454},
  abstract       = {This paper examines the assumption of transition independence of the stochastic discount factor (SDF) in the bond market. This assumption underlies the recovery result of Ross 2015. Following the methodology of Alvarez and Jermann 2005 and Hansen and Scheinkman 2009, we estimate the martingale component in the long-term factorization of the SDF using U.S. Treasury data. The empirically estimated martingale component is highly volatile and produces a downward-sloping term structure of bond Sharpe ratios. In contrast, the transition independence assumption implies a degenerate martingale component and an upward-sloping term structure of bond Sharpe ratios. Thus, transition independence is inconsistent with our empirical results.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {RiskPremia_FixedIncome, Implied_Fwd_Distrib},
  timestamp      = {2020-02-29 19:49},
}

@Article{Qin-Linetsky-2016,
  author               = {Qin, Likuan and Linetsky, Vadim},
  date                 = {2016-02},
  journaltitle         = {Operations Research},
  title                = {Positive Eigenfunctions of Markovian Pricing Operators: Hansen-Scheinkman Factorization, Ross Recovery, and Long-Term Pricing},
  doi                  = {10.1287/opre.2015.1449},
  issn                 = {0030-364X},
  number               = {1},
  pages                = {99--117},
  volume               = {64},
  abstract             = {This paper develops a spectral theory of Markovian asset pricing models where the underlying economic uncertainty follows a continuous-time Markov process X with a general state space (Borel right process, or BRP) and the stochastic discount factor (SDF) is a positive semimartingale multiplicative functional of X. A key result is the uniqueness theorem for a positive eigenfunction of the pricing operator such that X is recurrent under a new probability measure associated with this eigenfunction (recurrent eigenfunction). As economic applications, we prove uniqueness of the Hansen and Scheinkman factorization of the Markovian SDF corresponding to the recurrent eigenfunction; extend the Recovery Theorem from discrete time, finite state irreducible Markov chains to recurrent BRPs; and obtain the long-maturity asymptotics of the pricing operator. When an asset pricing model is specified by given risk-neutral probabilities together with a short rate function of the Markovian state, we give sufficient conditions for the existence of a recurrent eigenfunction and provide explicit examples in a number of important financial models, including affine and quadratic diffusion models and an affine model with jumps. These examples show that the recurrence assumption, in addition to fixing uniqueness, rules out unstable economic dynamics, such as the short rate asymptotically going to infinity or to a zero lower bound trap without possibility of escaping.},
  citeulike-article-id = {14029320},
  citeulike-linkout-0  = {http://dx.doi.org/10.1287/opre.2015.1449},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-05-06 20:56:00},
  timestamp            = {2020-02-29 19:49},
}

@Article{Ross-2015,
  author               = {Ross, Steve},
  date                 = {2015-04},
  journaltitle         = {The Journal of Finance},
  title                = {The Recovery Theorem},
  doi                  = {10.1111/jofi.12092},
  number               = {2},
  pages                = {615--648},
  volume               = {70},
  abstract             = {We can only estimate the distribution of stock returns, but from option prices we observe the distribution of state prices. State prices are the product of risk aversion the pricing kernel and the natural probability distribution. The Recovery Theorem enables us to separate these to determine the market's forecast of returns and risk aversion from state prices alone. Among other things, this allows us to recover the pricing kernel, market risk premium, and probability of a catastrophe and to construct model-free tests of the efficient market hypothesis.},
  citeulike-article-id = {14024360},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/jofi.12092},
  day                  = {1},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-04-29 21:11:13},
  timestamp            = {2020-02-29 19:49},
}

@MastersThesis{Spears-2013,
  author               = {Spears, Trent},
  date                 = {2013},
  institution          = {Oxford University},
  title                = {On estimating the risk-neutral and real-world probability measures},
  url                  = {https://core.ac.uk/download/pdf/13501943.pdf},
  abstract             = {This dissertation is about inferring market beliefs about the real-world probability distribution describing the future financial returns of an underlying asset from option price data.

The primary goal of this dissertation is to present a replication of the econometrics and results of Ross Recovery as applied to S\&P 500 index option price data from 27 April, 2011},
  citeulike-article-id = {14025087},
  groups               = {Implied_Fwd_Distrib},
  owner                = {cristi},
  posted-at            = {2016-04-30 13:31:55},
  timestamp            = {2020-02-29 19:49},
}

@Article{Yuen-2015,
  author               = {Yuen, Selwyn},
  date                 = {2015},
  journaltitle         = {SSRN e-Print},
  title                = {The Fear Factor and the Cross Section of Stock Returns},
  abstract             = {I document that the risk-neutral left tail risk estimated from S\&P 500 index options is priced in the cross section of stock returns using both Fama-MacBeth regressions and portfolio sorts. Stocks in the bottom quintile of comovement with left tail risk have 3.0 percent higher returns per year than stocks in the top quintile, with a 3-factor alpha of 2.4 percent per year. The option-implied left tail embeds both the expected crash risk and the investors' subjective perception of fear. Short of separating the two effects, each of the alternate interpretations has interesting implications for existing models. Under the crash-risk interpretation, my results favor models with time-varying recovery rates over time-varying disaster probabilities. Under the fear interpretation, my results are consistent with the class of models featuring time-varying crash aversion or beliefs. Empirically, the left tail risk premium is not confounded by known effects such as coskewness, downside beta, idiosyncratic volatility, skewness, and kurtosis.},
  citeulike-article-id = {14025513},
  groups               = {Implied_Fwd_Distrib},
  howpublished         = {Available at http://www.kellogg.northwestern.edu/faculty/yuen/YuenPaper2.pdf},
  owner                = {cristi},
  posted-at            = {2016-05-01 15:36:54},
  timestamp            = {2020-02-29 19:49},
}

@Article{Yeh-et-al-2017b,
  author         = {Yeh, Chin-Chia Michael and Zhu, Yan and Ulanova, Liudmila and Begum, Nurjahan and Ding, Yifei and Dau, Hoang Anh and Zimmerman, Zachary and Silva, Diego Furtado and Mueen, Abdullah and Keogh, Eamonn},
  date           = {2017-06-24},
  journaltitle   = {Data Min Knowl Discov},
  title          = {Time series joins, motifs, discords and shapelets: a unifying view that exploits the matrix profile},
  doi            = {10.1007/s10618-017-0519-9},
  issn           = {1384-5810},
  number         = {1},
  pages          = {1--41},
  volume         = {32},
  abstract       = {The last decade has seen a flurry of research on all-pairs-similarity-search (or similarity joins) for text, DNA and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. However, there has been surprisingly little progress made on similarity joins for time series subsequences. The lack of progress probably stems from the daunting nature of the problem. For even modest sized datasets the obvious nested-loop algorithm can take months, and the typical speed-up techniques in this domain (i.e., indexing, lower-bounding, triangular-inequality pruning and early abandoning) at best produce only one or two orders of magnitude speedup. In this work we introduce a novel scalable algorithm for time series subsequence all-pairs-similarity-search. For exceptionally large datasets, the algorithm can be trivially cast as an anytime algorithm and produce high-quality approximate solutions in reasonable time and/or be accelerated by a trivial porting to a GPU framework. The exact similarity join algorithm computes the answer to the time series motif and time series discord problem as a side-effect, and our algorithm incidentally provides the fastest known algorithm for both these extensively-studied problems. We demonstrate the utility of our ideas for many time series data mining problems, including motif discovery, novelty discovery, shapelet discovery, semantic segmentation, density estimation, and contrast set mining. Moreover, we demonstrate the utility of our ideas on domains as diverse as seismology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring and medicine.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-02-29 19:52},
}

@Article{Kiriu-Hibiki-2019,
  author         = {Kiriu, Takuya and Hibiki, Norio},
  date           = {2019-04-25},
  journaltitle   = {Journal of the Operations Research Society of Japan},
  title          = {Estimating forward looking distribution with the Ross recovery theorem},
  doi            = {10.15807/jorsj.62.83},
  issn           = {0453-4514},
  number         = {2},
  pages          = {83--107},
  url            = {https://keio.pure.elsevier.com/en/publications/estimating-forward-looking-distribution-with-the-ross-recovery-th},
  urldate        = {2019-08-22},
  volume         = {62},
  abstract       = {Ross (2015) introduced a remarkable theorem, named the Theorem. It enables us to estimate the real world distribution from the risk neutral distribution derived from option prices under a particular assumption about a representative investor's risk preferences. The real world distribution estimated with the Recovery Theorem is suitable for many financial problems such as market risk management and portfolio optimization due to its forward looking nature. However, it is not easy to derive the appropriate estimators because of an ill-posed problem in the estimation process. We propose a new method to derive the accurate solution by formulating the regularization term involving prior information. Previous studies propose methods to estimate the real world distribution, but they do not investigate the estimation accuracy. We show the effectiveness of the proposed method through the numerical analysis with hypothetical data.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:52},
}

@Article{Audrino-et-al-2019,
  author         = {Audrino, Francesco and Huitema, Robert and Ludwig, Markus},
  date           = {2019-02-19},
  journaltitle   = {Journal of Financial Econometrics},
  title          = {An empirical implementation of the Ross recovery theorem as a prediction device},
  doi            = {10.1093/jjfinec/nbz002},
  issn           = {1479-8409},
  abstract       = {Building on the method of Ludwig (2015) to construct robust state price density surfaces from snapshots of option prices, we develop a nonparametric estimation strategy based on the recovery theorem of Ross (2015). Using options on the S\&P 500, we then investigate whether or not recovery yields predictive information beyond what can be gleaned from risk-neutral densities. Over the 13 year period from 2000 to 2012, we find that market timing strategies based on recovered moments outperform those based on risk-neutral moments.},
  day            = {19},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-02-29 19:53},
}

@Article{Das-et-al-2019,
  author         = {Das, Sanjiv R. and Kim, Seoyoung and Ostrov, Daniel N.},
  date           = {2019-01-31},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Dynamic Systemic Risk: Networks in Data Science},
  url            = {https://jfds.pm-research.com/content/1/1/141},
  abstract       = {In this article, the authors propose a theory-driven framework for monitoring system-wide risk by extending data science methods widely deployed in social networks. Their approach extends the one-firm Merton credit risk model to a generalized stochastic network-based framework across all financial institutions, comprising a novel approach to measuring systemic risk over time. The authors identify four desired properties for any systemic risk measure. They also develop measures for the risks created by each individual institution and a measure for risk created by each pairwise connection between institutions. Four specific implementation models are then explored, and brief empirical examples illustrate the ease of implementation of these four models and show general consistency among their results.},
  day            = {31},
  f1000-projects = {QuantInvest},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-05-25 12:09},
}

@Article{Das-et-al-2019a,
  author         = {Das, Shubhomoy and Islam, Md Rakibul and Jayakodi, Nitthilan Kannappan and Doppa, Janardhan Rao},
  date           = {2019-01-23},
  journaltitle   = {arXiv e-Print},
  title          = {Active Anomaly Detection via Ensembles: Insights, Algorithms, and Interpretability},
  url            = {https:://arxiv.org/abs/1901.08930},
  urldate        = {2019-03-07},
  abstract       = {Anomaly detection (AD) task corresponds to identifying the true anomalies from a given set of data instances. AD algorithms score the data instances and produce a ranked list of candidate anomalies, which are then analyzed by a human to discover the true anomalies. However, this process can be laborious for the human analyst when the number of false-positives is very high. Therefore, in many real-world AD applications including computer security and fraud prevention, the anomaly detector must be configurable by the human analyst to minimize the effort on false positives. In this paper, we study the problem of active learning to automatically tune ensemble of anomaly detectors to maximize the number of true anomalies discovered. We make four main contributions towards this goal. First, we present an important insight that explains the practical successes of AD ensembles and how ensembles are naturally suited for active learning. Second, we present several algorithms for active learning with tree-based AD ensembles. These algorithms help us to improve the diversity of discovered anomalies, generate rule sets for improved interpretability of anomalous instances, and adapt to streaming data settings in a principled manner. Third, we present a novel algorithm called GLocalized Anomaly Detection (GLAD) for active learning with generic AD ensembles. GLAD allows end-users to retain the use of simple and understandable global anomaly detectors by automatically learning their local relevance to specific data instances using label feedback. Fourth, we present extensive experiments to evaluate our insights and algorithms. Our results show that in addition to discovering significantly more anomalies than state-of-the-art unsupervised baselines, our active learning algorithms under the streaming-data setup are competitive with the batch setup.},
  day            = {23},
  f1000-projects = {QuantInvest},
  groups         = {Anomaly_Detection, ML_Test_FalsePosNeg, ML_Interpretability},
  timestamp      = {2020-05-25 12:09},
}

@Article{Das-et-al-2019c,
  author         = {Das, Mayukh and Yu, Yang and Dhami, Devendra Singh and Kunapuli, Gautam and Natarajan, Sriraam},
  date           = {2019-04-15},
  journaltitle   = {arXiv e-Print},
  title          = {Human-Guided Learning of Column Networks: Augmenting Deep Learning with Advice},
  url            = {https://arxiv.org/abs/1904.06950},
  urldate        = {2019-05-04},
  abstract       = {Recently, deep models have been successfully applied in several applications, especially with low-level representations. However, sparse, noisy samples and structured domains (with multiple objects and interactions) are some of the open challenges in most deep models. Column Networks, a deep architecture, can succinctly capture such domain structure and interactions, but may still be prone to sub-optimal learning from sparse and noisy samples. Inspired by the success of human-advice guided learning in AI, especially in data-scarce domains, we propose Knowledge-augmented Column Networks that leverage human advice/knowledge for better learning with noisy/sparse samples. Our experiments demonstrate that our approach leads to either superior overall performance or faster convergence (i.e., both effective and efficient).},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:09},
}

@Article{Das-et-al-2019b,
  author       = {Sanjiv R. Das and Seoyoung Kim and Bhushan Kothari},
  date         = {2019},
  journaltitle = {The Journal of Financial Data Science},
  title        = {Zero-Revelation RegTech: Detecting Risk through Linguistic Analysis of Corporate Emails and News},
  number       = {2},
  pages        = {8--34},
  url          = {https://jfds.pm-research.com/content/1/2/8},
  volume       = {1},
  abstract     = {Natural language processing is a fast-growing area of data science for the finance industry. The authors demonstrate how an applied linguistics expert system may be used to parse corporate email content and news to assess factors that predict escalating risk or the gradual shifting of other critical characteristics within the firm before they manifest in observable data and financial outcomes. The authors find that email content and news articles meaningfully predict increased risk and potential malaise. The authors also find that other structural characteristics, such as average email length, are strong predictors of risk and subsequent performance. Implementations of three spatial analyses of internal corporate communication, (i.e., email networks, vocabulary trends, and topic analysis) are presented. The authors propose a regulatory technology solution to systematically and effectively detect escalating risk or potential malaise without the need to manually read individual employee emails.},
  timestamp    = {2020-05-25 12:09},
}

@Article{Das-et-al-2019d,
  author         = {Sanjiv R. Das and Seoyoung Kim and Bhushan Kothari},
  date           = {2019-05-01},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Zero-Revelation RegTech: Detecting Risk through Linguistic Analysis of Corporate Emails and News},
  doi            = {10.3905/jfds.2019.1.2.008},
  issn           = {2640-3951},
  url            = {https://jfds.pm-research.com/content/1/2/8},
  urldate        = {2019-11-28},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:09},
}

@Article{Cao-et-al-2018a,
  author         = {Cao, Nan and Lin, Chaoguang and Zhu, Qiuhan and Lin, Yu-Ru and Teng, Xian and Wen, Xidao},
  date           = {2018-01},
  journaltitle   = {IEEE Trans Vis Comput Graph},
  title          = {Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data.},
  doi            = {10.1109/{TVCG}.2017.2744419},
  number         = {1},
  pages          = {23--33},
  volume         = {24},
  abstract       = {The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the "smart city" as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.},
  f1000-projects = {QuantInvest},
  groups         = {Anomaly_Detection},
  pmid           = {28866547},
  timestamp      = {2020-05-25 12:10},
}

@InCollection{Wu-2016a,
  author               = {Wu, Hu-Sheng},
  booktitle            = {2016 13th International Computer Conference on Wavelet Active Media Technology and Information Processing (ICCWAMTIP)},
  date                 = {2016-12},
  title                = {A survey of research on anomaly detection for time series},
  doi                  = {10.1109/iccwamtip.2016.8079887},
  isbn                 = {978-1-5090-6126-6},
  location             = {Chengdu, China},
  pages                = {426--431},
  publisher            = {IEEE},
  abstract             = {Abstract: Time series is an important class of temporal data objects and it can be easily obtained from scientific and financial applications, and anomaly detection for time series is becoming a hot research topic recently. This survey tries to provide a structured and comprehensive overview of the research on anomaly detection. In this paper, we have discussed the definition of anomaly and grouped existing techniques into different categories based on the underlying approach adopted by each technique. And for each category, we identify the advantages and disadvantages of the techniques in that category. Then, we provide a briefly discussion on the representative methods recently. Furthermore, we also point out some key issues about multivariate time series anomaly. Finally, some suggestions about anomaly detection are discussed and future research trends are also summarized, which is hopefully beneficial to the researchers of time series and other relative domains.},
  citeulike-article-id = {14527397},
  citeulike-linkout-0  = {http://dx.doi.org/10.1109/iccwamtip.2016.8079887},
  groups               = {Anomaly_Detection},
  posted-at            = {2018-01-31 10:38:33},
  timestamp            = {2020-05-25 12:10},
}

@Article{Belghazi-et-al-2018,
  author         = {Belghazi, Mohamed Ishmael and Rajeswar, Sai and Mastropietro, Olivier and Rostamzadeh, Negar and Mitrovic, Jovana and Courville, Aaron},
  date           = {2018-02-04},
  journaltitle   = {arXiv e-Print},
  title          = {Hierarchical Adversarially Learned Inference},
  url            = {https://arxiv.org/abs/1802.01071},
  urldate        = {2019-10-18},
  abstract       = {We propose a novel hierarchical generative model with a simple Markovian structure and a corresponding inference model. Both the generative and inference model are trained using the adversarial learning paradigm. We demonstrate that the hierarchical structure supports the learning of progressively more abstract representations as well as providing semantically meaningful reconstructions with different levels of fidelity. Furthermore, we show that minimizing the Jensen-Shanon divergence between the generative and inference network is enough to minimize the reconstruction error. The resulting semantically meaningful hierarchical latent structure discovery is exemplified on the CelebA dataset. There, we show that the features learned by our model in an unsupervised way outperform the best handcrafted features. Furthermore, the extracted features remain competitive when compared to several recent deep supervised approaches on an attribute prediction task on CelebA. Finally, we leverage the model's inference network to achieve state-of-the-art performance on a semi-supervised variant of the MNIST digit classification task.},
  day            = {4},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:12},
}

@Article{Chen-Vorobeychik-2018,
  author         = {Chen, Yifan and Vorobeychik, Yevgeniy},
  date           = {2018-12-05},
  journaltitle   = {arXiv e-Print},
  title          = {Regularized Ensembles and Transferability in Adversarial Learning},
  url            = {https://arxiv.org/abs/1812.01821},
  urldate        = {2019-10-18},
  abstract       = {Despite the considerable success of convolutional neural networks in a broad array of domains, recent research has shown these to be vulnerable to small adversarial perturbations, commonly known as adversarial examples. Moreover, such examples have shown to be remarkably portable, or transferable, from one model to another, enabling highly successful black-box attacks. We explore this issue of transferability and robustness from two dimensions: first, considering the impact of conventional lp regularization as well as replacing the top layer with a linear support vector machine (SVM), and second, the value of combining regularized models into an ensemble. We show that models trained with different regularizers present barriers to transferability, as does partial information about the models comprising the ensemble.},
  day            = {5},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:13},
}

@Article{Fidel-et-al-2019,
  author         = {Fidel, Gil and Bitton, Ron and Shabtai, Asaf},
  date           = {2019-09-08},
  journaltitle   = {arXiv e-Print},
  title          = {When Explainability Meets Adversarial Learning: Detecting Adversarial Examples using SHAP Signatures},
  url            = {https://arxiv.org/abs/1909.03418},
  urldate        = {2019-10-11},
  abstract       = {State-of-the-art deep neural networks (DNNs) are highly effective in solving many complex real-world problems. However, these models are vulnerable to adversarial perturbation attacks, and despite the plethora of research in this domain, to this day, adversaries still have the upper hand in the cat and mouse game of adversarial example generation methods vs. detection and prevention methods. In this research, we present a novel detection method that uses Shapley Additive Explanations (SHAP) values computed for the internal layers of a DNN classifier to discriminate between normal and adversarial inputs. We evaluate our method by building an extensive dataset of adversarial examples over the popular CIFAR-10 and MNIST datasets, and training a neural network-based detector to distinguish between normal and adversarial inputs. We evaluate our detector against adversarial examples generated by diverse state-of-the-art attacks and demonstrate its high detection accuracy and strong generalization ability to adversarial inputs generated with different attack methods.},
  day            = {8},
  f1000-projects = {QuantInvest},
  groups         = {ML_Explain},
  timestamp      = {2020-05-25 12:13},
}

@Article{Hamm-Noh-2018,
  author         = {Hamm, Jihun and Noh, Yung-Kyun},
  date           = {2018-05-29},
  journaltitle   = {arXiv e-Print},
  title          = {K-Beam Minimax: Efficient Optimization for Deep Adversarial Learning},
  url            = {https://arxiv.org/abs/1805.11640},
  urldate        = {2019-10-18},
  abstract       = {Minimax optimization plays a key role in adversarial training of machine learning algorithms, such as learning generative models, domain adaptation, privacy preservation, and robust learning. In this paper, we demonstrate the failure of alternating gradient descent in minimax optimization problems due to the discontinuity of solutions of the inner maximization. To address this, we propose a new epsilon-subgradient descent algorithm that addresses this problem by simultaneously tracking K candidate solutions. Practically, the algorithm can find solutions that previous saddle-point algorithms cannot find, with only a sublinear increase of complexity in K. We analyze the conditions under which the algorithm converges to the true solution in detail. A significant improvement in stability and convergence speed of the algorithm is observed in simple representative problems, GAN training, and domain-adaptation problems.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:13},
}

@Article{Kumar-et-al-2019,
  author         = {Kumar, Devinder and Ben-Daya, Ibrahim and Vats, Kanav and Feng, Jeffery and And, Graham Taylor and Wong, Alexander},
  date           = {2019-04-21},
  journaltitle   = {arXiv e-Print},
  title          = {Beyond Explainability: Leveraging Interpretability for Improved Adversarial Learning},
  url            = {https://arxiv.org/abs/1904.09633},
  urldate        = {2019-10-18},
  abstract       = {In this study, we propose the leveraging of interpretability for tasks beyond purely the purpose of explainability. In particular, this study puts forward a novel strategy for leveraging gradient-based interpretability in the realm of adversarial examples, where we use insights gained to aid adversarial learning. More specifically, we introduce the concept of spatially constrained one-pixel adversarial perturbations, where we guide the learning of such adversarial perturbations towards more susceptible areas identified via gradient-based interpretability. Experimental results using different benchmark datasets show that such a spatially constrained one-pixel adversarial perturbation strategy can noticeably improve the speed of convergence as well as produce successful attacks that were also visually difficult to perceive, thus illustrating an effective use of interpretability methods for tasks outside of the purpose of purely explainability.},
  day            = {21},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:13},
}

@InProceedings{Miller-et-al-2017a,
  author         = {Miller, D. J. and Hu, X. and Qiu, Z. and Kesidis, G.},
  booktitle      = {IEEE 27th International Workshop on Machine Learning for Signal Processing (MLSP)},
  date           = {2017-09-25},
  title          = {Adversarial learning: A critical review and active learning study},
  doi            = {10.1109/{MLSP}.2017.8168163},
  isbn           = {978-1-5090-6341-3},
  pages          = {1--6},
  publisher      = {IEEE},
  url            = {http://ieeexplore.ieee.org/document/8168163/},
  urldate        = {2019-10-18},
  abstract       = {This papers consists of two parts. The first is a critical review of prior art on adversarial learning, i) identifying some significant limitations of previous works, which have focused mainly on attack exploits and ii) proposing novel defenses against adversarial attacks. The second part is an experimental study considering the adversarial active learning scenario and an investigation of the efficacy of a mixed sample selection strategy for combating an adversary who attempts to disrupt the classifier learning.},
  day            = {25},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-05-25 12:15},
}

@Article{Bai-et-al-2019b,
  author         = {Bai, Lu and Cui, Lixin and Xu, Lixiang and Wang, Yue and Zhang, Zhihong and Hancock, Edwin R.},
  date           = {2019-10-21},
  journaltitle   = {arXiv e-Print},
  title          = {Entropic Dynamic Time Warping Kernels for Co-evolving Financial Time Series Analysis},
  url            = {https://arxiv.org/abs/1910.09153},
  urldate        = {2019-10-24},
  abstract       = {In this work, we develop a novel framework to measure the similarity between dynamic financial networks, i.e., time-varying financial networks. Particularly, we explore whether the proposed similarity measure can be employed to understand the structural evolution of the financial networks with time. For a set of time-varying financial networks with each vertex representing the individual time series of a different stock and each edge between a pair of time series representing the absolute value of their Pearson correlation, our start point is to compute the commute time matrix associated with the weighted adjacency matrix of the network structures, where each element of the matrix can be seen as the enhanced correlation value between pairwise stocks. For each network, we show how the commute time matrix allows us to identify a reliable set of dominant correlated time series as well as an associated dominant probability distribution of the stock belonging to this set. Furthermore, we represent each original network as a discrete dominant Shannon entropy time series computed from the dominant probability distribution. With the dominant entropy time series for each pair of financial networks to hand, we develop a similarity measure based on the classical dynamic time warping framework, for analyzing the financial time-varying networks. We show that the proposed similarity measure is positive definite and thus corresponds to a kernel measure on graphs. The proposed kernel bridges the gap between graph kernels and the classical dynamic time warping framework for multiple financial time series analysis. Experiments on time-varying networks extracted through New York Stock Exchange (NYSE) database demonstrate the effectiveness of the proposed approach.},
  day            = {21},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:18},
}

@Article{Echihabi-et-al-2018,
  author         = {Echihabi, Karima and Zoumpatianos, Kostas and Palpanas, Themis and Benbrahim, Houda},
  date           = {2018-10-01},
  journaltitle   = {Proc. VLDB Endow.},
  title          = {The lernaean hydra of data series similarity search},
  doi            = {10.14778/3282495.3282498},
  issn           = {2150-8097},
  number         = {2},
  pages          = {112--127},
  volume         = {12},
  abstract       = {Increasingly large data series collections are becoming commonplace across many different domains and applications. A key operation in the analysis of data series collections is similarity search, which has attracted lots of attention and effort over the past two decades. Even though several relevant approaches have been proposed in the literature, none of the existing studies provides a detailed evaluation against the available alternatives. The lack of comparative results is further exacerbated by the non-standard use of terminology, which has led to confusion and misconceptions. In this paper, we provide definitions for the different flavors of similarity search that have been studied in the past, and present the first systematic experimental evaluation of the efficiency of data series similarity search techniques. Based on the experimental results, we describe the strengths and weaknesses of each approach and give recommendations for the best approach to use under typical use cases. Finally, by identifying the shortcomings of each method, our findings lay the ground for solid further developments in the field.},
  day            = {1},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:19},
}

@Article{Kang-et-al-2019a,
  author         = {Kang, Yanfei and Spiliotis, Evangelos and Petropoulos, Fotios and Athiniotis, Nikolaos and Li, Feng and Assimakopoulos, Vassilios},
  date           = {2019-08-31},
  journaltitle   = {arXiv e-Print},
  title          = {Deja vu: forecasting with similarity},
  url            = {https://arxiv.org/abs/1909.00221},
  urldate        = {2019-09-11},
  abstract       = {Accurate forecasts are vital for supporting the decisions of modern companies. In order to improve statistical forecasting performance, forecasters typically select the most appropriate model for each data. However, statistical models presume a data generation process, while making strong distributional assumptions about the errors. In this paper, we present a new approach to time series forecasting that relaxes these assumptions. A target series is forecasted by identifying similar series from a reference set (deja vu). Then, instead of extrapolating, the future paths of the similar reference series are aggregated and serve as the basis for the forecasts of the target series. Thus, forecasting with similarity is a data-centric approach that tackles model uncertainty without depending on statistical forecasting models. We evaluate the approach using a rich collection of real data and show that it results in good forecasting accuracy, especially for yearly series.},
  day            = {31},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:21},
}

@Article{Madan-Schoutens-2018,
  author         = {Madan, Dilip B. and Schoutens, Wim},
  date           = {2018},
  journaltitle   = {SSRN e-Print},
  title          = {Selfsimilarity in long horizon asset returns},
  doi            = {10.2139/ssrn.3102406},
  issn           = {1556-5068},
  url            = {https://ssrn.com/abstract=3102406},
  urldate        = {2019-04-21},
  abstract       = {Daily return distributions are modeled by pure jump limit laws that are selfdecomposable laws. The returns may be seen as composed of a sum of independent and identically distributed increments or as a selfsimilar law scaling the sum of exponentially weighted past shocks or a combination thereof. To the extent the selfsimilar component is present and the scaling coefficient is above a half it is shown that long horizon returns may not converge to a normal distribution. Estimations conducted on 214 equity underliers over the period January 2007 to February 2017 support this lack of convergence to normality at very long horizons. An analysis of distributions embedded in option data shows that the convergence to normality is also halted risk neutrally. Selfsimilar components are estimated to have a physical half life between one or two days and a risk neutral half life around a year. In the long run markets are in an equilibrium state of motion engineered to avoid the evolution of good deals. The associated equilibrium solutions are illustrated. The implications of a selfsimilar scaling component for the equity bias, volatility and desirability of returns across horizons, horizon effects on expected returns, and Sharpe ratios are developed. Additionally long horizon return modeling is employed to construct alternative long horizon risk free rates using volatility targets.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:21},
}

@Article{Munnix-et-al-2014,
  author               = {Munnix, M. C. and Schafer, R. and Grothe, O.},
  date                 = {2014-05},
  journaltitle         = {Quantitative Finance},
  title                = {Estimating correlation and covariance matrices by weighting of market similarity},
  doi                  = {10.1080/14697688.2011.605075},
  number               = {5},
  pages                = {931--939},
  volume               = {14},
  abstract             = {We discuss a weighted estimation of correlation and covariance matrices from historical financial data. To this end, we introduce a weighting scheme that accounts for the similarity of previous market conditions to the present situation. The resulting estimators are less biased and show lower variance than either unweighted or exponentially weighted estimators. The weighting scheme is based on a similarity measure that compares the current correlation structure of the market to the structures at past times. Similarity is then measured by the matrix 2-norm of the difference of probe correlation matrices estimated for two different points in time. The method is validated in a simulation study and tested empirically in the context of mean?variance portfolio optimization. In the latter case we find an enhanced realized portfolio return as well as a reduced portfolio risk compared with alternative approaches based on different strategies and estimators.},
  citeulike-article-id = {13990212},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2011.605075},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2011.605075},
  day                  = {4},
  owner                = {cristi},
  posted-at            = {2016-03-29 11:29:51},
  publisher            = {Routledge},
  timestamp            = {2020-09-08 13:22},
}

@Article{Sidi-2020,
  author         = {Sidi, Lior},
  date           = {2020-02-08},
  journaltitle   = {arXiv e-Print},
  title          = {Improving S\&P stock prediction with time series stock similarity},
  url            = {https://arxiv.org/abs/2002.05784v1},
  urldate        = {2020-03-06},
  abstract       = {Stock market prediction with forecasting algorithms is a popular topic these days where most of the forecasting algorithms train only on data collected on a particular stock. In this paper, we enriched the stock data with related stocks just as a professional trader would have done to improve the stock prediction models. We tested five different similarities functions and found co-integration similarity to have the best improvement on the prediction model. We evaluate the models on seven S\&P stocks from various industries over five years period. The prediction model we trained on similar stocks had significantly better results with 0.55 mean accuracy, and 19.782 profit compare to the state of the art model with an accuracy of 0.52 and profit of 6.6.},
  day            = {8},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:22},
}

@Article{Yan-et-al-2018a,
  author         = {Yan, Gaowei and Jia, Songda and Ding, Jie and Xu, Xinying and Pang, Yusong},
  date           = {2019},
  journaltitle   = {Soft computing},
  title          = {A time series forecasting based on cloud model similarity measurement},
  issn           = {1432-7643},
  pages          = {5443-5454},
  url            = {https://link.springer.com/article/10.1007/s00500-018-3190-1},
  urldate        = {2019-09-11},
  volume         = {23},
  abstract       = {In this paper, a local cloud model similarity measurement (CMSM) is proposed as a novel method to measure the similarity of time series. Time series similarity measurement is an indispensable part for improving the efficiency and accuracy of prediction. The randomness and uncertainty of series data are critical problems in the processing of similarity measurement. CMSM obtains the internal information of time series from the general perspective and local trend using the cloud model, which reduces the uncertainty of measurement. The neighbor set is selected from time series by CMSM and used to construct a prediction model based on least squares support vector machine. The proposed technique reduces the potential for overfitting and uncertainty and improves model prediction quality and generalization. Experiments were performed with four datasets selected from Time Series Data Library. The experimental results show the feasibility and effectiveness of the proposed method.},
  day            = {13},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:23},
}

@Article{Yaros-Imielinski-2015,
  author               = {Yaros, John R. and Imielinski, Tomasz},
  date                 = {2015-10},
  journaltitle         = {Quantitative Finance},
  title                = {Data-driven methods for equity similarity prediction},
  doi                  = {10.1080/14697688.2015.1071079},
  number               = {10},
  pages                = {1657--1681},
  volume               = {15},
  abstract             = {Many applications rely on the accurate prediction of company similarity to be effective. Diversification avoids similarity for risk reduction. Hedging through equity-neutral investing seeks similarity in order to ensure risk in long positions is effectively offset by short positions, and vice-versa. Relative valuation requires formation of a ?peer group? to which financial ratios, such as the P/E ratio, can be compared. This article considers two data-sets that have not traditionally been used for this purpose: sell-side equity analyst coverage and news article co-occurrences. Each is shown to have predictive power over future correlation, a key measure of future similarity. It is further shown that the analyst and news data can be combined with historical correlation to form groups that are on par or even exceed the quality of the Global Industry Classification System, a leading industry taxonomy.},
  citeulike-article-id = {14444515},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2015.1071079},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2015.1071079},
  day                  = {3},
  posted-at            = {2017-10-03 10:09:39},
  publisher            = {Routledge},
  timestamp            = {2020-09-08 13:23},
}

@Article{Czasonis-et-al-2020a,
  author         = {Czasonis, Megan and Kritzman, Mark and Pamir, Baykan and Turkington, David},
  date           = {2020-02-29},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Enhanced Scenario Analysis},
  doi            = {10.3905/jpm.2020.1.125},
  issn           = {0095-4918},
  number         = {4},
  pages          = {69--79},
  urldate        = {2020-03-07},
  volume         = {46},
  abstract       = {Investors have long relied on scenario analysis as an alternative to mean-variance analysis to help them construct portfolios. Even though mean-variance analysis accounts for all potential scenarios, many investors find it difficult to implement because it requires them to specify statistical features of asset classes that are often unintuitive and difficult to estimate. Scenario analysis, by contrast, requires only that investors specify a small set of potential outcomes as projections of economic variables and assign probabilities to their occurrence. It is, therefore, more intuitive than mean-variance analysis, but it is highly subjective. In this article, the authors propose to replace the subjective elements of scenario analysis with a robust statistical process. They use a multivariate measure of statistical distance to estimate probabilities of prospective scenarios. Next, they construct portfolios that maximize utility for investors with different risk preferences. Last, the authors introduce a procedure for minimally modifying scenarios to render them consistent with prespecified views about their probabilities of occurrence.},
  day            = {29},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:23},
}

@Article{Meucci-2009c,
  author               = {Meucci, Attilio},
  date                 = {2009-06},
  journaltitle         = {Journal of Asset Management},
  title                = {Enhancing the Black Litterman and related approaches: Views and stress-test on risk factors},
  doi                  = {10.1057/jam.2008.42},
  issn                 = {1470-8272},
  number               = {2},
  pages                = {89--96},
  volume               = {10},
  abstract             = {The Black Litterman and related approaches modify the return distribution of a normally distributed market according to views or stress-test scenarios. We discuss how to broaden the range of applications of these approaches significantly by letting them act on the risk factors underlying the market, instead of the returns of the securities.},
  citeulike-article-id = {4760663},
  citeulike-linkout-0  = {http://dx.doi.org/10.1057/jam.2008.42},
  citeulike-linkout-1  = {http://www.ingentaconnect.com/content/pal/jam/2009/00000010/00000002/art00003},
  groups               = {Scenario generation, Black_Litterman, Risk_Stress},
  owner                = {cristi},
  posted-at            = {2016-03-12 20:15:10},
  publisher            = {Palgrave Macmillan},
  timestamp            = {2020-09-08 13:24},
}

@Article{Yu-et-al-2020a,
  author         = {Yu, Philip L. H. and Ng, F. C. and Ting, Jessica K. W.},
  date           = {2020-04-20},
  journaltitle   = {Quantitative Finance},
  title          = {Adjusting covariance matrix for risk management},
  issn           = {1469-7688},
  pages          = {In Press},
  url            = {https://www.tandfonline.com/doi/full/10.1080/14697688.2020.1739737},
  urldate        = {2020-05-03},
  abstract       = {The covariance matrix of asset returns can change drastically and generate huge losses in portfolio value under extreme conditions such as market interventions and financial crises. Estimation of the covariance matrix under a chaotic market is often a call to action in risk management. Nowadays, stress testing has become a standard procedure for many financial institutions to estimate the capital requirement for their portfolio holdings under various stress scenarios. A possible stress scenario is to adjust the covariance matrix to mimic the situation under an underlying stress event. It is reasonable that when some covariances are altered, other covariances should vary as well. Recently, Ng et al. proposed a unified approach to determine a proper correlation matrix which reflects the subjective views of correlations. However, this approach requires matrix vectorization and hence it is not computationally efficient for high dimensional matrices. Besides, it only adjusts correlations, but it is well known that high correlations often go together with high standard deviations during a crisis period. To address these limitations, we propose a Bayesian approach to covariance matrix adjustment by incorporating subjective views of covariances. Our approach is computationally efficient and can be applied to high dimensional matrices.},
  day            = {20},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 13:25},
}

@TechReport{Raman-et-al-2019,
  author         = {Natraj Raman and Jochen L. Leidner and Krishnen Vytelingum and Geoffrey Horrell},
  date           = {2019},
  institution    = {Simudyne},
  title          = {Synthetic Reality: Synthetic market data generation at scale using agent based modeling},
  type           = {techreport},
  url            = {https://simudyne.com/resources/synthetic-reality-synthetic-market-data-generation-at-scale/},
  urldate        = {2020-04-26},
  abstract       = {Agent Based Modeling (ABM) offers advances over traditional model testing applications in the financial markets. This paper, created in partnership with Refinitiv, introduces the topic of Agent Based Modeling and illustrates its application in a number of scenarios.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_SynthData},
  timestamp      = {2020-09-08 13:25},
}

@Article{Haldane-Turrell-2018a,
  author         = {Haldane, Andrew G. and Turrell, Arthur E.},
  date           = {2018-03-26},
  journaltitle   = {Journal of Evolutionary Economics},
  title          = {Drawing on different disciplines: macroeconomic agent-based models},
  doi            = {10.1007/s00191-018-0557-5},
  issn           = {0936-9937},
  number         = {1},
  pages          = {1--28},
  urldate        = {2020-04-27},
  volume         = {29},
  abstract       = {Macroeconomic modelling has been under intense scrutiny since the Great Financial Crisis, when serious shortcomings were exposed in the methodology used to understand the economy as a whole. Criticism has been levelled at the assumptions employed in the dominant models, particularly that economic agents are homogenous and optimising and that the economy is equilibrating. In a related paper (Haldane and Turrell Oxford Rev Econ Polic 34(1-2):219-251 2018), we argue that an interdisciplinary approach to modelling in macroeconomics is beneficial. Here we focus on what one such approach - agent-based modelling, which has been extensively used across a wide range of disciplines - could do for macroeconomics. Agent-based models are complementary to existing approaches to macroeconomics and are particularly well-suited to answering questions where complexity, heterogeneity, networks, and heuristics play an important role.},
  day            = {26},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ABM},
  timestamp      = {2020-09-08 13:26},
}

@Article{Beikirch-et-al-2018,
  author         = {Beikirch, Maximilian and Cramer, Simon and Frank, Martin and Otte, Philipp and Pabich, Emma and Trimborn, Torsten},
  date           = {2018-11-27},
  journaltitle   = {arXiv e-Print},
  title          = {Simulation of Stylized Facts in Agent-Based Computational Economic Market Models},
  url            = {https://arxiv.org/abs/1812.02726},
  urldate        = {2020-04-26},
  abstract       = {We study the qualitative and quantitative appearance of stylized facts in several agent-based computational economic market (ABCEM) models. We perform our simulations with the SABCEMM (Simulator for Agent-Based Computational Economic Market Models) tool recently introduced by the authors (Trimborn et al. 2019). Furthermore, we present novel ABCEM models created by recombining existing models and study them with respect to stylized facts as well. This can be efficiently performed by the SABCEMM tool thanks to its object-oriented software design. The code is available on GitHub (Trimborn et al. 2018), such that all results can be reproduced by the reader.},
  day            = {27},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ABM},
  timestamp      = {2020-09-08 13:26},
}

@InCollection{Bouchaud-2018,
  author         = {Bouchaud, Jean-Philippe},
  booktitle      = {Handbook of Computational Economics},
  date           = {2018},
  title          = {Agent-based models for market impact and volatility},
  doi            = {10.1016/bs.hescom.2018.02.002},
  publisher      = {Elsevier},
  url            = {https://www.sciencedirect.com/science/article/pii/S1574002118300029},
  abstract       = {Financial markets display a host of universal facts begging for a scientific explanation: Excess volatility, fat tails, and clustered activity are well known and have been studied for many years. More microstructural stylized facts have recently emerged, for example the long memory of the order flow or the square-root dependence of impact on the volume of metaorders. Agent-based models are attempts to account for these stylized facts in a unified manner. Devising faithful microstructural ABMs would allow one to answer crucial questions, such as those related to market stability. Can large orders destabilize markets? Is HFT activity detrimental? Can destabilizing feedback loops be mitigated by adequate regulation? The present review paper summarizes recent work in that direction. We discuss in particular the Santa-Fe zero-intelligence model, which provides a very interesting benchmark, but suffers from important drawbacks as well, such as strong mean-reversion effects. One can enrich the Santa-Fe model as to reproduce both diffusive prices, and the square-root impact law. The underlying mechanism can be well understood in terms of a generic -diffusion model for the dynamics of the liquidity, which can be solved analytically. Finally, we argue that the role of is probably overstated in classical theories, while a picture based on a self-reflexive price-impacting order flow has many merits. The recent accumulation of microstructural stylized facts, allowing one to focus on the price formation mechanism, all but confirm that fundamental information plays a relatively minor role in the dynamics of financial markets, at least on short to medium time scales.},
  f1000-projects = {QuantInvest},
  issn           = {1574-0021},
  timestamp      = {2020-09-08 13:26},
}

@Book{Chakrabarti-et-al-2019,
  date           = {2019},
  title          = {Network Theory and Agent-Based Modeling in Economics and Finance},
  doi            = {10.1007/978-981-13-8319-9},
  editor         = {Chakrabarti, Anindya S. and Pichl, Lukas and Kaizoji, Taisei},
  publisher      = {Springer Singapore},
  url            = {https://www.springer.com/gp/book/9789811383182},
  urldate        = {2020-04-27},
  abstract       = {his book presents the latest findings on network theory and agent-based modeling of economic and financial phenomena. In this context, the economy is depicted as a complex system consisting of heterogeneous agents that interact through evolving networks; the aggregate behavior of the economy arises out of billions of small-scale interactions that take place via countless economic agents. The book focuses on analytical modeling, and on the econometric and statistical analysis of the properties emerging from microscopic interactions. In particular, it highlights the latest empirical and theoretical advances, helping readers understand economic and financial networks, as well as new work on modeling behavior using rich, agent-based frameworks.

Innovatively, the book combines observational and theoretical insights in the form of networks and agent-based models, both of which have proved to be extremely valuable in understanding non-linear and evolving complex systems. Given its scope, the book will capture the interest of graduate students and researchers from various disciplines (e.g. economics, computer science, physics, and applied mathematics) whose work involves the domain of complexity theory.},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ABM},
  timestamp      = {2020-09-08 13:27},
}

@Article{Cramer-Trimborn-2019,
  author         = {Cramer, Simon and Trimborn, Torsten},
  date           = {2019-12-02},
  journaltitle   = {arXiv e-Print},
  title          = {Stylized Facts and Agent-Based Modeling},
  url            = {https://arxiv.org/abs/1912.02684},
  urldate        = {2020-04-26},
  abstract       = {The existence of stylized facts in financial data has been documented in many studies. In the past decade the modeling of financial markets by agent-based computational economic market models has become a frequently used modeling approach. The main purpose of these models is to replicate stylized facts and to identify sufficient conditions for their creations. In this paper we introduce the most prominent examples of stylized facts and especially present stylized facts of financial data. Furthermore, we given an introduction to agent-based modeling. Here, we not only provide an overview of this topic but introduce the idea of universal building blocks for agent-based economic market models.},
  day            = {2},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ABM},
  timestamp      = {2020-09-08 13:27},
}

@InCollection{Raman-Leidner-2019,
  author    = {Natraj Raman and Jochen L. Leidner},
  booktitle = {Advances in Practical Applications of Survivable Agents and Multi-Agent Systems: The {PAAMS} Collection},
  date      = {2019},
  title     = {Financial Market Data Simulation Using Deep Intelligence Agents},
  doi       = {10.1007/978-3-030-24209-1_17},
  pages     = {200--211},
  publisher = {Springer International Publishing},
  abstract  = {Trading strategies are often assessed against historical financial data in an effort to predict the profits and losses a strategy would generate in future. However, using only data from the past ignores the evolution of market microstructure and does not account for market conditions outside historical bounds. Simulations provide an effective supplement. We present an agent-based model to simulate financial market prices both under steady-state conditions and stress situations. Our new class of agents utilize recent advances in deep learning to make trading decisions and employ different trading objectives to ensure diversity in outcomes. The model supports various what-if scenarios such as sudden price crash, bearish or bullish market sentiment and shock contagion. We conduct evaluations on multiple asset classes including portfolio of assets and illustrate that the proposed agent decision mechanism outperforms other techniques. Our simulation model also successfully replicates the empirical stylized facts of financial markets.},
  timestamp = {2020-09-08 13:28},
  year      = {2019},
}

@Article{Pesenti-et-al-2020,
  author         = {Pesenti, Silvana M. and Bettini, Alberto and Millossovich, Pietro and Tsanakas, Andreas},
  date           = {2020},
  journaltitle   = {SSRN e-Print},
  title          = {Scenario weights for importance measurement (SWIM) - an R package for sensitivity analysis},
  doi            = {10.2139/ssrn.3515274},
  issn           = {1556-5068},
  url            = {https://www.ssrn.com/abstract=3515274},
  urldate        = {2020-03-06},
  abstract       = {The SWIM package implements a flexible sensitivity analysis framework, based primarily on results and tools developed by Pesenti et al. (2019). SWIM provides a stressed version of a stochastic model, subject to model components (random variables) fulfilling given probabilistic constraints (stresses). Possible stresses can be applied on moments, probabilities of given events, and risk measures such as Value-at-Risk and Expected Shortfall. SWIM operates upon a single set of simulated scenarios from a stochastic model, returning scenario weights, which encode the required stress and allow monitoring the impact of the stress on all model components. The scenario weights are calculated to minimise the relative entropy with respect to the baseline model, subject to the stress applied. As well as calculating scenario weights, the package provides tools for the analysis of stressed models, including plotting facilities and evaluation of sensitivity measures. SWIM does not require additional evaluations of the simulation model or explicit knowledge of its underlying statistical and functional relations; hence it is suitable for the analysis of black box models. The capabilities of SWIM are demonstrated through a case study of a credit portfolio model.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:25},
}

@Article{vanBeek-2020,
  author         = {{van Beek}, Misha},
  date           = {2020-04-20},
  journaltitle   = {arXiv e-Print},
  title          = {Consistent Calibration of Economic Scenario Generators: The Case for Conditional Simulation},
  url            = {https://arxiv.org/abs/2004.09042},
  urldate        = {2020-05-02},
  abstract       = {Economic Scenario Generators (ESGs) simulate economic and financial variables forward in time for risk management and asset allocation purposes. It is often not feasible to calibrate the dynamics of all variables within the ESG to historical data alone. Calibration to forward-information such as future scenarios and return expectations is needed for stress testing and portfolio optimization, but no generally accepted methodology is available. This paper introduces the Conditional Scenario Simulator, which is a framework for consistently calibrating simulations and projections of economic and financial variables both to historical data and forward-looking information. The framework can be viewed as a multi-period, multi-factor generalization of the Black-Litterman model, and can embed a wide array of financial and macroeconomic models. Two practical examples demonstrate this in a frequentist and Bayesian setting.},
  day            = {20},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:25},
}

@Article{Bilgili-et-al-2016,
  author               = {Bilgili, Mehmet S. and Ferconi, Maurizio and Ulitsky, Alex},
  date                 = {2017},
  journaltitle         = {Risk},
  title                = {Stress Hedging in Portfolio Construction},
  url                  = {https://www.risk.net/risk-management/5287961/stress-hedging-in-portfolio-construction},
  abstract             = {Scenario stress testing is a useful and increasingly popular approach to assess portfolio performance under different market conditions. In this paper we focus on how to incorporate stress scenario information directly in portfolio construction as additional constraints to control for potential losses and risks. To broaden the applicability of stress testing we propose robust constrained optimization approach for handling uncertainty in scenario parameters. An example of the -- Oil-Crisis-- event is used as a numerical illustration.},
  citeulike-article-id = {14146740},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2688772},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2837139code1603923.pdf?abstractid=2688772 and mirid=1},
  day                  = {12},
  owner                = {cristi},
  posted-at            = {2016-09-26 21:16:37},
  timestamp            = {2020-09-08 14:26},
}

@Article{Denev-Mutnikas-2016a,
  author               = {Denev, A. and Mutnikas, Y.},
  date                 = {2016},
  journaltitle         = {Risk Management},
  title                = {A formalized, integrated and visual approach to stress testing},
  doi                  = {10.1057/s41283-016-0009-1},
  pages                = {189-216},
  url                  = {https://link.springer.com/article/10.1057/s41283-016-0009-1},
  volume               = {18},
  abstract             = {In this paper we will give for the first time a formal mathematical language to the steps used currently by financial institutions when calculating the impact of a stress scenario on a balance sheet that depends on more granular or different factors than those provided in the scenario. We will introduce the language of Probabilistic Graphical Models (PGM) - a technique rooted in machine learning - to show how the different models used at each step can be put together in a coherent picture thus giving a holistic view of the entire models setup. This will give us a solid basis to discuss some weaknesses and problems with the stress testing exercises run by the industry as of today. We will show empirical analyses to substantiate better some of our claims.},
  citeulike-article-id = {14487862},
  groups               = {Test_Scenario},
  posted-at            = {2017-12-03 22:13:19},
  timestamp            = {2020-09-08 14:26},
}

@Article{Ardia-Meucci-2015,
  author       = {David Ardia and Attilio Meucci},
  date         = {2015},
  journaltitle = {Risk},
  title        = {Stress testing in non-normal markets via entropy pooling},
  url          = {https://www.risk.net/risk-management/asset-liability-management/2410967/stress-testing-non-normal-markets-entropy},
  abstract     = {The combination of trading signals, or general views on the market, within a prior risk model to compute an optimal allocation that incorporates these views is one of the main challenges in quantitative portfolio construction. Similarly, embedding stress tests in a risk model in a statistically sound way is key to a healthy risk management process.},
  owner        = {zkgst0c},
  timestamp    = {2020-09-08 14:26},
}

@TechReport{Meucci-2012,
  author      = {Attilio Meucci},
  date        = {2012},
  institution = {EDHEC},
  title       = {A Fully Integrated Liquidity and Market Risk Model},
  url         = {https://risk.edhec.edu/sites/risk/files/edhec-working-paper-a-fully-integrated-liquidity_1355308374697.pdf},
  abstract    = {We introduce a new framework to integrate liquidity risk, funding risk and market risk, which goes beyond the simple bid-ask spread overlay to a VaR number. In our approach, we overlay a whole distribution of liquidity uncertainty to each future market-risk scenario. Then we allow for the liquidity uncertainty to vary scenario by scenario, depending on what liquidation policy or funding policy is implemented in that scenario.

The result is one easy-to-interpret and easy-to-implement formula for the total liquidity-plus-market-risk P\&L distribution.

Using this formula we can stress-test different market risk P\&L distributions and different scenario-dependent liquidation policies and funding policies; compute total risk and decompose it into a novel liquidity-plus-market risk formula; and define a liquidity score as a monetary measure of portfolio liquidity.

Our approach relies on three pillars: first, the literature on optimal execution, to model liquidity risk as a function of the actual trading involved; second, an analytical conditional convolution, to blend market risk and liquidity/funding risk; third the Fully Flexible Probabilities framework, to model and stress-test market risk even in highly non-normal portfolios with complex derivatives.

Our approach can be implemented efficiently with portfolios of thousand of securities. The code for the case study is available for download},
  timestamp   = {2020-09-08 14:27},
}

@InCollection{Overbeck-2012,
  author               = {Overbeck, Ludger},
  booktitle            = {Handbook of Computational Finance},
  date                 = {2012},
  title                = {Computational Issues in Stress Testing},
  doi                  = {10.1007/978-3-642-17254-0\_24},
  editor               = {Duan, Jin-Chuan and Hardle, Wolfgang K. and Gentle, James E.},
  pages                = {651--673},
  publisher            = {Springer},
  series               = {Springer Handbooks of Computational Statistics},
  abstract             = {Stress testing should be an integral part of any risk management approach for financial institutions. It can be basically viewed as an analysis of a portfolio of transaction under severe but still reasonable scenarios. Those scenarios might be based on a sensitivity analysis with respect to the model parameter, like a large shift in spread curves, or an increase in default probabilities. Then the corresponding transaction and portfolios are revalued at those stressed parameters. This does not increase the computational effort compared to the revaluation under the assumed normal statistical scenario. However a second class of stress testing approaches relies on the factor model usually underlying most portfolio risk models. In credit risk this might be an asset-value model or a macro-economic model for the default rates. In market risk the factor model are interest rates, spread indices and equity indices. The stress can then be formulated in terms of severe shocks on those factors. Technically this is based on the restricting the sample space of factors. If one wants now to assess the risk of a portfolio under those factor stress scenarios, again the worst case losses should be considered from this sub-sample. In a plain Monte-Carlo-based sample a huge number of simulations are necessary. In the contributions we will show how this problem is solved with importance sampling techniques. Usually the Monte-Carlo sample of the underlying factors is shifted to the regions of interest, i.e. much more stress scenarios are generated than in the original scenario generation. This is in particular successful for portfolios, like in credit, which are mostly long the risk.},
  citeulike-article-id = {13989043},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-642-17254-024},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-642-17254-024},
  groups               = {Test_Scenario},
  owner                = {cristi},
  posted-at            = {2016-03-27 17:02:54},
  timestamp            = {2020-09-08 14:27},
}

@Book{Siddique-et-al-2019,
  author    = {Akhtar Siddique and Iftekhar Hasan and David Lynch},
  date      = {2019},
  title     = {Stress Testing: Approaches, Methods and Applications},
  edition   = {2},
  publisher = {Risk Books},
  url       = {@Book{Siddique-Hasan-2012,
  author    = {Akhtar Siddique and Iftekhar Hasan},
  title     = {Stress Testing: Approaches, Methods and Applications},
  date      = {2012},
  publisher = {Risk Books},
  url       = {https://riskbooks.com/stress-testing-approaches-methods-and-applications},
  abstract  = {As a result of the financial crisis, stress testing now constitutes an extensive element of financial institutions' risk management and capital adequacy assessments. However, definitions of what constitutes stress testing and how stress testing should be used have been in a state of flux since the financial crisis.

With Basel III, the Dodd-Frank Act and Interagency Stress Testing guidance in the US and CRD-IV in Europe, understanding the practicalities of stress testing in light of these regulatory practices and considerations is key. Whilst these different sets of guidance and regulation bring clarity with regards to stress testing expectations, multiple approaches towards solving very similar problems have emerged.

Stress Testing: Approaches, Methods and Applications analyses and expounds the various approaches and highlights those most appropriate with regard to the guidance.},
  owner     = {zkgst0c},
  timestamp = {2019-09-03 14:50},
}},
  abstract  = {As a result of the financial crisis, stress testing now constitutes an extensive element of financial institutions' risk management and capital adequacy assessments. However, definitions of what constitutes stress testing and how stress testing should be used have been in a state of flux since the financial crisis.

With Basel III, the Dodd-Frank Act and Interagency Stress Testing guidance in the US and CRD-IV in Europe, understanding the practicalities of stress testing in light of these regulatory practices and considerations is key. Whilst these different sets of guidance and regulation bring clarity with regards to stress testing expectations, multiple approaches towards solving very similar problems have emerged.

Stress Testing: Approaches, Methods and Applications analyses and expounds the various approaches and highlights those most appropriate with regard to the guidance.},
  owner     = {zkgst0c},
  timestamp = {2020-09-08 14:30},
}

@Article{Denev-Mutnikas-2016,
  author               = {Denev, Alexander and Mutnikas, Yaacov},
  date                 = {2016-05},
  journaltitle         = {SSRN e-Print},
  title                = {A Formalized, Integrated and Visual Approach to Stress Testing},
  url                  = {https://ssrn.com/abstract=2780983},
  abstract             = {In this paper we will give for the first time a formal mathematical language to the steps used currently by financial institutions when calculating the impact of a stress scenario on a balance sheet that depends on more granular or different factors than those provided in the scenario. We will introduce the language of Probabilistic Graphical Models (PGM) a technique rooted in machine learning to show how the different models used at each step can be put together in a coherent picture thus giving a holistic view of the entire models setup. This will give us a solid basis to discuss some weaknesses and problems with the stress testing exercises run by the industry as of today. We will show empirical analyses to substantiate better some of our claims.},
  citeulike-article-id = {14038228},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2780983},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2780983code1475411.pdf?abstractid=2780983 and mirid=1},
  day                  = {19},
  groups               = {Test_Scenario},
  owner                = {cristi},
  posted-at            = {2016-05-20 00:30:15},
  timestamp            = {2020-09-08 14:33},
}

@Article{Assefa-et-al-2019,
  author       = {Samuel Assefa and Danial Dervovic and Mahmoud Mahfouz and Tucker Balch and Prashant Reddy and Manuela Veloso},
  date         = {2019},
  journaltitle = {NeurIPS'19 Workshop on Robust AI in Financial Services},
  title        = {Generating Synthetic Data in Finance: Opportunities, Challenges and Pitfalls},
  url          = {https://www.jpmorgan.com/jpmpdf/1320748216372.pdf},
  abstract     = {Financial services generate a huge volume of data that is extremely complex and
varied. These datasets are often stored in silos within organisations for various
reasons, including but not limited to, regulatory requirements and business needs.
As a result, data sharing within different lines of business as well as outside of the
organisation (e.g. to the research community) is severely limited. It is therefore
critical to investigate methods for synthesising financial datasets that follow the
same properties of the real data while respecting the need for privacy of the parties
involved in a particular dataset.
This introductory paper aims to highlight the growing need for effective synthetic
data generation in the financial domain. We highlight three main areas of focus for
the academic community: 1) Generating realistic synthetic datasets. 2) Measuring
the similarities between real and generated datasets 3) Ensuring the generative
process satisfies any privacy constraints.
Although these challenges are also present in other domains, the extra regulatory
and privacy requirements add another dimension of complexity and offer a unique
opportunity to study the topic in financial services. Finally, we aim to develop a
shared vocabulary and context for generating synthetic financial data using two
types of financial datasets as examples.},
  groups       = {ML_DataAugment, Scenario_SynthData},
  timestamp    = {2020-09-08 14:33},
}

@Article{Cont-2001,
  author       = {Cont, R.},
  date         = {2001-02},
  journaltitle = {Quantitative Finance},
  title        = {Empirical properties of asset returns: stylized facts and statistical issues},
  doi          = {10.1080/713665670},
  issn         = {1469-7688},
  number       = {2},
  pages        = {223--236},
  volume       = {1},
  abstract     = {We present a set of stylized empirical facts emerging from the statistical analysis of price variations in various types of financial markets. We first discuss some general issues common to all statistical studies of financial time series. Various statistical properties of asset returns are then described: distributional properties, tail properties and extreme fluctuations, pathwise regularity, linear and nonlinear dependence of returns in time and across stocks. Our description emphasizes properties common to a wide variety of markets and instruments. We then show how these statistical properties invalidate many of the common statistical approaches used to study financial data sets and examine some of the statistical problems encountered in each case.},
  groups       = {Returns_Not_Normal, StylzdFacts_FinDistrib},
  timestamp    = {2020-09-08 14:33},
}

@TechReport{Allen-Satchell-2014,
  author      = {David Allen and Stephen Satchell},
  date        = {2014},
  institution = {University of Sydney},
  title       = {The Four Horsemen: Heavy-tails, Negative Skew, Volatility Clustering, Asymmetric Dependence},
  number      = {DP-004},
  url         = {https://pdfs.semanticscholar.org/f26c/5487ff0b533153cec2e4383826be9df59cde.pdf},
  abstract    = {In the wake of the worst financial crisis since the Great Depression, there has been a proliferation of new risk management and portfolio construction approaches. These approaches endeavour to capture the stylised facts of financial asset returns: heavy tails, negative skew, volatility clustering and asymmetric dependence.

Many approaches capture two or three characteristics, while capturing all four in a scalable framework remains elusive. We propose a novel approach that captures all four stylised characteristics using EGARCH, the skewed t copula and extreme-value theory. Using eight data sets we show the approach is superior to eight benchmark models in both a VaR forecasting and a dynamic portfolio rebalancing framework. The approach generates significant economic value relative to the 1/N rule and the Gaussian approach.

We also find that accounting for asymmetric dependence leads to a consistent improvement in VaR prediction and out-of sample portfolio performance including lower drawdowns.},
  groups      = {Vol_Cluster},
  owner       = {Anne},
  timestamp   = {2020-09-08 14:34},
}

@Article{Buehler-et-al-2020a,
  author       = {Hans Buehler and Blanka Horvath and Terry Lyons and Imanol Perez Arribas and Ben Wood},
  date         = {2020-07-21},
  journaltitle = {SSRN e-Print},
  title        = {Generating financial markets with signatures},
  url          = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3657366},
  abstract     = {Neural network based data-driven market simulation unveils a new and flexible
way of modelling financial time series, which has recently inspired a surge of research activity
in the quantitative finance community. Though generative market simulation is model-free in
the sense that it makes no assumptions on the stochastic dynamics of the underlying paths, the
concrete modelling choices are nevertheless decisive for the performance of the resulting market
generators and the features of the simulated paths. We contrast some classical approaches of
market simulation with simulation based on generative modelling and highlight some advantages
and pitfalls of the new approach. While most generative models tend to rely on large amounts
of training data, we present here a generative model that works reliably even in environments
where the amount of available training data is small, irregularly paced or oscillatory. We show
how a rough paths-based feature map encoded by the signature of the path outperforms returns based market generation both numerically and from a theoretical point of view. Finally, we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our signature-based Market Generator to deep hedging.},
  file         = {:http\://arxiv.org/pdf/2006.14498v1:PDF},
  keywords     = {q-fin.ST, cs.LG, q-fin.CP, q-fin.MF, stat.ML},
  timestamp    = {2020-09-08 14:34},
}

@Article{Wiese-et-al-2020,
  author         = {Wiese, Magnus and Knobloch, Robert and Korn, Ralf and Kretschmer, Peter},
  date           = {2020-04-06},
  journaltitle   = {Quantitative Finance},
  title          = {Quant GANs: deep generation of financial time series},
  issn           = {1469-7688},
  pages          = {In Press},
  url            = {https://www.tandfonline.com/doi/full/10.1080/14697688.2020.1730426},
  urldate        = {2020-04-08},
  abstract       = {Modeling financial time series by stochastic processes is a challenging task and a central area of research in financial mathematics. As an alternative, we introduce Quant GANs, a data-driven model which is inspired by the recent success of generative adversarial networks (GANs). Quant GANs consist of a generator and discriminator function, which utilize temporal convolutional networks (TCNs) and thereby achieve to capture long-range dependencies such as the presence of volatility clusters. The generator function is explicitly constructed such that the induced stochastic process allows a transition to its risk-neutral distribution. Our numerical results highlight that distributional properties for small and large lags are in an excellent agreement and dependence properties such as volatility clusters, leverage effects, and serial autocorrelations can be generated by the generator function of Quant GANs, demonstrably in high fidelity.},
  day            = {6},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_ML},
  timestamp      = {2020-09-08 14:35},
}

@Article{DeMeerPardo-Lopez-2020,
  author       = {{De Meer Pardo}, Fernando and Rafael Cobo Lopez},
  date         = {2020},
  journaltitle = {The Journal of Financial Data Science},
  title        = {Mitigating Overfitting on Financial Datasets with Generative Adversarial Networks},
  issue        = {1},
  pages        = {76-85},
  url          = {https://jfds.pm-research.com/content/2/1/76},
  volume       = {2},
  abstract     = {Overfitting is an inevitable phenomenon when applying deep learning techniques to financial data, given the relative scarcity of available historical data and the ever-changing nature of financial series. This seemingly unavoidable pitfall has heavily impaired the application of many machine learning techniques, such as deep or reinforcement learning, to financial settings. Data augmentation can be an option to circumvent this, specifically generative adversarial networks (GANs). GANs are a type of neural network architecture that focuses on sample generation. Through adversarial training, the GAN can implicitly learn the underlying structure inherent to the dynamics of financial series and acquire the capacity to generate scenarios that share many similarities to those seen in the historic time series. In this article, the authors propose a data augmentation technique using the Wasserstein GAN with gradient penalty and show how training deep models on synthetic data mitigates overfitting, improving their performance on test data when compared to models trained solely on real data.},
  groups       = {Scenario_ML},
  timestamp    = {2020-09-08 14:38},
}

@TechReport{Lezmi-et-al-2020,
  author         = {Lezmi, Edmond and Roche, J.ules and Roncalli, Thierry and Xu, Jiali},
  date           = {2020},
  institution    = {Amundi},
  title          = {Improving the Robustness of Trading Strategy Backtesting with Boltzmann Machines and Generative Adversarial Networks},
  url            = {https://research-center.amundi.com/page/Publications/Working-Paper/2020/Improving-the-Robustness-of-Trading-Strategy-Backtesting-with-Boltzmann-Machines-and-Generative-Adversarial-Networks},
  urldate        = {2019-09-22},
  abstract       = {In this article, we explore generative models in order to build a market generator. The underlying idea is to simulate artificial multi-dimensional financial time series, whose statistical properties are the same as those observed in the financial markets. In particular, these synthetic data must preserve the first four statistical moments (mean, standard deviation, skewness and kurtosis), the stochastic dependence between the different dimensions (copula structure) and across time (autocorrelation function). The first part of the article reviews the more relevant generative models, which are restricted Boltzmann machines, generative adversarial networks, and convolutional Wasserstein models. The second part of the article is dedicated to financial applications by considering the simulation of multi-dimensional times series and estimating the probability distribution of backtest statistics. The final objective is to develop a framework for improving the risk management of quantitative investment strategies.},
  f1000-projects = {QuantInvest},
  issn           = {1556-5068},
  journaltitle   = {SSRN e-Print},
  timestamp      = {2020-09-08 14:38},
}

@InCollection{Deklel-et-al-2017,
  author         = {Deklel, Amr K. and Saleh, Mohamed A. and Hamdy, Alaa M. and Saad, Elsayed M.},
  booktitle      = {34th National Radio Science Conference (NRSC)},
  date           = {2017-03-13},
  title          = {Transfer learning with long term artificial neural network memory (LTANN-MEM) and neural symbolization algorithm (NSA) for solving high dimensional multi-objective symbolic regression problems},
  doi            = {10.1109/{NRSC}.2017.7893495},
  isbn           = {978-1-5090-4611-9},
  pages          = {343--352},
  publisher      = {IEEE},
  abstract       = {Long Term Artificial Neural Network Memory (LTANN-MEM) and Neural Symbolization Algorithm (NSA) are proposed for solving symbolic regression problems. Although this approach is capable of solving Boolean decoder problems of sizes 6, 11 and 20, it is not capable of solving decoder problems of higher dimensions like decoder-37; decoder-n is decoder with sum of inputs and outputs is n for example decoder-20 is decoder with 4 inputs and 16 outputs. It is shown here that LTANN-MEM and NSA approach is a kind of transfer learning however it lacks for sub tasking transfer and updatable LTANN-MEM. An approach for adding the sub tasking transfer and LTANN-MEM updates is discussed here and examined by solving decoder problems of sizes 37, 70 and 135 efficiently. Comparisons with two learning classifier systems are performed and it is found that the proposed approach in this work outperforms both of them. It is shown that the proposed approach is used also for solving decoder-264 efficiently. According to the best of our knowledge, there is no reported approach for solving this high dimensional problem.},
  day            = {13},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:39},
}

@Book{Brunton-Kutz-2019,
  author    = {Steven L. Brunton and J. Nathan Kutz},
  date      = {2019},
  title     = {Data-Driven Science and Engineering},
  doi       = {10.1017/9781108380690},
  publisher = {Cambridge University Press},
  url       = {https://www.cambridge.org/core/books/datadriven-science-and-engineering/77D52B171B60A496EAFE4DB662ADC36E},
  abstract  = {Data-driven discovery is revolutionizing the modeling, prediction, and control of complex systems. This textbook brings together machine learning, engineering mathematics, and mathematical physics to integrate modeling and control of dynamical systems with modern methods in data science. It highlights many of the recent advances in scientific computing that enable data-driven methods to be applied to a diverse range of complex systems, such as turbulence, the brain, climate, epidemiology, finance, robotics, and autonomy. Aimed at advanced undergraduate and beginning graduate students in the engineering and physical sciences, the text presents a range of topics and methods from introductory to state of the art.},
  timestamp = {2020-09-08 14:39},
}

@Article{Derner-et-al-2020,
  author       = {Erik Derner and Ji{\v{r}}{\'{\i}} Kubal{\'{\i}}k and Nicola Ancona and Robert Babu{\v{s}}ka},
  date         = {2020},
  journaltitle = {Applied Soft Computing},
  title        = {Constructing parsimonious analytic models for dynamic systems via symbolic regression},
  doi          = {10.1016/j.asoc.2020.106432},
  pages        = {106432},
  url          = {https://www.sciencedirect.com/science/article/abs/pii/S1568494620303720?dgcid=rss_sd_all},
  volume       = {94},
  abstract     = {Developing mathematical models of dynamic systems is central to many disciplines of engineering and science. Models facilitate simulations, analysis of the system's behavior, decision making and design of automatic control algorithms. Even inherently model-free control techniques such as reinforcement learning (RL) have been shown to benefit from the use of models, typically learned online. Any model construction method must address the tradeoff between the accuracy of the model and its complexity, which is difficult to strike. In this paper, we propose to employ symbolic regression (SR) to construct parsimonious process models described by analytic equations. We have equipped our method with two different state-of-the-art SR algorithms which automatically search for equations that fit the measured data: Single Node Genetic Programming (SNGP) and Multi-Gene Genetic Programming (MGGP). In addition to the standard problem formulation in the state-space domain, we show how the method can also be applied to input-output models of the NARX (nonlinear autoregressive with exogenous input) type. We present the approach on three simulated examples with up to 14-dimensional state space: an inverted pendulum, a mobile robot, and a bipedal walking robot. A comparison with deep neural networks and local linear regression shows that SR in most cases outperforms these commonly used alternative methods. We demonstrate on a real pendulum system that the analytic model found enables a RL controller to successfully perform the swing-up task, based on a model constructed from only 100 data samples.},
  timestamp    = {2020-09-08 14:40},
}

@Article{Champion-et-al-2019,
  author       = {Kathleen Champion and Bethany Lusch and J. Nathan Kutz and Steven L. Brunton},
  date         = {2019},
  journaltitle = {Proceedings of the National Academy of Sciences},
  title        = {Data-driven discovery of coordinates and governing equations},
  doi          = {10.1073/pnas.1906995116},
  number       = {45},
  pages        = {22445--22451},
  url          = {https://www.pnas.org/content/116/45/22445.short},
  volume       = {116},
  abstract     = {The discovery of governing equations from scientific data has the potential to transform data-rich fields that lack well-characterized quantitative descriptions. Advances in sparse regression are currently enabling the tractable identification of both the structure and parameters of a nonlinear dynamical system from data. The resulting models have the fewest terms necessary to describe the dynamics, balancing model complexity with descriptive ability, and thus promoting interpretability and generalizability. This provides an algorithmic approach to Occam's razor for model discovery. However, this approach fundamentally relies on an effective coordinate system in which the dynamics have a simple representation. In this work, we design a custom deep autoencoder network to discover a coordinate transformation into a reduced space where the dynamics may be sparsely represented. Thus, we simultaneously learn the governing equations and the associated coordinate system. We demonstrate this approach on several example high-dimensional systems with low-dimensional behavior. The resulting modeling framework combines the strengths of deep neural networks for flexible representation and sparse identification of nonlinear dynamics (SINDy) for parsimonious models. This method places the discovery of coordinates and models on an equal footing.},
  timestamp    = {2020-09-08 14:40},
}

@Article{Champion-et-al-2019a,
  author       = {Kathleen P. Champion and Steven L. Brunton and J. Nathan Kutz},
  date         = {2019},
  journaltitle = {{SIAM} Journal on Applied Dynamical Systems},
  title        = {Discovery of Nonlinear Multiscale Systems: Sampling Strategies and Embeddings},
  doi          = {10.1137/18m1188227},
  number       = {1},
  pages        = {312--333},
  volume       = {18},
  abstract     = {A major challenge in the study of dynamical systems is that of model discovery: turning data into models that are not just predictive, but provide insight into the nature of the underlying dynamical system that generated the data. This problem is made more difficult by the fact that many systems of interest exhibit diverse behaviors across multiple time scales. We introduce a number of data-driven strategies for discovering nonlinear multiscale dynamical systems and their embeddings from data. We consider two canonical cases: (i) systems for which we have full measurements of the governing variables and (ii) systems for which we have incomplete measurements. For systems with full state measurements, we show that the recent sparse identification of nonlinear dynamical systems (SINDy) method can discover governing equations with relatively little data, provided that accurate measurements of the derivatives can be computed from the data. We introduce a sampling method that allows SINDy to scale efficiently to problems with multiple time scales; specifically, we can discover distinct governing equations at slow and fast scales. For systems with incomplete observations, we show that the Hankel alternative view of Koopman (HAVOK) method, based on time-delay embedding coordinates, can be used to obtain a linear model and Koopman invariant measurement system that nearly perfectly captures the dynamics of nonlinear quasiperiodic systems on the attractor. We introduce two strategies for using HAVOK on systems with multiple time scales. Together, our approaches provide a suite of mathematical strategies for reducing the data required to discover and model nonlinear multiscale systems.},
  timestamp    = {2020-09-08 14:40},
}

@Article{Udrescu-Tegmark-2020,
  author       = {Silviu-Marian Udrescu and Max Tegmark},
  date         = {2020},
  journaltitle = {Science Advances},
  title        = {{AI} Feynman: A physics-inspired method for symbolic regression},
  doi          = {10.1126/sciadv.aay2631},
  number       = {16},
  pages        = {eaay2631},
  url          = {https://advances.sciencemag.org/content/6/16/eaay2631.abstract},
  volume       = {6},
  abstract     = {A core challenge for both physics and artificial intelligence (AI) is symbolic regression: finding a symbolic expression that matches data from an unknown function. Although this problem is likely to be NP-hard in principle, functions of practical interest often exhibit symmetries, separability, compositionality, and other simplifying properties. In this spirit, we develop a recursive multidimensional symbolic regression algorithm that combines neural network fitting with a suite of physics-inspired techniques. We apply it to 100 equations from the Feynman Lectures on Physics, and it discovers all of them, while previous publicly available software cracks only 71; for a more difficult physics-based test set, we improve the state-of-the-art success rate from 15 to 90\%.},
  month        = {apr},
  timestamp    = {2020-09-08 14:40},
  year         = {2020},
}

@Article{Udrescu-et-al-2020,
  author       = {Silviu-Marian Udrescu and Andrew Tan and Jiahai Feng and Orisvaldo Neto and Tailin Wu and Max Tegmark},
  date         = {2020-06-18},
  journaltitle = {arXiv e-Print},
  title        = {AI Feynman 2.0: Pareto-optimal symbolic regression exploiting graph modularity},
  url          = {https://arxiv.org/abs/2006.10782},
  abstract     = {We present an improved method for symbolic regression that seeks to fit data to formulas that are Pareto-optimal, in the sense of having the best accuracy for a given complexity. It improves on the previous state-of-the-art by typically being orders of magnitude more robust toward noise and bad data, and also by discovering many formulas that stumped previous methods. We develop a method for discovering generalized symmetries (arbitrary modularity in the computational graph of a formula) from gradient properties of a neural network fit. We use normalizing flows to generalize our symbolic regression method to probability distributions from which we only have samples, and employ statistical hypothesis testing to accelerate robust brute-force search.},
  file         = {:http\://arxiv.org/pdf/2006.10782v1:PDF},
  keywords     = {cs.LG, cs.AI, cs.IT, math.IT, physics.comp-ph, stat.ML},
  timestamp    = {2020-09-08 14:40},
}

@Article{Neumann-et-al-2020,
  author       = {Pascal Neumann and Liwei Cao and Danilo Russo and Vassilios S. Vassiliadis and Alexei A. Lapkin},
  date         = {2020},
  journaltitle = {Chemical Engineering Journal},
  title        = {A new formulation for symbolic regression to identify physico-chemical laws from experimental data},
  doi          = {10.1016/j.cej.2019.123412},
  pages        = {123412},
  volume       = {387},
  abstract     = {A modification to the mixed-integer nonlinear programming (MINLP) formulation for symbolic regression was proposed with the aim of identification of physical models from noisy experimental data. In the proposed formulation, a binary tree in which equations are represented as directed, acyclic graphs, is fully constructed for a pre-defined number of layers. The introduced modification results in the reduction in the number of required binary variables and removal of redundancy due to possible symmetry of the tree formulation. The formulation was tested using numerical models and was found to be more efficient than the previous literature example with respect to the numbers of predictor variables and training data points. The globally optimal search was extended to identify physical models and to cope with noise in the experimental data predictor variable. The methodology was proven to be successful in identifying the correct physical models describing the relationship between shear stress and shear rate for both Newtonian and non-Newtonian fluids, and simple kinetic laws of chemical reactions. Future work will focus on addressing the limitations of the present formulation and solver to enable extension of target problems to larger, more complex physical models.},
  timestamp    = {2020-09-08 14:41},
}

@Article{Nicolau-Agapitos-2020,
  author       = {Miguel Nicolau and Alexandros Agapitos},
  date         = {2020},
  journaltitle = {Genetic Programming and Evolvable Machines},
  title        = {Choosing function sets with better generalisation performance for symbolic regression models},
  doi          = {10.1007/s10710-020-09391-4},
  pages        = {In Press},
  url          = {https://link.springer.com/article/10.1007/s10710-020-09391-4},
  abstract     = {Supervised learning by means of Genetic Programming (GP) aims at the evolutionary synthesis of a model that achieves a balance between approximating the target function on the training data and generalising on new data. The model space searched by the Evolutionary Algorithm is populated by compositions of primitive functions defined in a function set. Since the target function is unknown, the choice of function set's constituent elements is primarily guided by the makeup of function sets traditionally used in the GP literature. Our work builds upon previous research of the effects of protected arithmetic operators (i.e. division, logarithm, power) on the output value of an evolved model for input data points not encountered during training. The scope is to benchmark the approximation/generalisation of models evolved using different function set choices across a range of 43 symbolic regression problems. The salient outcomes are as follows. Firstly, Koza's protected operators of division and exponentiation have a detrimental effect on generalisation, and should therefore be avoided. This result is invariant of the use of moderately sized validation sets for model selection. Secondly, the performance of the recently introduced analytic quotient operator is comparable to that of the sinusoidal operator on average, with their combination being advantageous to both approximation and generalisation. These findings are consistent across two different system implementations, those of standard expression-tree GP and linear Grammatical Evolution. We highlight that this study employed very large test sets, which create confidence when benchmarking the effect of different combinations of primitive functions on model generalisation. Our aim is to encourage GP researchers and practitioners to use similar stringent means of assessing generalisation of evolved models where possible, and also to avoid certain primitive functions that are known to be inappropriate.},
  timestamp    = {2020-09-08 14:41},
}

@InCollection{Haslam-et-al-2016,
  author         = {Haslam, Edward and Xue, Bing and Zhang, Mengjie},
  booktitle      = {IEEE Congress on Evolutionary Computation (CEC)},
  date           = {2016-07-24},
  title          = {Further investigation on genetic programming with transfer learning for symbolic regression},
  doi            = {10.1109/{CEC}.2016.7744245},
  isbn           = {978-1-5090-0623-6},
  pages          = {3598--3605},
  publisher      = {IEEE},
  abstract       = {Transfer learning is an important approach in machine learning, which aims to solve a problem by utilising the knowledge learnt from another problem domain. There has been extensive research and great achievement on transfer learning for image analysis and other tasks, but research on transfer learning in genetic programming (GP) for symbolic regression is still in the very early stage. However, GP has a natural way of expressing knowledge by trees or subtrees, which can be automatically discovered during the evolutionary process. An initial work on GP with transfer learning was proposed to transfer knowledge through best trees or subtrees from to source domain to facilitate the learning in the target domain. However, there are still a number of important issues remaining not investigated. This paper further investigates the ability of GP with transfer learning on different types of transfer scenarios, investigates the influence of a key parameter and the effect of transfer learning on the evolutionary training process, and also analyses how the knowledge learnt from the source domain was utilised during the learning process on the target domain. The results show that GP with transfer learning can generally perform well on different types of transfer scenarios. The transferred knowledge can provide a good initial population for the GP learning on the target domain, speed up the convergence, and help obtain better final solutions. However, the benefits of transfer learning varies in different scenarios.},
  day            = {24},
  f1000-projects = {QuantInvest},
  groups         = {SymbolRegres_Main},
  timestamp      = {2020-09-08 14:43},
}

@Article{Koenecke-Gajewar-2019,
  author         = {Koenecke, Allison and Gajewar, Amita},
  date           = {2019-04-29},
  journaltitle   = {arXiv e-Print},
  title          = {Curriculum Learning in Deep Neural Networks for Financial Forecasting},
  url            = {https://arxiv.org/abs/1904.12887},
  abstract       = {For any financial organization, computing accurate quarterly forecasts for various products is one of the most critical operations. As the granularity at which forecasts are needed increases, traditional statistical time series models may not scale well. We apply deep neural networks in the forecasting domain by experimenting with techniques from Natural Language Processing (Encoder-Decoder LSTMs) and Computer Vision (Dilated CNNs), as well as incorporating transfer learning. A novel contribution of this paper is the application of curriculum learning to neural network models built for time series forecasting. We illustrate the performance of our models using Microsoft's revenue data corresponding to Enterprise, and Small, Medium and Corporate products, spanning approximately 60 regions across the globe for 8 different business segments, and totaling in the order of tens of billions of USD. We compare our models' performance to the ensemble model of traditional statistics and machine learning techniques currently used by Microsoft Finance. With this in-production model as a baseline, our experiments yield an approximately 30\% improvement in overall accuracy on test data. We find that our curriculum learning LSTM-based model performs best, showing that it is reasonable to implement our proposed methods without overfitting on medium-sized data.},
  day            = {29},
  f1000-projects = {QuantInvest},
  groups         = {ML_TransferLrng, ML_ForcstTimeSrs},
  timestamp      = {2020-09-08 14:44},
}

@Article{Krishna-Menzies-2018,
  author         = {Krishna, Rahul and Menzies, Tim},
  date           = {2018},
  journaltitle   = {IEEE Transactions on Software Engineering},
  title          = {Bellwethers: A baseline method for transfer learning},
  doi            = {10.1109/{TSE}.2018.2821670},
  issue          = {11},
  pages          = {1081-1105},
  url            = {https://ieeexplore.ieee.org/document/8329264},
  volume         = {45},
  abstract       = {Software analytics builds quality prediction models for software projects. Experience shows that (a) the more projects studied, the more varied are the conclusions; and (b) project managers lose faith in the results of software analytics if those results keep changing. To reduce this conclusion instability, we propose the use of bellwethers: given N projects from a community the bellwether is the project whose data yields the best predictions on all others. The bellwethers offer a way to mitigate conclusion instability because conclusions about a community are stable as long as this bellwether continues as the best oracle. Bellwethers are also simple to discover (just wrap a for-loop around standard data miners). When compared to other transfer learning methods (TCA+, transfer Naive Bayes, value cognitive boosting), using just the bellwether data to construct a simple transfer learner yields comparable predictions. Further, bellwethers appear in many SE tasks such as defect prediction, effort estimation, and bad smell detection. We hence recommend using bellwethers as a baseline method for transfer learning against which future work should be compared.},
  f1000-projects = {QuantInvest},
  groups         = {ML_TransferLrng},
  timestamp      = {2020-09-08 14:44},
}

@Article{Lee-et-al-2018f,
  author         = {Lee, Kuan-Hui and Ros, German and Li, Jie and Gaidon, Adrien},
  date           = {2018-10-09},
  journaltitle   = {arXiv e-Print},
  title          = {SPIGAN: Privileged Adversarial Learning from Simulation},
  url            = {https://arxiv.org/abs/1810.03756},
  urldate        = {2019-10-18},
  abstract       = {Deep Learning for Computer Vision depends mainly on the source of supervision.Photo-realistic simulators can generate large-scale automatically labeled syntheticdata, but introduce a domain gap negatively impacting performance. We propose anew unsupervised domain adaptation algorithm, called SPIGAN, relying on Sim-ulator Privileged Information (PI) and Generative Adversarial Networks (GAN).We use internal data from the simulator as PI during the training of a target tasknetwork. We experimentally evaluate our approach on semantic segmentation. Wetrain the networks on real-world Cityscapes and Vistas datasets, using only unla-beled real-world images and synthetic labeled data with z-buffer (depth) PI fromthe SYNTHIA dataset. Our method improves over no adaptation and state-of-the-art unsupervised domain adaptation techniques.},
  day            = {9},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_SynthData},
  timestamp      = {2020-09-08 14:44},
}

@Article{Li-et-al-2017l,
  author         = {Li, Chunyuan and Liu, Hao and Chen, Changyou and Pu, Yunchen and Chen, Liqun and Henao, Ricardo and Carin, Lawrence},
  date           = {2017-09-05},
  journaltitle   = {arXiv e-Print},
  title          = {ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching},
  url            = {https://arxiv.org/abs/1709.01215},
  urldate        = {2019-10-18},
  abstract       = {We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.},
  day            = {5},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_SynthData},
  timestamp      = {2020-09-08 14:44},
}

@Article{Shen-et-al-2019d,
  author         = {Shen, Zhiqiang and He, Zhankui and Xue, Xiangyang},
  date           = {2019-07-17},
  journaltitle   = {Proceedings of the AAAI Conference on Artificial Intelligence},
  title          = {MEAL: Multi-Model Ensemble via Adversarial Learning},
  doi            = {10.1609/aaai.v33i01.33014886},
  issn           = {2374-3468},
  pages          = {4886--4893},
  url            = {https://aaai.org/ojs/index.php/{AAAI}/article/view/4417},
  urldate        = {2019-10-18},
  volume         = {33},
  abstract       = {Often the best performing deep neural models are ensembles of multiple base-level networks. Unfortunately, the space required to store these many networks, and the time required to execute them at test-time, prohibits their use in applications where test sets are large (e.g., ImageNet). In this paper, we present a method for compressing large, complex trained ensembles into a single network, where knowledge from a variety of trained deep neural networks (DNNs) is distilled and transferred to a single DNN. In order to distill diverse knowledge from different trained (teacher) models, we propose to use adversarial-based learning strategy where we define a block-wise training loss to guide and optimize the predefined student network to recover the knowledge in teacher models, and to promote the discriminator network to distinguish teacher vs. student features simultaneously. The proposed ensemble method (MEAL) of transferring distilled knowledge with adversarial learning exhibits three important advantages: (1) the student network that learns the distilled knowledge with discriminators is optimized better than the original model; (2) fast inference is realized by a single forward pass, while the performance is even better than traditional ensembles from multi-original models; (3) the student network can learn the distilled knowledge from a teacher model that has arbitrary structures. Extensive experiments on CIFAR-10/100, SVHN and ImageNet datasets demonstrate the effectiveness of our MEAL method. On ImageNet, our ResNet-50 based MEAL achieves top-1 to 5 21.79\% and 5.99\% val error, which outperforms the original model by 2.06\% abd 1.14\%.},
  day            = {17},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:45},
}

@Article{deSilva-et-al-2020,
  author       = {{de Silva}, Brian and Kathleen Champion and Markus Quade and Jean-Christophe Loiseau and J. Kutz and Steven Brunton},
  date         = {2020},
  journaltitle = {Journal of Open Source Software},
  title        = {{PySINDy}: A Python package for the sparse identification of nonlinear dynamical systems from data},
  doi          = {10.21105/joss.02104},
  number       = {49},
  pages        = {2104},
  url          = {https://joss.theoj.org/papers/10.21105/joss.02104},
  volume       = {5},
  abstract     = {Scientists have long quantified empirical observations by developing mathematical models that characterize the observations, have some measure of interpretability, and are capable of making predictions. Dynamical systems models in particular have been widely used to study, explain, and predict system behavior in a wide range of application areas, with examples ranging from Newton's laws of classical mechanics to the Michaelis-Menten kinetics for modeling enzyme kinetics. While governing laws and equations were traditionally derived by hand, the current growth of available measurement data and resulting emphasis on data-driven modeling motivates algorithmic approaches for model discovery. A number of such approaches have been developed in recent years and have generated widespread interest, including Eureqa (Schmidt \& Lipson, 2009), sure independence screening and sparsifying operator (Ouyang, Curtarolo, Ahmetcik, Scheffler, \& Ghiringhelli, 2018), and the sparse identification of nonlinear dynamics (SINDy) (Brunton, Proctor, \& Kutz, 2016). Maximizing the impact of these model discovery methods requires tools to make them widely accessible to scientists across domains and at various levels of mathematical expertise.},
  timestamp    = {2020-09-08 14:45},
}

@Article{deSilva-et-al-2020a,
  author       = {{de Silva}, Brian M. and David M. Higdon and Steven L. Brunton and J. Nathan Kutz},
  date         = {2019-06-19},
  journaltitle = {arXiv e-Print},
  title        = {Discovery of Physics from Data: Universal Laws and Discrepancies},
  url          = {https://arxiv.org/abs/1906.07906},
  abstract     = {Machine learning (ML) and artificial intelligence (AI) algorithms are now being used to automate the discovery of physics principles and governing equations from measurement data alone. However, positing a universal physical law from data is challenging without simultaneously proposing an accompanying discrepancy model to account for the inevitable mismatch between theory and measurements. By revisiting the classic problem of modeling falling objects of different size and mass, we highlight a number of nuanced issues that must be addressed by modern data-driven methods for automated physics discovery. Specifically, we show that measurement noise and complex secondary physical mechanisms, like unsteady fluid drag forces, can obscure the underlying law of gravitation, leading to an erroneous model. We use the sparse identification of nonlinear dynamics (SINDy) method to identify governing equations for real-world measurement data and simulated trajectories. Incorporating into SINDy the assumption that each falling object is governed by a similar physical law is shown to improve the robustness of the learned models, but discrepancies between the predictions and observations persist due to subtleties in drag dynamics. This work highlights the fact that the naive application of ML/AI will generally be insufficient to infer universal physical laws without further modification.},
  file         = {:http\://arxiv.org/pdf/1906.07906v3:PDF},
  keywords     = {cs.LG, physics.class-ph, stat.ML},
  timestamp    = {2020-09-08 14:45},
}

@Article{Derner-et-al-2019,
  author         = {Derner, Erik and Kubalik, Jiri and Ancona, Nicola and Babuska, Robert},
  date           = {2019-03-27},
  journaltitle   = {arXiv e-Print},
  title          = {Symbolic Regression for Constructing Analytic Models in Reinforcement Learning},
  url            = {https://arxiv.org/abs/1903.11483},
  urldate        = {2019-05-05},
  abstract       = {Reinforcement learning (RL) is a widely used approach for controlling systems with unknown or time-varying dynamics. Even though RL does not require a model of the system, it is known to be faster and safer when using models learned online. We propose to employ symbolic regression (SR) to construct parsimonious process models described by analytic equations for real-time RL control. We have tested our method with two different state-of-the-art SR algorithms which automatically search for equations that fit the measured data. In addition to the standard problem formulation in the state-space domain, we show how the method can also be applied to input-output models of the NARX (nonlinear autoregressive with exogenous input) type. We present the approach on three simulated examples with up to 14-dimensional state space: an inverted pendulum, a mobile robot, and a biped walking robot. A comparison with deep neural networks and local linear regression shows that SR in most cases outperforms these commonly used alternative methods. We demonstrate on a real pendulum system that the analytic model found enables RL to successfully perform the swing-up task, based on a model constructed from only 100 data samples.},
  day            = {27},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:46},
}

@Article{Kubalik-et-al-2019,
  author         = {Kubalik, Jiri and Zegklitz, Jan and Derner, Erik and Babuska, Robert},
  date           = {2019-03-22},
  journaltitle   = {arXiv e-Print},
  title          = {Symbolic Regression Methods for Reinforcement Learning},
  url            = {https://arxiv.org/abs/1903.09688},
  abstract       = {Reinforcement learning algorithms can be used to optimally solve dynamic decision-making and control problems. With continuous-valued state and input variables, reinforcement learning algorithms must rely on function approximators to represent the value function and policy mappings. Commonly used numerical approximators, such as neural networks or basis function expansions, have two main drawbacks: they are black-box models offering no insight in the mappings learned, and they require significant trial and error tuning of their meta-parameters. In this paper, we propose a new approach to constructing smooth value functions by means of symbolic regression. We introduce three off-line methods for finding value functions based on a state transition model: symbolic value iteration, symbolic policy iteration, and a direct solution of the Bellman equation. The methods are illustrated on four nonlinear control problems: velocity control under friction, one-link and two-link pendulum swing-up, and magnetic manipulation. The results show that the value functions not only yield well-performing policies, but also are compact, human-readable and mathematically tractable. This makes them potentially suitable for further analysis of the closed-loop system. A comparison with alternative approaches using neural networks shows that our method constructs well-performing value functions with substantially fewer parameters.},
  day            = {22},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:46},
}

@InCollection{Orzechowski-et-al-2018,
  author         = {Orzechowski, Patryk and La Cava, William and Moore, Jason H.},
  booktitle      = {Proceedings of the Genetic and Evolutionary Computation Conference GECCO '18},
  date           = {2018-07-15},
  title          = {Where are we now? A large benchmark study of recent symbolic regression methods},
  doi            = {10.1145/3205455.3205539},
  editor         = {Aguirre, Hernan},
  isbn           = {9781450356183},
  location       = {New York, New York, USA},
  pages          = {1183--1190},
  publisher      = {ACM Press},
  abstract       = {In this paper we provide a broad benchmarking of recent genetic programming approaches to symbolic regression in the context of state of the art machine learning approaches. We use a set of nearly 100 regression benchmark problems culled from open source repositories across the web. We conduct a rigorous benchmarking of four recent symbolic regression approaches as well as nine machine learning approaches from scikit-learn. The results suggest that symbolic regression performs strongly compared to state-of-the-art gradient boosting algorithms, although in terms of running times is among the slowest of the available methodologies. We discuss the results in detail and point to future research directions that may allow symbolic regression to gain wider adoption in the machine learning community.},
  day            = {15},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 14:47},
}

@Article{FrancoPedroso-et-al-2019,
  author         = {Franco-Pedroso, Javier and Gonzalez-Rodriguez, Joaquin and Planas, Maria and Cubero, Jorge and Cobo, Rafael and Pablos, Fernando},
  date           = {2019-11-19},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {The ETS challenges: a machine learning approach to the evaluation of simulated financial time series for improving generation processes},
  issue          = {3},
  number         = {32},
  pages          = {68-86},
  url            = {https://jfds.pm-research.com/content/1/3/68.short},
  volume         = {1},
  abstract       = {This paper presents an evaluation framework that attempts to quantify the "degree of realism" of simulated financial time series, whatever the simulation method could be, with the aim of discover unknown characteristics that are not being properly reproduced by such methods in order to improve them. For that purpose, the evaluation framework is posed as a machine learning problem in which some given time series examples have to be classified as simulated or real financial time series. The "challenge" is proposed as an open competition, similar to those published at the Kaggle platform, in which participants must send their classification results along with a description of the features and the classifiers used. The results of these "challenges" have revealed some interesting properties of financial data, and have lead to substantial improvements in our simulation methods under research, some of which will be described in this work.},
  day            = {19},
  f1000-projects = {QuantInvest},
  groups         = {ML_Classif_QWIM},
  timestamp      = {2020-09-08 15:18},
}

@Article{FrancoPedroso-et-al-2019a,
  author         = {Franco-Pedroso, Javier and Gonzalez-Rodriguez, Joaquin and Planas, Maria and Cubero, Jorge and Cobo, Rafael and Pablos, Fernando},
  date           = {2019-11-19},
  journaltitle   = {The Journal of Financial Data Science},
  title          = {Generating Virtual Scenarios of Multivariate Financial Data for Quantitative Trading Applications},
  issue          = {2},
  pages          = {55-77},
  url            = {https://jfds.pm-research.com/content/1/2/55},
  volume         = {1},
  abstract       = {In this article, the authors present a novel approach to the generation of virtual scenarios of multivariate financial data of arbitrary length and composition of assets. With this approach, decades of realistic time synchronized data can be simulated for a large number of assets, producing diverse scenarios to test and improve quantitative investment strategies. The authors' approach is based on the analysis and synthesis of the time dependent individual and joint characteristics of real financial time series, using stochastic sequences of market trends to draw multivariate returns from time-dependent probability functions that preserve both distributional properties of asset returns and time-dependent correlation among time series. Moreover, new time-synchronized assets can be arbitrarily generated through a principal component analysis-based procedure to obtain any number of assets in the final virtual scenario. The validation of such a simulation is tested with an extensive set of measurements and shows a significant degree of agreement with the reference performance of real financial series, better than that obtained with other classical and state-of-the-art approaches.},
  day            = {19},
  f1000-projects = {QuantInvest},
  groups         = {ML_Classif_QWIM},
  timestamp      = {2020-09-08 15:18},
}

@InCollection{Gharghabi-et-al-2018,
  author         = {Gharghabi, Shaghayegh and Imani, Shima and Bagnall, Anthony and Darvishzadeh, Amirali and Keogh, Eamonn},
  booktitle      = {IEEE International Conference on Data Mining (ICDM)},
  date           = {2018-11-17},
  title          = {Matrix profile XII: mpdist: A novel time series distance measure to allow data mining in more challenging scenarios},
  doi            = {10.1109/{ICDM}.2018.00119},
  isbn           = {978-1-5386-9159-5},
  pages          = {965--970},
  publisher      = {IEEE},
  url            = {https://ieeexplore.ieee.org/document/8594928},
  abstract       = {At their core, many time series data mining algorithms can be reduced to reasoning about the shapes of time series subsequences. This requires a distance measure, and most algorithms use Euclidean Distance or Dynamic Time Warping (DTW) as their core subroutine. We argue that these distance measures are not as robust as the community believes. The undue faith in these measures derives from an overreliance on benchmark datasets and self-selection bias. The community is reluctant to address more difficult domains, for which current distance measures are ill-suited. In this work, we introduce a novel distance measure MPdist. We show that our proposed distance measure is much more robust than current distance measures. Furthermore, it allows us to successfully mine datasets that would defeat any Euclidean or DTW distance-based algorithm. Additionally, we show that our distance measure can be computed so efficiently, it allows analytics on fast streams.},
  day            = {17},
  f1000-projects = {QuantInvest},
  groups         = {Scenario_TimeSeries},
  timestamp      = {2020-09-08 15:19},
}

@Article{Golub-et-al-2018,
  author         = {Golub, Bennett and Greenberg, David and Ratcliffe, Ronald},
  date           = {2018-04-05},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {Market-Driven Scenarios: An Approach for Plausible Scenario Construction},
  doi            = {10.3905/jpm.2018.1.079},
  number         = {5},
  pages          = {6-20},
  url            = {https://jpm.pm-research.com/content/early/2018/04/04/jpm.2018.1.079},
  volume         = {44},
  abstract       = {The use of scenario analysis to better understand portfolios has increased significantly since the global financial crisis. In this article, the authors describe a stress scenario framework and process that has been developed for risk management and investment management purposes. This hybrid framework, which the authors refer to as market-driven scenarios, works as follows. Scenario forecasts of key market indicators are first formulated by market practitioners. An econometric framework then uses these indicators as inputs to imply plausible shocks to a global set of risk factors. These factor shocks are finally put into a portfolio valuation engine, yielding hypothetical fund profit and loss (P\&L) that can be decomposed into its underlying drivers. Key to the effectiveness of this approach is the cross-functional involvement of investors, risk managers, and economists. In conjunction, the authors define potential geopolitical or other macro events, specify potential economic outcomes, and translate them into shocks to key policy risk variables and risk model factors. The process is completed by applying the shocks to portfolios and evaluating whether P\&L outcomes are consistent with fund mandates and whether positioning is deliberate, diversified, and scaled.},
  day            = {5},
  f1000-projects = {QuantInvest},
  groups         = {PortfOptim_Scenario, FrcstQWIM_Hybrid, Scenario_Portfolio, Scenario_Market},
  timestamp      = {2020-09-08 15:20},
}

@Article{Gunay-2015,
  author               = {Samet Gunay},
  date                 = {2015},
  journaltitle         = {The Journal of Applied Business Research},
  title                = {Power laws in financial markets: Scaling exponent H and alpha-stable distributions},
  number               = {1},
  url                  = {https://clutejournals.com/index.php/JABR/article/view/9009},
  volume               = {31},
  abstract             = {In this study, we analyzed whether daily returns of Brent crude oil, dollar/yen foreign exchange, Dow and Jones Industrial Average Index and 12-month libor display power law features in the scaling exponent and probability distributions or not, using different methods. Due to the fact that the simulated time series with different values showed the robustness of Higuchi's Fractal Dimension and Peng's Statistic, we used these two models in the analysis of the scaling features of the returns. On the other hand, in order to examine power law behaviors of probability distributions, we estimated parameters of the alpha-stable distributions for the return series using the Ecf and Percentile methods. Results showed that the Brent crude oil and 12-month libor have a high persistency in the returns, while the dollar/yen foreign exchange and Dow and Jones Industrial Average Index returns have short memory. According to the alpha-stable parameter estimations, all of the return series have thicker tails than normal distribution. Similar to the highest persistency of 12-month libor returns in the scaling exponent analysis, we have seen that this variable also has the thickest tails in the probability distributions, meaning that 12-month libor returns have the highest power law features within the series.},
  citeulike-article-id = {14387600},
  groups               = {Scenario_TimeSeries, Returns_Not_Normal},
  posted-at            = {2017-07-03 22:57:21},
  timestamp            = {2020-09-08 15:22},
}

@Article{Liu-2015a,
  author         = {Liu, Xinyi},
  date           = {2015-10},
  journaltitle   = {Journal of Risk},
  title          = {Historical simulation with component weight and ghosted scenarios},
  issn           = {1465-1211},
  number         = {11},
  pages          = {1-25},
  url            = {http://www.risk.net/journal-of-risk/technical-paper/2427781/historical-simulation-with-component-weight-and-ghosted-scenarios},
  urldate        = {2019-05-30},
  volume         = {18},
  abstract       = {Historical simulation (HS) is a popular value-at-risk (VaR) approach that has the advantage of being intuitive and easy to implement. However, its response to most recent news has been too slow, its two "tails" (upper and lower) cannot learn from each other and it is not robust if there is insufficient data. In this paper, we put forward two strategies for improving HS in these weak areas with only minor additional computational costs. The first strategy is a "ghosted" scenario and the second is a two-component (short-run and long-run) exponentially weighted moving average scheme. The VaR is then calculated according to the empirical distribution of the two-component weighted real and ghosted scenarios.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 15:26},
}

@Article{Marrs-2019,
  author         = {Tyler Marrs},
  date           = {2019},
  journaltitle   = {Towards Data Science},
  title          = {Introduction to Matrix Profiles: A Novel Data Structure for Mining Time Series},
  url            = {https://towardsdatascience.com/introduction-to-matrix-profiles-5568f3375d90},
  urldate        = {2019-11-13},
  abstract       = {In the article, you were introduced to the Matrix Profile and how it can be used to analyze time series data. It is a less known approach as it is still new, but is a fast and domain agnostic method. Once you have the Matrix Profile it is trivial to extract common patterns (motifs) and anomalies (discords). In our brief NYC Taxi example, I illustrated anomaly extraction through visualization.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 15:28},
  type           = {WEBSITE},
}

@Article{Mikalsen-et-al-2018,
  author               = {Mikalsen, Karl O. and Bianchi, Filippo M. and Soguero-Ruiz, Cristina and Jenssen, Robert},
  date                 = {2018-12},
  journaltitle         = {Pattern Recognition},
  title                = {Time Series Cluster Kernel for Learning Similarities between Multivariate Time Series with Missing Data},
  doi                  = {10.1016/j.patcog.2017.11.030},
  issn                 = {0031-3203},
  pages                = {569-581},
  volume               = {76},
  abstract             = {Similarity-based approaches represent a promising direction for time series analysis. However, many such methods rely on parameter tuning, and some have shortcomings if the time series are multivariate (MTS), due to dependencies between attributes, or the time series contain missing data. In this paper, we address these challenges within the powerful context of kernel methods by proposing the robust time series cluster kernel (TCK). The approach taken leverages the missing data handling properties of Gaussian mixture models (GMM) augmented with informative prior distributions. An ensemble learning approach is exploited to ensure robustness to parameters by combining the clustering results of many GMM to form the final kernel.

We evaluate the TCK on synthetic and real data and compare to other state-of-the-art techniques. The experimental results demonstrate that the TCK is robust to parameter choices, provides competitive results for MTS without missing data and outstanding results for missing data.},
  citeulike-article-id = {14499039},
  citeulike-linkout-0  = {http://dx.doi.org/10.1016/j.patcog.2017.11.030},
  groups               = {Scenario_TimeSeries, ML_ClustTimeSrs, Scenario_SynthData},
  posted-at            = {2017-12-07 22:36:35},
  timestamp            = {2020-09-08 15:29},
}

@Article{Mulvey-et-al-2019,
  author         = {Mulvey, John M. and Martellini, Lionel and Hao, Han and Li, Nongchao},
  date           = {2019-02-28},
  journaltitle   = {The Journal of Portfolio Management},
  title          = {A Factor- and Goal-Driven Model for Defined Benefit Pensions: Setting Realistic Benefits},
  issue          = {3},
  pages          = {165-177},
  url            = {https://jpm.iijournals.com/content/45/3/165},
  urldate        = {2019-03-05},
  volume         = {45},
  abstract       = {A factor and goal-driven framework for assessing asset allocation and contribution decisions within defined-benefit pension plans is developed in this article. A critical element is setting future benefits with reference to the ability of the pension sponsors to support liabilities under reasonable investment expectations. The approach suggested by the authors combines a micro study of a representative cohort of individuals with an aggregation across a target population to estimate consistency between the micro and macro environments. A stochastic inflation risk factor affects both contribution and spending cash flows. This agent-based model suggested by the authors provides a more realistic framework than traditional approaches for setting pension benefits.},
  day            = {28},
  f1000-projects = {QuantInvest},
  groups         = {Goals_Inflation, Scenario_ABM},
  publisher      = {Institutional Investor Journals Umbrella},
  timestamp      = {2020-09-08 15:31},
}

@Article{Rebonato-2018b,
  author         = {Rebonato, Riccardo},
  date           = {2018},
  journaltitle   = {Journal of Risk},
  title          = {The quickest way to lose the money you cannot afford to lose: reverse stress testing with maximum entropy},
  issn           = {1465-1211},
  number         = {3},
  pages          = {83--93},
  url            = {https://www.risk.net/journal-of-risk/5388476/the-quickest-way-to-lose-the-money-you-cannot-afford-to-lose-reverse-stress-testing-with-maximum-entropy},
  urldate        = {2019-05-30},
  volume         = {20},
  abstract       = {We extend a technique devised by Saroka and Rebonato to "optimally" deform a yield curve in order to deal with a common and practically relevant class of optimization problems subject to linear constraints. In particular, we show how the idea can be applied to the case of reverse stress testing, and we present a case study to illustrate how it works. Finally, we point out a maximum-entropy interpretation of (or justification for) the procedure and present some obvious generalizations.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 15:34},
}

@Article{Skoglund-2019,
  author         = {Skoglund, Jimmy},
  date           = {2019},
  journaltitle   = {Journal of Risk Model Validation},
  title          = {Quantification of model risk in stress testing and scenario analysis},
  issn           = {1753-9579},
  number         = {1},
  pages          = {1-23},
  url            = {https://www.risk.net/journal-of-risk-model-validation/6446831/quantification-of-model-risk-in-stress-testing-and-scenario-analysis},
  urldate        = {2019-03-14},
  volume         = {13},
  abstract       = {Being able to understand and quantify the model risk inherent in loss-projection models used in macroeconomic stress testing and impairment estimation is a significant concern for both banks and regulators. The application of relative entropy techniques allows model misspecification robustness to be numerically quantified using exponential tilting toward an alternative probability law. Employing a particular loss-forecasting model, we quantify the worst-case-loss term structures of that model, yielding insights into the behavior of the worst-case scenario. In general, the worst case obtained represents an upward scaling of the term structure consistent with the exponential tilting adjustment. The relative entropy approach to model risk we use has its foundation in economics with robust forecasting analysis, and it has recently started to be applied in risk management. This technique can complement traditional model risk quantification techniques, where a specific direction or range of model misspecification reasons are usually considered, such as model sensitivity analysis, model parameter uncertainty analysis, competing models and conservative model assumptions.},
  f1000-projects = {QuantInvest},
  timestamp      = {2020-09-08 15:36},
}

@Article{Tang-et-al-2018a,
  author         = {Tang, Xiaoxiao and Hu, Feifang and Wang, Peiming},
  date           = {2018-08},
  journaltitle   = {Journal of Forecasting},
  title          = {Out-of-sample equity premium prediction: A scenario analysis approach},
  doi            = {10.1002/for.2519},
  issn           = {0277-6693},
  number         = {5},
  pages          = {604--626},
  urldate        = {2019-03-07},
  volume         = {37},
  abstract       = {We propose two methods of equity premium prediction with single and multiple predictors respectively and evaluate their out‐of‐sample performance using US stock data with 15 popular predictors for equity premium prediction. The first method defines three scenarios in terms of the expected returns under the scenarios and assumes a Markov chain governing the occurrence of the scenarios over time. It employs predictive quantile regressions of excess return on a predictor for three quantiles to estimate the occurrence of the scenarios over an in‐sample period and the transition probabilities of the Markov chain, predicts the expected returns under the scenarios, and generates an equity premium forecast by combining the predicted expected returns under three scenarios with the estimated transition probabilities. The second method generates an equity premium forecast by combining the individual forecasts from the first method across all predictors. For most of predictors, the first method outperforms the benchmark method of historical average and the traditional predictive linear regression with a single predictor both statistically and economically, and the second method consistently performs better than several competing methods used in the literature. The performance of our methods is further examined under different scenarios and economic conditions, and is robust for two different out‐of‐sample periods and specifications of the scenarios.},
  f1000-projects = {QuantInvest},
  groups         = {FrcstQWIM_Equity},
  timestamp      = {2020-09-08 15:39},
}

@Article{Buehler-et-al-2021,
  author           = {Hans Buehler and Blanka Horvath and Terry Lyons and Imanol Perez Arribas and Ben Wood},
  date             = {2021},
  journaltitle     = {Risk},
  title            = {Generating financial markets with signatures},
  url              = {https://www.risk.net/cutting-edge/banking/7841726/generating-financial-markets-with-signatures},
  abstract         = {While most generative models tend to rely on large amounts of training data, here Hans Buehler et al present a generative model that works reliably even in environments where the amount of available training data is small, irregularly paced or oscillatory. They show how a rough paths-based feature map encoded by the signature of the path outperforms returns-based market generation both numerically and from a theoretical point of view. Finally, they propose a suitable performance evaluation metric for financial time series and discuss some connections between their signature-based market generator and deep hedging},
  creationdate     = {2021-07-02T23:58:26},
  keywords         = {q-fin.ST, cs.LG, q-fin.CP, q-fin.MF, stat.ML},
  modificationdate = {2021-07-02T23:58:26},
  timestamp        = {2020-07-23 13:43},
}

@Article{Buehler-et-al-2020,
  author           = {Hans Buehler and Blanka Horvath and Terry Lyons and Imanol Perez Arribas and Ben Wood},
  date             = {2020-06-21},
  journaltitle     = {arXiv e-Print},
  title            = {A Data-driven Market Simulator for Small Data Environments},
  url              = {https://arxiv.org/abs/2006.14498},
  abstract         = {Neural network based data-driven market simulation unveils a new and flexible way of modelling financial time series without imposing assumptions on the underlying stochastic dynamics. Though in this sense generative market simulation is model-free, the concrete modelling choices are nevertheless decisive for the features of the simulated paths. We give a brief overview of currently used generative modelling approaches and performance evaluation metrics for financial time series, and address some of the challenges to achieve good results in the latter. We also contrast some classical approaches of market simulation with simulation based on generative modelling and highlight some advantages and pitfalls of the new approach. While most generative models tend to rely on large amounts of training data, we present here a generative model that works reliably in environments where the amount of available training data is notoriously small. Furthermore, we show how a rough paths perspective combined with a parsimonious Variational Autoencoder framework provides a powerful way for encoding and evaluating financial time series in such environments where available training data is scarce. Finally, we also propose a suitable performance evaluation metric for financial time series and discuss some connections of our Market Generator to deep hedging.},
  creationdate     = {2021-07-02T23:58:26},
  keywords         = {q-fin.ST, cs.LG, q-fin.CP, q-fin.MF, stat.ML},
  modificationdate = {2021-07-02T23:58:26},
  timestamp        = {2020-07-23 13:31},
}

@Article{Carbonneau-2020,
  author           = {Alexandre Carbonneau},
  date             = {2020-07-29},
  journaltitle     = {arXiv e-Print},
  title            = {Deep Hedging of Long-Term Financial Derivatives},
  eprinttype       = {arXiv},
  url              = {https://arxiv.org/abs/2007.15128},
  abstract         = {This study presents a deep reinforcement learning approach for global hedging of long-term financial derivatives. A similar setup as in Coleman et al. (2007) is considered with the risk management of lookback options embedded in guarantees of variable annuities with ratchet features. The deep hedging algorithm of Buehler et al. (2019a) is applied to optimize neural networks representing global hedging policies with both quadratic and non-quadratic penalties. To the best of the author's knowledge, this is the first paper that presents an extensive benchmarking of global policies for long-term contingent claims with the use of various hedging instruments (e.g. underlying and standard options) and with the presence of jump risk for equity. Monte Carlo experiments demonstrate the vast superiority of non-quadratic global hedging as it results simultaneously in downside risk metrics two to three times smaller than best benchmarks and in significant hedging gains. Analyses show that the neural networks are able to effectively adapt their hedging decisions to different penalties and stylized facts of risky asset dynamics only by experiencing simulations of the financial market exhibiting these features. Numerical results also indicate that non-quadratic global policies are significantly more geared towards being long equity risk which entails earning the equity risk premium.},
  creationdate     = {2021-07-02T23:58:26},
  keywords         = {q-fin.RM, q-fin.CP, q-fin.ST, 91G20 (Primary) 91G60, 91G70 (Secondary)},
  modificationdate = {2021-07-02T23:58:26},
  timestamp        = {2020-09-27 11:15},
}

@Article{Kondratyev-et-al-2020a,
  author           = {Kondratyev, Alexei and Schwarz, Christian and Horvath, Blanka},
  date             = {2020},
  journaltitle     = {SSRN e-Print},
  title            = {Data Anonymisation, Outlier Detection and Fighting Overfitting with Restricted Boltzmann Machines},
  doi              = {10.2139/ssrn.3526436},
  issn             = {1556-5068},
  url              = {https://www.ssrn.com/abstract=3526436},
  urldate          = {2020-03-06},
  abstract         = {We propose a novel approach to the anonymisation of datasets through non-parametric learning of the underlying multivariate distribution of dataset features and generation of the new synthetic samples from the learned distribution. The main objective is to ensure equal (or better) performance of the classifiers and regressors trained on synthetic datasets in comparison with the same classifiers and regressors trained on the original data. The ability to generate unlimited number of synthetic data samples from the learned distribution can be a remedy in fighting overtting when dealing with small original datasets. When the synthetic data generator is trained as an autoencoder with the bottleneck information compression structure we can also expect to see a reduced number of outliers in the generated datasets, thus further improving the generalization capabilities of the classifiers trained on synthetic data. We achieve these objectives with the help of the Restricted Boltzmann Machine, a special type of generative neural network that possesses all the required properties of a powerful data anonymiser.},
  creationdate     = {2021-07-03T00:01:17},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_SynthData},
  modificationdate = {2021-07-03T00:01:17},
  timestamp        = {2020-08-20 11:56},
}

@Article{Kondratyev-Schwarz-2020,
  author           = {Kondratyev, Alexei and Schwarz, Christian},
  date             = {2020},
  journaltitle     = {Risk},
  title            = {The Market Generator},
  issn             = {1556-5068},
  url              = {https://www.risk.net/cutting-edge/7401191/the-market-generator},
  urldate          = {2019-11-07},
  abstract         = {We propose to use a special type of generative neural network - a Restricted Boltzmann Machine (RBM) - to build a powerful generator of synthetic market data that can replicate the probability distribution of the original market data. An RBM constructed with stochastic binary activation units in both the hidden and the visible layers (Bernoulli RBM) can learn complex dependence structures while avoiding overfitting. In this paper we consider an efficient data transformation and sampling approach that allows us to operate Bernoulli RBM on real-valued data sets and control the degree of autocorrelation and non-stationarity in the generated time series.},
  creationdate     = {2021-07-03T00:01:17},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_SynthData},
  modificationdate = {2021-07-03T00:01:17},
  timestamp        = {2020-08-20 13:04},
}

@Article{Kondratyev-et-al-2020,
  author           = {Kondratyev, Alexei and Schwarz, Christian and Horvath, Blanka},
  date             = {2020},
  journaltitle     = {Risk},
  title            = {Data Anonymisation, Outlier Detection and Fighting Overfitting with Restricted Boltzmann Machines},
  url              = {https://www.risk.net/cutting-edge/banking/7669111/the-data-anonymiser},
  abstract         = {We propose a novel approach to the anonymisation of datasets through non-parametric learning of the underlying multivariate distribution of dataset features and generation of the new synthetic samples from the learned distribution. The main objective is to ensure equal (or better) performance of the classifiers and regressors trained on synthetic datasets in comparison with the same classifiers and regressors trained on the original data. The ability to generate unlimited number of synthetic data samples from the learned distribution can be a remedy in fighting overtting when dealing with small original datasets. When the synthetic data generator is trained as an autoencoder with the bottleneck information compression structure we can also expect to see a reduced number of outliers in the generated datasets, thus further improving the generalization capabilities of the classifiers trained on synthetic data. We achieve these objectives with the help of the Restricted Boltzmann Machine, a special type of generative neural network that possesses all the required properties of a powerful data anonymiser.},
  creationdate     = {2021-07-03T00:01:17},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_SynthData},
  modificationdate = {2021-07-03T00:01:17},
  timestamp        = {2020-08-20 11:57},
}

@PhdThesis{Koshiyama-2020,
  author           = {Koshiyama, Adriano Soares},
  date             = {2020},
  institution      = {University College London},
  title            = {Applications of Machine Learning Methods in Financial Markets},
  url              = {https://discovery.ucl.ac.uk/id/eprint/10110310/},
  abstract         = {This thesis investigates the application of new machine learning algorithms like   Generative Adversarial Networks (GANs), Transfer Learning, etc. as building blocks for reliable investment decisions. To improve a machine learning-based trading strategy assessment one needs to consider the problem of backtest overfitting - strategies outperforming on training data but underperform when presented with new data. In this sense, this thesis considers three independent forms to deal with this problem: a) correcting performance metrics, such as the Sharpe ratio, using covariance-penalty methods; b) using GANs to synthesize financial time series data and use them to improve trading strategies calibration and performance assessment; and c) developing novel Transfer Learning-based strategies to deal with data scarcity, spurious correlation and, by consequence, aiding in avoiding backtest overfitting. This research is important for several reasons: (i) backtest overfitting is a pervasive problem that impacts practitioners and researchers across the financial markets; (ii) the increasing adoption of data-driven methodologies, particularly machine learning methods across markets is demanding new forms of model validation and assessment; and (iii) GANs and Transfer Learning are modelling paradigms with proven success in areas such as computer vision, natural language processing, yet with insubstantial amount of research in the finance domain. This research comprises three experiments briefly described below: Avoiding Backtesting Overfitting by Covariance-Penalties - in this experiment we propose a new approach to deal with financial overfitting, a covariance-penalty correction, in which a risk metric is adjusted given the number of parameters and amount of data used to underpin a trading strategy. We outline the theoretical foundation and main results behind the covariance-penalty correction for trading strategies. After that, we pursue an empirical investigation and compare its performance with some other approaches in the realm of covariance-penalties across more than 1,300 assets, using ordinary and total least squares. The findings suggest that covariance-penalties are a suitable procedure to avoid backtesting overfitting, and total least squares outperforms ordinary least squares. Generative Adversarial Networks for Financial Trading Strategies - this experiment proposes the use of Conditional GANs (cGANs) for trading strategies calibration and aggregation. To this purpose, we provide a full methodology on: (i) the training and selection of a cGAN for time series data; (ii) how each sample is used for strategies calibration; and (iii) how all generated samples can be used for ensemble modelling. To provide evidence that our approach is well grounded, we have designed an experiment with multiple trading strategies, encompassing 579 assets. We compared cGAN with an ensemble scheme and model validation methods, both suited for time series. Our results suggest that cGANs are an efficient alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha. Transferring Learning Across Trading Strategies - in this experiment we introduce QuantNet: an architecture that is capable to transfer knowledge across systematic trading strategies in several financial markets. By having a system that can leverage and share knowledge across them, our aim is two-fold: to circumvent the so-called backtest overfitting problem; and to generate higher risk-adjusted returns and less drawdowns. In order to evaluate QuantNet, we compared its performance to the option of not performing transfer learning, that is, using market-specific old-fashioned machine learning. In summary, our findings suggest that QuantNet performs better than non transfer-based trading strategies, improving Sharpe ratio in 15\% and Calmar ratio in 41\% across 3103 assets in 58 equity markets across the world. The major contributions of this work are: A covariance-penalty correction formula that can be widely used by quantitative strategists in the financial services; A new approach to synthesise financial time series that can be applied to calibrate and improve model selection and assessment; A new framework to learn systematic trading strategies by cross-pollinating gained knowledge in different financial markets. This work has been done in conjunction with Nomura International, Goldman Sachs International, and The Alan Turing Institute. This research has directly resulted in 7 papers.},
  creationdate     = {2021-07-03T00:02:04},
  modificationdate = {2021-07-03T00:02:04},
  timestamp        = {2020-11-18 11:39},
}

@Article{Koshiyama-et-al-2021,
  author           = {Koshiyama, Adriano and Firoozye, Nick and Treleaven, Philip},
  date             = {2021},
  journaltitle     = {Quantitative Finance},
  title            = {Generative Adversarial Networks for Financial Trading Strategies Fine-Tuning and Combination},
  doi              = {10.1080/14697688.2020.1790635},
  number           = {5},
  pages            = {797-813},
  volume           = {21},
  abstract         = {Systematic trading strategies are algorithmic procedures that allocate assets aiming to optimize a certain performance criterion. To obtain an edge in a highly competitive environment, the analyst needs to proper fine-tune its strategy, or discover how to combine weak signals in novel alpha creating manners. Both aspects, namely fine-tuning and combination, have been extensively researched using several methods, but emerging techniques such as Generative Adversarial Networks can have an impact into such aspects. Therefore, our work proposes the use of Conditional Generative Adversarial Networks (cGANs) for trading strategies calibration and aggregation. To this purpose, we provide a full methodology on: (i) the training and selection of a cGAN for time series data; (ii) how each sample is used for strategies calibration; and (iii) how all generated samples can be used for ensemble modelling. To provide evidence that our approach is well grounded, we have designed an experiment with multiple trading strategies, encompassing 579 assets. We compared cGAN with an ensemble scheme and model validation methods, both suited for time series. Our results suggest that cGANs are a suitable alternative for strategies calibration and combination, providing outperformance when the traditional techniques fail to generate any alpha.},
  creationdate     = {2021-07-03T00:02:04},
  day              = {7},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:02:04},
  timestamp        = {2020-09-06 01:45},
}

@Article{Marti-et-al-2020,
  author           = {Marti, Gautier and Nielsen, Frank and Bihkowski, Mikolaj and Donnat, Philippe},
  date             = {2020},
  journaltitle     = {arXiv e-Print},
  title            = {A review of two decades of correlations, hierarchies, networks and clustering in financial markets},
  eprinttype       = {arXiv},
  url              = {https:://arxiv.org/abs/1703.00485},
  urldate          = {2020-10-07},
  abstract         = {This document is a preliminary version of an in-depth review on the state of the art of clustering financial time series and the study of correlation networks. This preliminary document is intended for researchers in this field so that they can feedback to allow amendments, corrections and addition of new material unknown to the authors of this review. The aim of the document is to gather in one place the relevant material that can help the researcher in the field to have a bigger picture, the quantitative researcher to play with this alternative modeling of the financial time series, and the decision maker to leverage the insights obtained from these methods. We hope that this document will form a basis for implementation of an open toolbox of standard tools to study correlations, hierarchies, networks and clustering in financial markets. We also plan to maintain pointers to online material and an updated version of this work at www.datagrapple.com/Tech.},
  creationdate     = {2021-07-03T00:02:46},
  day              = {1},
  groups           = {Networks and investment management, Clustering and network analysis, Invest_Network},
  modificationdate = {2021-07-03T00:02:46},
  timestamp        = {2020-10-23 13:33},
}

@InProceedings{Marti-2020,
  author           = {Marti, Gautier},
  booktitle        = {ICASSP - IEEE nternational Conference on Acoustics, Speech and Signal Processing},
  date             = {2020-05-04},
  title            = {{CORRGAN}: sampling realistic financial correlation matrices using generative adversarial networks},
  doi              = {10.1109/ICASSP40776.2020.9053276},
  isbn             = {978-1-5090-6631-5},
  pages            = {8459-8463},
  publisher        = {IEEE},
  url              = {https://ieeexplore.ieee.org/document/9053276/},
  urldate          = {2020-06-19},
  abstract         = {We propose a novel approach for sampling realistic financial correlation matrices. This approach is based on generative adversarial networks. Experiments demonstrate that generative adversarial networks are able to recover most of the known stylized facts about empirical correlation matrices estimated on asset returns. This is the first time such results are documented in the literature. Practical financial applications range from trading strategies enhancement to risk and portfolio stress testing. Such generative models can also help ground empirical finance deeper into science by allowing for falsifiability of statements and more objective comparison of empirical methods.},
  creationdate     = {2021-07-03T00:02:46},
  day              = {4},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:02:46},
  timestamp        = {2020-06-20 01:45},
}

@Article{Ardia-Bluteau-2017,
  author           = {David Ardia and Keven Bluteau},
  date             = {2017},
  journaltitle     = {Wilmott},
  title            = {Stress-Testing With Parametric Models and Fully Flexible Probabilities},
  doi              = {10.1002/wilm.10565},
  number           = {87},
  pages            = {52--55},
  volume           = {2017},
  abstract         = {We propose a simple methodology to simulate scenarios from a parametric risk model while accounting for stress-test views via fully flexible probabilities.},
  creationdate     = {2021-07-03T00:03:42},
  modificationdate = {2021-07-03T00:03:42},
  timestamp        = {2020-07-25 08:43},
}

@Article{Meucci-et-al-2012,
  author           = {Meucci, Attilio and Ardia, David and Keel, Simon},
  date             = {2012},
  journaltitle     = {Risk},
  title            = {Fully Flexible Extreme Views},
  pages            = {39--49},
  volume           = {14(2)},
  abstract         = {We extend the Fully Flexible Views generalization of the Black-Litterman approach to effectively handle extreme views on the tails of a distribution.

First, we provide a recursive algorithm to process views on the conditional value at risk, which cannot be handled directly by the original implementation of Fully Flexible Views. Second, we represent both the prior and the posterior distribution on a grid, instead of by means of Monte Carlo scenarios: this way it becomes possible to cover parsimoniously even the far tails of the underlying distribution.

Documented code is available for download.},
  creationdate     = {2021-07-03T00:04:22},
  howpublished     = {Available at SSRN: http://ssrn.com/abstract=1542083},
  keywords         = {Entropy Pooling, Kullback-Leibler, Black-Litterman, VaR, CVaR, grid-probability pair, Monte Carlo, Gauss-Hermite polynomials, Newton-Raphson, kernel estimator},
  modificationdate = {2021-07-03T00:04:22},
  owner            = {cristi},
  timestamp        = {2019-09-14 22:09},
}

@Article{Meucci-et-al-2015,
  author           = {Attilio Meucci and Alberto Santangelo and Romain Deguest},
  date             = {2015},
  journaltitle     = {Risk},
  title            = {Risk budgeting and diversification based on optimised uncorrelated factors},
  url              = {https://www.risk.net/risk-management/2433224/risk-budgeting-and-diversification-based-on-optimised-uncorrelated-factors},
  abstract         = {We measure the contributions to risk of a set of factors, strategies, or investments, based on "Minimum-Torsion Bets", namely a set of uncorrelated factors, optimized to closely track the factors used to allocate the portfolio. We then introduce a novel definition of contributions to risk, which generalizes the "marginal contributions to risk", traditionally used in banks for risk budgeting and in asset management to build risk parity strategies.

The Minimum-Torsion Bets allow us to also introduce a natural diversification score, the Effective Number of Minimum-Torsion Bets, which we use to measure and manage diversification.

We discuss the advantages of the Minimum-Torsion Bets over the traditional approach to diversification based on marginal contributions to risk. We present two case studies, a security-based investment in the stocks of the S\&P 500, and a factor-based investment in the five Fama-French factors.},
  creationdate     = {2021-07-03T00:04:22},
  groups           = {Risk_Budgeting, Invest_Risk, Invest_Diversif},
  modificationdate = {2021-07-03T00:04:22},
  owner            = {zkgst0c},
  timestamp        = {2020-08-20 13:05},
}

@Article{Sebastian-Gebbie-2019,
  author           = {Sebastian, Ann and Gebbie, Tim},
  date             = {2019-10-12},
  journaltitle     = {arXiv e-Print},
  title            = {Systematic Asset Allocation using Flexible Views for South African Markets},
  url              = {https://arxiv.org/abs/1910.05555},
  urldate          = {2019-10-24},
  abstract         = {We implement a systematic asset allocation model using the Historical Simulation with Flexible Probabilities (HS-FP) framework developed by Meucci. The HS-FP framework is a flexible non-parametric estimation approach that considers future asset class behavior to be conditional on time and market environments, and derives a forward looking distribution that is consistent with this view while remaining close as possible to the prior distribution. The framework derives the forward looking distribution by applying unequal time and state conditioned probabilities to historical observations of asset class returns. This is achieved using relative entropy to find estimates with the least distortion to the prior distribution. Here, we use the HS-FP framework on South African financial market data for asset allocation purposes; by estimating expected returns, correlations and volatilities that are better represented through the measured market cycle. We demonstrated a range of state variables that can be useful towards understanding market environments. Concretely, we compare the out-of-sample performance for a specific configuration of the HS-FP model relative to classic Mean Variance Optimization(MVO) and Equally Weighted (EW) benchmark models. The framework displays low probability of backtest overfitting and the out-of-sample net returns and Sharpe ratio point estimates of the HS-FP model outperforms the benchmark models. However, the results are inconsistent when training windows are varied, the Sharpe ratio is seen to be inflated, and the method does not demonstrate statistically significant out-performance on a gross and net basis.},
  creationdate     = {2021-07-03T00:04:41},
  day              = {12},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:04:41},
}

@Article{Bertsimas-Vanparys-2017,
  author           = {Bertsimas, Dimitris and Van Parys, Bart},
  date             = {2017-11-27},
  journaltitle     = {arXiv e-Print},
  title            = {Bootstrap Robust Prescriptive Analytics},
  url              = {https://arxiv.org/abs/1711.09974},
  urldate          = {2019-10-12},
  abstract         = {We address the problem of prescribing an optimal decision in a framework where its cost depends on uncertain problem parameters $Y$ that need to be learned from data. Earlier work by Bertsimas and Kallus (2014) transforms classical machine learning methods that merely predict $Y$ from supervised training data $[(x1, y1), , (xn, yn)]$ into prescriptive methods taking optimal decisions specific to a particular covariate context $X= x$. Their prescriptive methods factor in additional observed contextual information on a potentially large number of covariates $X= x$ to take context specific actions $z( x)$ which are superior to any static decision $z$. Any naive use of limited training data may, however, lead to gullible decisions over-calibrated to one particular data set. In this paper, we borrow ideas from distributionally robust optimization and the statistical bootstrap of Efron (1982) to propose two novel prescriptive methods based on (nw) Nadaraya-Watson and (nn) nearest-neighbors learning which safeguard against overfitting and lead to improved out-of-sample performance. Both resulting robust prescriptive methods reduce to tractable convex optimization problems and enjoy a limited disappointment on bootstrap data. We illustrate the data-driven decision-making framework and our novel robustness notion on a small news vendor problem as well as a small portfolio allocation problem.},
  creationdate     = {2021-07-03T00:05:13},
  day              = {27},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:05:13},
  timestamp        = {2020-07-23 13:31},
}

@Article{Cavaliere-et-al-2020,
  author               = {Cavaliere, Giuseppe and Heino Bohn Nielsen and Rahbek, Anders},
  date                 = {2020},
  journaltitle         = {SSRN e-Print},
  title                = {An Introduction to Bootstrap Theory in Time Series Econometrics},
  url                  = {https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3589144},
  abstract             = {This article provides an introduction to methods and challenges underlying application of the bootstrap in econometric modelling of economic and financial time series. Validity, or asymptotic validity, of the bootstrap is discussed as this is a key element in deciding whether the bootstrap is applicable in empirical contexts. That is, as detailed here, bootstrap validity relies on regularity conditions, which need to be verified on a case-by-case basis. To fix ideas, asymptotic validity is discussed in terms of the leading example of bootstrap-based hypothesis testing in the well-known first order auto-regressive model. In particular, bootstrap versions of classic convergence in probability and distribution, and hence of laws of large numbers and central limit theorems, are discussed as crucial ingredients to establish bootstrap validity. Regularity conditions and their implications for possible improvements in terms of (empirical) size and power for bootstrap-based testing, when compared to asymptotic testing, are illustrated by simulations. Following this, an overview of selected recent advances in the application of bootstrap methods in econometrics is also given.},
  citeulike-article-id = {14332230},
  citeulike-linkout-0  = {http://dx.doi.org/10.1111/jtsa.12128},
  creationdate         = {2021-07-03T00:05:34},
  modificationdate     = {2021-07-03T00:05:34},
  posted-at            = {2017-04-05 11:16:42},
  timestamp            = {2020-09-05 17:09},
}

@Article{Cavaliere-et-al-2020a,
  author           = {Giuseppe Cavaliere and Heino Bohn Nielsen and Rasmus S{\o}ndergaard Pedersen and Anders Rahbek},
  date             = {2020-09},
  journaltitle     = {Journal of Econometrics},
  title            = {Bootstrap inference on the boundary of the parameter space, with application to conditional volatility models},
  doi              = {10.1016/j.jeconom.2020.05.006},
  abstract         = {It is a well-established fact that - with an unknown number of nuisance parameters at the boundary - testing a null hypothesis on the boundary of the parameter space is infeasible in practice as the limiting distributions of standard test statistics are non-pivotal. In particular, likelihood ratio statistics have limiting distributions which can be characterized in terms of quadratic forms minimized over cones, where the shape of the cones depends on the unknown location of the (possibly multiple) model parameters not restricted by the null hypothesis. We propose to solve this inference problem by a novel bootstrap, which we show to be valid under general conditions, irrespective of the presence of (unknown) nuisance parameters on the boundary. That is, the new bootstrap replicates the unknown limiting distribution of the likelihood ratio statistic under the null hypothesis and is bounded (in probability) under the alternative. The new bootstrap approach, which is very simple to implement, is based on shrinkage of the parameter estimates used to generate the bootstrap sample toward the boundary of the parameter space at an appropriate rate. As an application of our general theory, we treat the problem of inference in finite-order ARCH models with coefficients subject to inequality constraints. Extensive Monte Carlo simulations illustrate that the proposed bootstrap has attractive finite sample properties both under the null and under the alternative hypothesis.},
  creationdate     = {2021-07-03T00:05:34},
  modificationdate = {2021-07-03T00:05:34},
  publisher        = {Elsevier {BV}},
  timestamp        = {2020-09-28 15:05},
}

@Article{Cogneau-Zakamouline-2013,
  author               = {Cogneau, Philippe and Zakamouline, Valeri},
  date                 = {2013-09},
  journaltitle         = {Quantitative Finance},
  title                = {Block bootstrap methods and the choice of stocks for the long run},
  doi                  = {10.1080/14697688.2012.713115},
  number               = {9},
  pages                = {1443--1457},
  volume               = {13},
  abstract             = {Financial advisors commonly recommend that the investment horizon should be rather long in order to benefit from the ?time diversification?. In this case, in order to choose the optimal portfolio, it is necessary to estimate the risk and reward of several alternative portfolios over a long-run given a sample of observations over a short-run. Two interrelated obstacles in these estimations are lack of sufficient data and the uncertainty in the nature of the return generating process.

To overcome these obstacles researchers rely heavily on block bootstrap methods. In this paper we demonstrate that the estimates provided by a block bootstrap method are generally biased and we propose two methods of bias reduction. We show that an improper use of a block bootstrap method usually causes underestimation of the risk of a portfolio whose returns are independent over time and overestimation of the risk of a portfolio whose returns are mean-reverting.},
  citeulike-article-id = {13933237},
  citeulike-linkout-0  = {http://dx.doi.org/10.1080/14697688.2012.713115},
  citeulike-linkout-1  = {http://www.tandfonline.com/doi/abs/10.1080/14697688.2012.713115},
  creationdate         = {2021-07-03T00:07:11},
  day                  = {1},
  modificationdate     = {2021-07-03T00:07:12},
  owner                = {cristi},
  posted-at            = {2016-02-15 16:47:57},
  publisher            = {Routledge},
  timestamp            = {2019-09-02 10:07},
}

@Article{ElKaroui-Purdom-2018,
  author           = {{El Karoui}, Noureddine and Purdom, Elizabeth},
  date             = {2018},
  journaltitle     = {The Journal of Machine Learning Research},
  title            = {Can we trust the bootstrap in high-dimensions? the case of linear models},
  doi              = {10.5555/3291125.3291130},
  urldate          = {2020-01-23},
  abstract         = {We consider the performance of the bootstrap in high-dimensions for the setting of linear regression, where p < n  but p/np/n is not close to zero. We consider ordinary least-squares as well as robust regression methods and adopt a minimalist performance requirement: can the bootstrap give us good confidence intervals for a single coordinate of beta (where beta is the true regression vector)? We show through a mix of numerical and theoretical work that the bootstrap is fraught with problems. Both of the most commonly used methods of bootstrapping for regression-residual bootstrap and pairs bootstrap-give very poor inference on beta as the ratio p/np/n grows. We find that the residual bootstrap tend to give anti-conservative estimates (inflated Type I error), while the pairs bootstrap gives very conservative estimates (severe loss of power) as the ratio p/np/n grows. We also show that the jackknife resampling technique for estimating the variance of betaHat severely overestimates the variance in high dimensions. We contribute alternative procedures based on our theoretical results that result in dimensionality adaptive and robust bootstrap methods.},
  creationdate     = {2021-07-03T00:08:36},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:08:36},
  timestamp        = {2020-01-24 09:08},
}

@Article{Hambuckers-Heuchenne-2016,
  author               = {Hambuckers, Julien and Heuchenne, Cedric},
  date                 = {2016-07},
  journaltitle         = {Journal of Forecasting},
  title                = {Estimating the Out-of-Sample Predictive Ability of Trading Rules: A Robust Bootstrap Approach},
  doi                  = {10.1002/for.2380},
  number               = {4},
  pages                = {347--372},
  volume               = {35},
  abstract             = {In this paper, we provide a novel way to estimate the out-of-sample predictive ability of a trading rule. Usually, this ability is estimated using a sample-splitting scheme, true out-of-sample data being rarely available. We argue that this method makes poor use of the available data and creates data-mining possibilities. Instead, we introduce an alternative.632 bootstrap approach. This method enables building in-sample and out-of-sample bootstrap datasets that do not overlap but exhibit the same time dependencies. We show in a simulation study that this technique drastically reduces the mean squared error of the estimated predictive ability. We illustrate our methodology on IBM, MSFT and DJIA stock prices, where we compare 11 trading rules specifications. For the considered datasets, two different filter rule specifications have the highest out-of-sample mean excess returns. However, all tested rules cannot beat a simple buy-and-hold strategy when trading at a daily frequency.},
  citeulike-article-id = {14071857},
  citeulike-linkout-0  = {http://dx.doi.org/10.1002/for.2380},
  creationdate         = {2021-07-03T00:08:55},
  day                  = {1},
  groups               = {FrcstQWIM_Test},
  modificationdate     = {2021-07-03T00:08:55},
  owner                = {cristi},
  posted-at            = {2016-06-18 19:17:36},
  timestamp            = {2019-09-14 21:53},
}

@Article{Honore-Hu-2017,
  author               = {Honore, Bo E. and Hu, Luojia},
  date                 = {2017-07-01},
  journaltitle         = {Econometrica},
  title                = {Poor (Wo)man's Bootstrap},
  doi                  = {10.3982/ecta13465},
  issn                 = {0012-9682},
  number               = {4},
  pages                = {1277--1301},
  volume               = {85},
  abstract             = {The bootstrap is a convenient tool for calculating standard errors of the parameter estimates of complicated econometric models. Unfortunately, the fact that these models are complicated often makes the bootstrap extremely slow or even practically infeasible. This paper proposes an alternative to the bootstrap that relies only on the estimation of one-dimensional parameters. We introduce the idea in the context of M and GMM estimators. A modification of the approach can be used to estimate the variance of two-step estimators.},
  citeulike-article-id = {14399522},
  citeulike-linkout-0  = {http://dx.doi.org/10.3982/ecta13465},
  creationdate         = {2021-07-03T00:09:39},
  day                  = {1},
  modificationdate     = {2021-07-03T00:09:40},
  posted-at            = {2018-01-08 11:35:45},
  publisher            = {Blackwell Publishing Ltd},
  timestamp            = {2019-09-14 21:53},
}

@Article{Horowitz-2019,
  author           = {Horowitz, Joel L.},
  date             = {2019-08-02},
  journaltitle     = {Annual review of economics},
  title            = {Bootstrap methods in econometrics},
  doi              = {10.1146/annurev-economics-080218-025651},
  issn             = {1941-1383},
  number           = {1},
  pages            = {193--224},
  urldate          = {2020-03-19},
  volume           = {11},
  abstract         = {The bootstrap is a method for estimating the distribution of an estimator or test statistic by resampling one's data or a model estimated from the data. Under conditions that hold in a wide variety of econometric applications, the bootstrap provides approximations to distributions of statistics, coverage probabilities of confidence intervals, and rejection probabilities of hypothesis tests that are more accurate than the approximations of first-order asymptotic distribution theory. The reductions in the differences between true and nominal coverage or rejection probabilities can be very large. In addition, the bootstrap provides a way to carry out inference in certain settings where obtaining analytic distributional approximations is difficult or impossible. This article explains the usefulness and limitations of the bootstrap in contexts of interest in econometrics. The presentation is informal and expository. It provides an intuitive understanding of how the bootstrap works. Mathematical details are available in the references that are cited.},
  creationdate     = {2021-07-03T00:11:05},
  day              = {2},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:11:05},
  timestamp        = {2020-04-06 20:13},
}

@InCollection{Sani-et-al-2015,
  author           = {Sani, Amir and Lazaric, Alessandro and Ryabko, Daniil},
  booktitle        = {IEEE International Symposium on Information Theory (ISIT)},
  date             = {2015-06-14},
  title            = {The replacement bootstrap for dependent data},
  doi              = {10.1109/ISIT.2015.7282644},
  isbn             = {978-1-4673-7704-1},
  pages            = {1194--1198},
  publisher        = {IEEE},
  url              = {http://ieeexplore.ieee.org/document/7282644/},
  abstract         = {Applications that deal with time-series data often require evaluating complex statistics for which each time series is essentially one data point. When only a few time series are available, bootstrap methods are used to generate additional samples that can be used to evaluate empirically the statistic of interest. In this work a novel bootstrap method is proposed, which is shown to have some asymptotic consistency guarantees under the only assumption that the time series are stationary and ergodic. This contrasts previously available results that impose mixing or finite-memory assumptions on the data. Empirical evaluation on simulated and real data, using a practically relevant and complex extrema statistic is provided.},
  creationdate     = {2021-07-03T00:11:28},
  day              = {14},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:11:28},
}

@InCollection{Wang-Tu-2014,
  author           = {Wang, Honglang and Tu, Wanzhu},
  booktitle        = {Wiley statsref: statistics reference online},
  date             = {2014-04-14},
  title            = {Bootstrap methods: the classical theory and recent development},
  doi              = {10.1002/9781118445112.stat04579.pub2},
  editor           = {Balakrishnan, N. and Colton, Theodore and Everitt, Brian and Piegorsch, Walter and Ruggeri, Fabrizio and Teugels, Jozef L.},
  isbn             = {9781118445112},
  location         = {Chichester, UK},
  pages            = {1--12},
  publisher        = {John Wiley \& Sons, Ltd},
  urldate          = {2020-01-23},
  abstract         = {Bootstrap is a resampling method for statistical inference. Under fairly general conditions, the technique can be used to approximate sampling distributions of almost any statistics, by recycling data from the observed sample, that is, resampling. In this article, we review the theoretical tenets of bootstrapping, focusing primarily on the fundamental property of consistency, while showing examples where lack of consistency can lead to failures of the method. We also describe residual and pairs bootstrap methods in linear models, as well as their applications in low- and high-dimensional problems. Finally, we discuss a modified bootstrap procedure in big data situations.},
  creationdate     = {2021-07-03T00:11:48},
  day              = {14},
  f1000-projects   = {QuantInvest},
  modificationdate = {2021-07-03T00:11:48},
}

@Article{Tsamardinos-et-al-2018,
  author           = {Tsamardinos, Ioannis and Greasidou, Elissavet and Borboudakis, Giorgos},
  date             = {2018-05-09},
  journaltitle     = {Machine Learning},
  title            = {Bootstrapping the out-of-sample predictions for efficient and accurate cross-validation},
  doi              = {10.1007/s10994-018-5714-4},
  issn             = {0885-6125},
  pages            = {1--28},
  abstract         = {Cross-Validation (CV), and out-of-sample performance-estimation protocols in general, are often employed both for (a) selecting the optimal combination of algorithms and values of hyper-parameters (called a configuration) for producing the final predictive model, and (b) estimating the predictive performance of the final model. However, the cross-validated performance of the best configuration is optimistically biased. We present an efficient bootstrap method that corrects for the bias, called Bootstrap Bias Corrected CV (BBC-CV). BBC-CV main idea is to bootstrap the whole process of selecting the best-performing configuration on the out-of-sample predictions of each configuration, without additional training of models. In comparison to the alternatives, namely the nested cross-validation (Varma and Simon in BMC Bioinform 7(1):91, 2006) and a method by Tibshirani and Tibshirani (Ann Appl Stat 822-829, 2009), BBC-CV is computationally more efficient, has smaller variance and bias, and is applicable to any metric of performance (accuracy, AUC, concordance index, mean squared error). Subsequently, we employ again the idea of bootstrapping the out-of-sample predictions to speed up the CV process. Specifically, using a bootstrap-based statistical criterion we stop training of models on new folds of inferior (with high probability) configurations. We name the method Bootstrap Bias Corrected with Dropping CV (BBCD-CV) that is both efficient and provides accurate performance estimates.},
  creationdate     = {2021-07-03T00:12:13},
  day              = {9},
  f1000-projects   = {QuantInvest},
  groups           = {ML_Test_CrossVal, ML_Validation},
  modificationdate = {2021-07-03T00:12:13},
  pmcid            = {PMC6191021},
}

@InCollection{Romano-Wolf-2018,
  author           = {Romano, Joseph P. and Wolf, Michael},
  booktitle        = {Predictive econometrics and big data},
  date             = {2018},
  title            = {Multiple Testing of One-Sided Hypotheses: Combining Bonferroni and the Bootstrap},
  doi              = {10.1007/978-3-319-70942-0_4},
  editor           = {Kreinovich, Vladik and Sriboonchitta, Songsak and Chakpitak, Nopasit},
  isbn             = {978-3-319-70941-3},
  pages            = {78--94},
  publisher        = {Springer International Publishing},
  volume           = {753},
  abstract         = {In many multiple testing problems, the individual null hypotheses (i) concern univariate parameters and (ii) are one-sided. In such problems, power gains can be obtained for bootstrap multiple testing procedures in scenarios where some of the parameters are 'deep in the null' by making certain adjustments to the null distribution under which to resample. In this paper, we compare a Bonferroni adjustment that is based on finite-sample considerations with certain 'asymptotic' adjustments previously suggested in the literature.},
  creationdate     = {2021-07-03T00:12:41},
  f1000-projects   = {QuantInvest},
  groups           = {Test_MultiHypotheses, Test_Scenario},
  issn             = {1860-{949X}},
  modificationdate = {2021-07-03T00:12:41},
}

@Book{Chihara-Hesterberg-2018,
  author           = {Laura M. Chihara and Tim C. Hesterberg},
  date             = {2018},
  title            = {Mathematical Statistics with Resampling and R, 2nd Edition},
  pagetotal        = {560},
  publisher        = {Wiley},
  url              = {https://www.wiley.com/en-us/Mathematical Statistics with Resampling and R, 2nd Edition-p-9781119416531},
  abstract         = {Resampling helps students understand the meaning of sampling distributions, sampling variability, P-values, hypothesis tests, and confidence intervals. The second edition of Mathematical Statistics with Resampling and R combines modern resampling techniques and mathematical statistics. This book has been classroom-tested to ensure an accessible presentation, uses the powerful and flexible computer language R for data analysis and explores the benefits of modern resampling techniques.

This book offers an introduction to permutation tests and bootstrap methods that can serve to motivate classical inference methods. The book strikes a balance between theory, computing, and applications, and the new edition explores additional topics including consulting, paired t test, ANOVA and Google Interview Questions. Throughout the book, new and updated case studies are included representing a diverse range of subjects such as flight delays, birth weights of babies, and telephone company repair times. These illustrate the relevance of the real-world applications of the material. This new edition:
-- Puts the focus on statistical consulting that emphasizes giving a client an understanding of data and goes beyond typical expectations
-- Presents new material on topics such as the paired t test, Fisher's Exact Test and the EM algorithm
-- Offers a new section on "Google Interview Questions" that illustrates statistical thinking
-- Provides a new chapter on ANOVA
-- Contains more exercises and updated case studies, data sets, and R code 

Written for undergraduate students in a mathematical statistics course as well as practitioners and researchers, the second edition of Mathematical Statistics with Resampling and R presents a revised and updated guide for applying the most current resampling techniques to mathematical statistics.},
  creationdate     = {2021-07-03T00:13:24},
  modificationdate = {2021-07-03T00:13:24},
  timestamp        = {2020-05-27 13:15},
}

@Article{Frahm-2015,
  author               = {Frahm, Gabriel},
  date                 = {2015},
  journaltitle         = {Theory and Decision},
  title                = {A theoretical foundation of portfolio resampling},
  doi                  = {10.1007/s11238-014-9453-0},
  number               = {1},
  pages                = {107--132},
  volume               = {79},
  abstract             = {A portfolio-resampling procedure invented by Richard and Robert Michaud is a subject of highly controversial discussion and big scientific dispute. It has been evaluated in many empirical studies and Monte Carlo experiments. Apart from the contradictory findings, the Michaud approach still lacks a theoretical foundation. I prove that portfolio resampling has a strong foundation in the classic theory of rational behavior. Every noise trader could do better by applying the Michaud procedure. By contrast, a signal trader who has enough prediction power and risk-management skills should refrain from portfolio resampling. The key note is that in most simulation studies, investors are considered as noise traders. This explains why portfolio resampling performs well in simulation studies, but could be mediocre in real life.},
  citeulike-article-id = {13992401},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/s11238-014-9453-0},
  citeulike-linkout-1  = {http://link.springer.com/article/10.1007/s11238-014-9453-0},
  creationdate         = {2021-07-03T00:13:41},
  modificationdate     = {2021-07-03T00:13:41},
  owner                = {cristi},
  posted-at            = {2016-04-01 12:30:15},
  publisher            = {Springer US},
  timestamp            = {2019-09-14 21:43},
}

@InCollection{Mammen-Nandi-2012,
  author               = {Mammen, Enno and Nandi, Swagata},
  booktitle            = {Handbook of Computational Statistics},
  date                 = {2012},
  title                = {Bootstrap and Resampling},
  doi                  = {10.1007/978-3-642-21551-3_17},
  editor               = {Gentle, James E. and Hardle, Wolfgang K. and Mori, Yuichi},
  pages                = {499--527},
  publisher            = {Springer Berlin Heidelberg},
  series               = {Springer Handbooks of Computational Statistics},
  abstract             = {Thebootstrap is by now a standard method in modern statistics. Its roots go back to a lot ofresampling ideas that were around in the seventies. The seminal work of Efron synthesized some of the earlierresampling ideas and established a new framework for simulation based statistical analysis. The idea of thebootstrap is to develop a setup to generate more (pseudo) data using the information of the original data. True underlying sample properties are reproduced as closely as possible and unknown model characteristics are replaced by sample estimates.},
  citeulike-article-id = {14514292},
  citeulike-linkout-0  = {http://dx.doi.org/10.1007/978-3-642-21551-317},
  citeulike-linkout-1  = {http://link.springer.com/chapter/10.1007/978-3-642-21551-317},
  creationdate         = {2021-07-03T00:14:01},
  modificationdate     = {2021-07-03T00:14:01},
  posted-at            = {2018-01-10 02:05:31},
  timestamp            = {2019-09-14 22:09},
}

@Article{Michaud-Michaud-2015,
  author               = {Michaud, Richard O. and Michaud, Robert},
  date                 = {2015-09},
  journaltitle         = {SSRN e-Print},
  title                = {Estimation Error and Portfolio Optimization: A Resampling Solution},
  url                  = {https://ssrn.com/abstract=2658657},
  abstract             = {Markowitz (1959) mean-variance (MV) portfolio optimization has been the practical standard for asset allocation and equity portfolio management for almost fifty years. However, it is known to be overly sensitive to estimation error in risk-return estimates and have poor out-of-sample performance characteristics. The Resampled Efficiency (RE) techniques presented in Michaud (1998) introduce Monte Carlo methods to properly represent investment information uncertainty in computing MV portfolio optimality and in defining trading and monitoring rules.

This paper reviews and updates the literature on estimation error and RE portfolio optimization and rebalancing. We resolve several open issues and misunderstandings that have emerged since Michaud (1998). In particular, we show RE optimization to be a Bayesian-based generalization and enhancement of Markowitz's solution.},
  citeulike-article-id = {13978588},
  citeulike-linkout-0  = {http://ssrn.com/abstract=2658657},
  citeulike-linkout-1  = {http://papers.ssrn.com/sol3/Delivery.cfm/SSRNID2666615code370794.pdf?abstractid=2658657 and mirid=1},
  creationdate         = {2021-07-03T00:14:21},
  day                  = {10},
  modificationdate     = {2021-07-03T00:14:21},
  owner                = {cristi},
  posted-at            = {2016-03-12 22:17:26},
  timestamp            = {2020-07-23 13:37},
}

@InCollection{Oneto-2019,
  author           = {Luca Oneto},
  booktitle        = {Model Selection and Error Estimation in a Nutshell},
  date             = {2019-07},
  title            = {Resampling Methods},
  doi              = {10.1007/978-3-030-24359-3_4},
  pages            = {25--31},
  publisher        = {Springer International Publishing},
  abstract         = {Resampling methods, also called Out-of-Sample methods, are favoured by practitioners because they work well in many situations and allow the application of simple statistical techniques for estimating the quantities of interest.},
  creationdate     = {2021-07-03T00:14:38},
  modificationdate = {2021-07-03T00:14:38},
  timestamp        = {2020-10-05 00:04},
}

@Article{Sarris-et-al-2020,
  author           = {Dimitrios Sarris and Evangelos Spiliotis and Vassilios Assimakopoulos},
  date             = {2020},
  journaltitle     = {Operational Research},
  title            = {Exploiting resampling techniques for model selection in forecasting: an empirical evaluation using out-of-sample tests},
  doi              = {10.1007/s12351-017-0347-0},
  number           = {2},
  pages            = {701--721},
  volume           = {20},
  abstract         = {Model selection is a complex task widely examined in the literature due to the major gains in forecasting accuracy when performed successfully. To do so, many approaches have been proposed exploiting the available historical data in different ways. In-sample testing is the most common approach but is highly affected by the data and parameter estimation uncertainty. Out-of-sample tests, which use part of the data to evaluate the performance of the forecasting methods, are also well-known alternatives which usually lead to improvements. However, these are still vulnerable to data uncertainty such as noise and outliers. On the other hand, resampling techniques can be used to produce multiple clones of a time series with the same characteristics but a different component of randomness. In this paper, a model selection technique is proposed which takes advantage of the bootstrapping process to mitigate the effect of noise in the original data and then applies out-of-sample tests to the generated series to evaluate the forecasting performance of different methods. The approach is assessed across a large dataset of diverse time series and benchmarked versus other traditional approaches leading to promising results.},
  creationdate     = {2021-07-03T00:15:05},
  modificationdate = {2021-07-03T00:15:05},
  publisher        = {Island Press},
}

@Article{Yeh-2020a,
  author           = {Yeh, Chin-Chia Michael},
  date             = {2020},
  journaltitle     = {arXiv e-Print},
  title            = {Towards a Near Universal Time Series Data Mining Tool: Introducing the Matrix Profile},
  url              = {https://arxiv.org/abs/1811.03064},
  abstract         = {The last decade has seen a flurry of research on all-pairs-similarity-search (or, self-join) for text, DNA, and a handful of other datatypes, and these systems have been applied to many diverse data mining problems. Surprisingly, however, little progress has been made on addressing this problem for time series subsequences. In this thesis, we have introduced a near universal time series data mining tool called matrix profile which solves the all-pairs-similarity-search problem and caches the output in an   easy-to-access fashion. The proposed algorithm is not only parameter-free, exact and scalable, but also applicable for both single and multidimensional time series. By building time series data mining methods on top of matrix profile, many time series data mining tasks (e.g., motif discovery, discord discovery, shapelet discovery, semantic segmentation, and clustering) can be efficiently solved. Because the same matrix profile can be shared by a diverse set of time series data mining methods, matrix profile is versatile and computed-once-use-many-times data structure. We demonstrate the utility of matrix profile for many time series data mining problems, including motif discovery, discord discovery, weakly labeled time series classification, and representation learning on domains as diverse as seismology, entomology, music processing, bioinformatics, human activity monitoring, electrical power-demand monitoring, and medicine. We hope the matrix profile is not the end but the beginning of many more time series data mining projects.},
  creationdate     = {2021-07-03T00:15:27},
  day              = {5},
  f1000-projects   = {QuantInvest},
  groups           = {Scenario_TimeSeries, ML_ClustTimeSrs},
  modificationdate = {2021-07-03T00:15:27},
}

@Comment{jabref-meta: databaseType:biblatex;}
